# é˜¶æ®µ2ï¼šRNN/LSTM åºåˆ—å»ºæ¨¡ä¸æ–‡æœ¬ç”Ÿæˆ

## ğŸ“‹ æ¦‚è¿°

åœ¨é˜¶æ®µ2ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢ç´¢å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)ï¼Œå­¦ä¹ å¦‚ä½•å¤„ç†åºåˆ—æ•°æ®å¹¶å®ç°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡è¿™ä¸ªé˜¶æ®µï¼Œæ‚¨å°†äº†è§£ï¼š

- RNNçš„å·¥ä½œåŸç†å’Œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- LSTMé—¨æ§æœºåˆ¶çš„è®¾è®¡ç†å¿µ
- åºåˆ—åˆ°åºåˆ—çš„æ–‡æœ¬ç”Ÿæˆ
- Teacher Forcingè®­ç»ƒç­–ç•¥
- æ¢¯åº¦è£å‰ªå’Œæ­£åˆ™åŒ–æŠ€æœ¯

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
stage2_rnn_lstm/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ rnn.py              # RNNæ¨¡å‹å®ç°
â”‚   â””â”€â”€ lstm.py             # LSTMæ¨¡å‹å®ç°
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ text_data.py        # æ–‡æœ¬æ•°æ®å¤„ç†å·¥å…·
â”œâ”€â”€ data/                   # æ•°æ®å­˜å‚¨ç›®å½•
â”œâ”€â”€ train.py                # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ generate.py             # æ–‡æœ¬ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ visualize.ipynb         # å¯è§†åŒ–åˆ†æ
â””â”€â”€ README.md              # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬è®­ç»ƒï¼ˆå­—ç¬¦çº§LSTMï¼‰

```bash
cd stage2_rnn_lstm
python train.py --model_type lstm --vocab_type char --epochs 15
```

### 2. è¯çº§RNNè®­ç»ƒ

```bash
python train.py --model_type rnn --vocab_type word --epochs 20 --seq_length 32
```

### 3. è‡ªå®šä¹‰è®­ç»ƒé…ç½®

```bash
python train.py \
    --model_type lstm \
    --vocab_type char \
    --epochs 25 \
    --batch_size 64 \
    --seq_length 128 \
    --hidden_size 512 \
    --num_layers 3 \
    --learning_rate 0.001 \
    --dropout 0.2 \
    --max_grad_norm 5.0
```

### 4. ä½¿ç”¨è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®

```bash
python train.py --data_file path/to/your/text.txt --epochs 30
```

## ğŸ“Š è®­ç»ƒå‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--model_type` | `lstm` | æ¨¡å‹ç±»å‹ï¼š`rnn`, `lstm`, `gru` |
| `--vocab_type` | `char` | è¯æ±‡ç±»å‹ï¼š`char` æˆ– `word` |
| `--epochs` | `20` | è®­ç»ƒè½®æ•° |
| `--batch_size` | `32` | æ‰¹æ¬¡å¤§å° |
| `--seq_length` | `64` | åºåˆ—é•¿åº¦ |
| `--learning_rate` | `0.001` | å­¦ä¹ ç‡ |
| `--hidden_size` | `256` | éšè—çŠ¶æ€ç»´åº¦ |
| `--num_layers` | `2` | ç½‘ç»œå±‚æ•° |
| `--embedding_dim` | `128` | åµŒå…¥ç»´åº¦ |
| `--dropout` | `0.3` | Dropoutç‡ |
| `--max_grad_norm` | `1.0` | æ¢¯åº¦è£å‰ªé˜ˆå€¼ |
| `--teacher_forcing_ratio` | `1.0` | Teacher forcingæ¯”ä¾‹ |
| `--save_dir` | `./checkpoints` | æ¨¡å‹ä¿å­˜ç›®å½• |

## ğŸ” æ¨¡å‹æ¶æ„

### 1. SimpleRNN
```
Input â†’ Embedding â†’ RNN Layers â†’ Linear â†’ Output
                     â†“
              Hidden States (å¾ªç¯è¿æ¥)
```

**ç‰¹ç‚¹ï¼š**
- åŸºç¡€çš„å¾ªç¯ç¥ç»ç½‘ç»œ
- å®¹æ˜“äº§ç”Ÿæ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- é€‚åˆçŸ­åºåˆ—ä»»åŠ¡

### 2. SimpleLSTM
```
Input â†’ Embedding â†’ LSTM Layers â†’ Linear â†’ Output
                      â†“
              Memory Cells + Hidden States
                 (é—¨æ§æœºåˆ¶)
```

**LSTMé—¨æ§ï¼š**
- **é—å¿˜é—¨**ï¼šå†³å®šä¸¢å¼ƒå“ªäº›ä¿¡æ¯
- **è¾“å…¥é—¨**ï¼šå†³å®šå­˜å‚¨å“ªäº›æ–°ä¿¡æ¯  
- **è¾“å‡ºé—¨**ï¼šæ§åˆ¶è¾“å‡ºçš„ä¿¡æ¯
- **è®°å¿†ç»†èƒ**ï¼šé•¿æœŸè®°å¿†å­˜å‚¨

### 3. BiLSTMï¼ˆåŒå‘LSTMï¼‰
```
Forward LSTM  â†’ â†’ â†’ â†’
Input Sequence â†“ â†“ â†“ â†“ â†’ Classification
Backward LSTM â† â† â† â†
```

**é€‚ç”¨åœºæ™¯ï¼š**
- æ–‡æœ¬åˆ†ç±»ä»»åŠ¡
- éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åœºæ™¯

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

### ä¸»è¦æŒ‡æ ‡
- **æŸå¤±å€¼ (Loss)**ï¼šäº¤å‰ç†µæŸå¤±
- **å›°æƒ‘åº¦ (Perplexity)**ï¼šexp(loss)ï¼Œè¶Šå°è¶Šå¥½
- **ç”Ÿæˆè´¨é‡**ï¼šäººå·¥è¯„ä¼°æµç•…æ€§å’Œè¿è´¯æ€§

### é¢„æœŸæ€§èƒ½
ä½¿ç”¨é»˜è®¤å‚æ•°åœ¨ç¤ºä¾‹æ•°æ®ä¸Šçš„é¢„æœŸç»“æœï¼š

| æ¨¡å‹ | è¯æ±‡ç±»å‹ | æœ€ç»ˆå›°æƒ‘åº¦ | è®­ç»ƒæ—¶é—´ |
|------|----------|------------|----------|
| RNN | å­—ç¬¦çº§ | 15-25 | 5-10åˆ†é’Ÿ |
| LSTM | å­—ç¬¦çº§ | 8-15 | 10-15åˆ†é’Ÿ |
| LSTM | è¯çº§ | 20-35 | 8-12åˆ†é’Ÿ |

## ğŸ¨ æ–‡æœ¬ç”Ÿæˆç­–ç•¥

### 1. è´ªå¿ƒè§£ç 
```python
next_token = torch.argmax(probabilities)
```
- æ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯
- ç”Ÿæˆç¡®å®šä½†å¯èƒ½é‡å¤çš„æ–‡æœ¬

### 2. éšæœºé‡‡æ ·
```python
next_token = torch.multinomial(probabilities, 1)
```
- æŒ‰æ¦‚ç‡åˆ†å¸ƒéšæœºé‡‡æ ·
- ç”Ÿæˆå¤šæ ·ä½†å¯èƒ½ä¸è¿è´¯çš„æ–‡æœ¬

### 3. æ¸©åº¦é‡‡æ ·
```python
probabilities = torch.softmax(logits / temperature, dim=-1)
```
- `temperature < 1`ï¼šæ›´ä¿å®ˆçš„é€‰æ‹©
- `temperature > 1`ï¼šæ›´éšæœºçš„é€‰æ‹©
- `temperature = 1`ï¼šæ ‡å‡†softmax

### 4. Top-ké‡‡æ ·
```python
top_k_probs, top_k_indices = torch.topk(probabilities, k)
```
- åªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„kä¸ªè¯
- å¹³è¡¡å¤šæ ·æ€§å’Œè´¨é‡

## ğŸ“š å…³é”®æ¦‚å¿µè§£æ

### 1. æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
**ç°è±¡ï¼š** RNNè®­ç»ƒæ—¶ï¼Œè¯¯å·®ä¿¡å·åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ¸å‡å¼±

**åŸå› ï¼š**
```
âˆ‚L/âˆ‚h_t = (âˆ‚L/âˆ‚h_{t+1}) * W_hh * Ïƒ'(h_t)
```
å½“æƒé‡çŸ©é˜µçš„è°±èŒƒæ•° < 1æ—¶ï¼Œæ¢¯åº¦å‘ˆæŒ‡æ•°è¡°å‡

**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨LSTM/GRUé—¨æ§æœºåˆ¶
- æ¢¯åº¦è£å‰ª
- åˆé€‚çš„æƒé‡åˆå§‹åŒ–

### 2. Teacher Forcing
**è®­ç»ƒæ—¶ï¼š** ä½¿ç”¨çœŸå®çš„ç›®æ ‡åºåˆ—ä½œä¸ºä¸‹ä¸€æ—¶åˆ»çš„è¾“å…¥
**æ¨ç†æ—¶ï¼š** ä½¿ç”¨æ¨¡å‹è‡ªå·±çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€æ—¶åˆ»çš„è¾“å…¥

**ä¼˜ç¼ºç‚¹ï¼š**
- âœ… è®­ç»ƒç¨³å®šï¼Œæ”¶æ•›å¿«
- âŒ è®­ç»ƒä¸æ¨ç†å­˜åœ¨å·®å¼‚
- âŒ å¯èƒ½å¯¼è‡´è¯¯å·®ç´¯ç§¯

### 3. æ¢¯åº¦è£å‰ª
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
```
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- ç¨³å®šè®­ç»ƒè¿‡ç¨‹
- å¸¸ç”¨é˜ˆå€¼ï¼š1.0-5.0

## ğŸ”§ æ•°æ®å¤„ç†

### å­—ç¬¦çº§å¤„ç†
```python
char_vocab = CharacterVocabulary()
char_vocab.build_vocabulary(texts)
indices = char_vocab.text_to_indices("Hello World!")
```

**ä¼˜ç‚¹ï¼š**
- è¯æ±‡è¡¨å°ï¼Œè®­ç»ƒå¿«
- èƒ½å¤„ç†æœªè§è¿‡çš„è¯
- é€‚åˆç”Ÿæˆåˆ›é€ æ€§æ–‡æœ¬

**ç¼ºç‚¹ï¼š**
- åºåˆ—é•¿ï¼Œè®¡ç®—é‡å¤§
- éš¾ä»¥æ•æ‰è¯çº§è¯­ä¹‰

### è¯çº§å¤„ç†
```python
word_vocab = WordVocabulary(max_vocab_size=10000)
word_vocab.build_vocabulary(texts)
indices = word_vocab.text_to_indices("Hello World!")
```

**ä¼˜ç‚¹ï¼š**
- åºåˆ—çŸ­ï¼Œæ•ˆç‡é«˜
- ä¿æŒè¯æ±‡è¯­ä¹‰å®Œæ•´
- æ›´ç¬¦åˆè‡ªç„¶è¯­è¨€ç»“æ„

**ç¼ºç‚¹ï¼š**
- è¯æ±‡è¡¨å¤§ï¼Œå†…å­˜æ¶ˆè€—å¤š
- å­˜åœ¨OOVï¼ˆæœªç™»å½•è¯ï¼‰é—®é¢˜

## ğŸ¯ å®éªŒå»ºè®®

### 1. è¶…å‚æ•°å¯¹æ¯”å®éªŒ
```bash
# æ¯”è¾ƒä¸åŒæ¨¡å‹ç±»å‹
python train.py --model_type rnn --epochs 15
python train.py --model_type lstm --epochs 15  
python train.py --model_type gru --epochs 15

# æ¯”è¾ƒä¸åŒéšè—ç»´åº¦
python train.py --hidden_size 128 --epochs 15
python train.py --hidden_size 256 --epochs 15
python train.py --hidden_size 512 --epochs 15
```

### 2. åºåˆ—é•¿åº¦å½±å“
```bash
# çŸ­åºåˆ—
python train.py --seq_length 32 --epochs 20

# ä¸­ç­‰åºåˆ—  
python train.py --seq_length 64 --epochs 20

# é•¿åºåˆ—
python train.py --seq_length 128 --epochs 20
```

### 3. æ­£åˆ™åŒ–æŠ€æœ¯
```bash
# ä¸åŒdropoutç‡
python train.py --dropout 0.1 --epochs 15
python train.py --dropout 0.3 --epochs 15
python train.py --dropout 0.5 --epochs 15

# ä¸åŒæ¢¯åº¦è£å‰ªé˜ˆå€¼
python train.py --max_grad_norm 0.5 --epochs 15
python train.py --max_grad_norm 1.0 --epochs 15
python train.py --max_grad_norm 5.0 --epochs 15
```

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æŸå¤±ä¸ä¸‹é™
**å¯èƒ½åŸå› ï¼š**
- å­¦ä¹ ç‡è¿‡é«˜æˆ–è¿‡ä½
- æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- åºåˆ—é•¿åº¦ä¸åˆé€‚

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# è°ƒæ•´å­¦ä¹ ç‡
python train.py --learning_rate 0.01
python train.py --learning_rate 0.0001

# è°ƒæ•´æ¢¯åº¦è£å‰ª
python train.py --max_grad_norm 0.25

# ä½¿ç”¨LSTMæ›¿ä»£RNN
python train.py --model_type lstm
```

### Q2: ç”Ÿæˆæ–‡æœ¬è´¨é‡å·®
**å¯èƒ½åŸå› ï¼š**
- è®­ç»ƒä¸å……åˆ†
- æ¸©åº¦å‚æ•°ä¸åˆé€‚
- æ•°æ®é‡å¤ªå°‘

**è§£å†³æ–¹æ¡ˆï¼š**
- å¢åŠ è®­ç»ƒè½®æ•°
- è°ƒæ•´ç”Ÿæˆæ¸©åº¦ï¼ˆ0.7-1.2ï¼‰
- ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®
- å°è¯•ä¸åŒé‡‡æ ·ç­–ç•¥

### Q3: å†…å­˜ä¸è¶³
**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å‡å°‘æ‰¹æ¬¡å¤§å°
python train.py --batch_size 16

# å‡å°‘åºåˆ—é•¿åº¦
python train.py --seq_length 32

# å‡å°‘éšè—ç»´åº¦
python train.py --hidden_size 128
```

## ğŸ“ ä»£ç ç¤ºä¾‹

### å¿«é€Ÿæ–‡æœ¬ç”Ÿæˆ
```python
import torch
from models.lstm import SimpleLSTM
from utils.text_data import CharacterVocabulary

# åŠ è½½æ¨¡å‹å’Œè¯æ±‡è¡¨
model = SimpleLSTM(vocab_size=100, hidden_size=256)
model.load_state_dict(torch.load('checkpoints/best_lstm_char.pt')['model_state_dict'])

vocab = CharacterVocabulary()
vocab.load('checkpoints/char_vocabulary.pkl')

# ç”Ÿæˆæ–‡æœ¬
model.eval()
generated = model.generate(
    start_token=vocab.char2idx['T'],
    max_length=200,
    temperature=0.8
)

generated_text = vocab.indices_to_text(generated)
print(generated_text)
```

### è‡ªå®šä¹‰æ•°æ®è®­ç»ƒ
```python
from utils.text_data import CharacterVocabulary, create_data_loaders

# å‡†å¤‡ä½ çš„æ–‡æœ¬æ•°æ®
texts = ["Your text data here...", "More text...", ...]

# æ„å»ºè¯æ±‡è¡¨
vocab = CharacterVocabulary()
vocab.build_vocabulary(texts)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader, val_loader = create_data_loaders(
    train_texts, val_texts, vocab, 
    seq_length=64, batch_size=32
)

# è®­ç»ƒæ¨¡å‹...
```

## ğŸ”— ä¸‹ä¸€æ­¥

å®Œæˆè¿™ä¸ªé˜¶æ®µåï¼Œæ‚¨å¯ä»¥ï¼š

1. **è¿›å…¥é˜¶æ®µ3**ï¼šå­¦ä¹ æ³¨æ„åŠ›æœºåˆ¶å’ŒSeq2Seqæ¨¡å‹
2. **æ·±åº¦å®éªŒ**ï¼šå°è¯•æ›´å¤æ‚çš„æ•°æ®é›†ï¼ˆå¦‚æ•´æœ¬å°è¯´ï¼‰
3. **æ¨¡å‹ä¼˜åŒ–**ï¼šå®ç°æ›´é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥
4. **åº”ç”¨æ‰©å±•**ï¼šå°è¯•å…¶ä»–åºåˆ—ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ï¼‰

---

## ğŸ‰ æ­å–œï¼

å®Œæˆé˜¶æ®µ2åï¼Œæ‚¨å·²ç»æŒæ¡äº†ï¼š
- RNN/LSTMçš„å·¥ä½œåŸç†å’Œå®ç°
- åºåˆ—å»ºæ¨¡çš„æ ¸å¿ƒæŠ€æœ¯
- æ–‡æœ¬ç”Ÿæˆçš„å„ç§ç­–ç•¥
- å¤„ç†åºåˆ—æ•°æ®çš„æœ€ä½³å®è·µ

è¿™äº›çŸ¥è¯†ä¸ºç†è§£æ›´é«˜çº§çš„æ¨¡å‹ï¼ˆå¦‚Transformerï¼‰å¥ å®šäº†é‡è¦åŸºç¡€ï¼