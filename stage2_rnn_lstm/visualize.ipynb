{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# é˜¶æ®µ2ï¼šRNN/LSTM å¯è§†åŒ–åˆ†æ\n",
    "\n",
    "æœ¬notebookç”¨äºå¯è§†åŒ–åˆ†æRNN/LSTMæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å’Œç”Ÿæˆç»“æœï¼ŒåŒ…æ‹¬ï¼š\n",
    "- è®­ç»ƒæ›²çº¿åˆ†æ\n",
    "- ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹\n",
    "- éšè—çŠ¶æ€å¯è§†åŒ–\n",
    "- æ¨¡å‹å‚æ•°åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# æ·»åŠ æ¨¡å—è·¯å¾„\n",
    "sys.path.append('.')\n",
    "from models.rnn import create_rnn_model\n",
    "from models.lstm import SimpleLSTM\n",
    "from utils.text_data import CharacterVocabulary, WordVocabulary\n",
    "from generate import load_model_and_vocab, temperature_sampling\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®å›¾å½¢æ ·å¼\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è®­ç»ƒå†å²å¯è§†åŒ–\n",
    "\n",
    "åˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å’Œå›°æƒ‘åº¦å˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history_path, save_path=None):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿\n",
    "    \n",
    "    Args:\n",
    "        history_path (str): è®­ç»ƒå†å²æ–‡ä»¶è·¯å¾„\n",
    "        save_path (str): ä¿å­˜è·¯å¾„\n",
    "    \"\"\"\n",
    "    # åŠ è½½è®­ç»ƒå†å²\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('RNN/LSTM è®­ç»ƒå†å²åˆ†æ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # è®­ç»ƒæŸå¤±\n",
    "    axes[0, 0].plot(history['train_losses'], label='è®­ç»ƒæŸå¤±', linewidth=2)\n",
    "    if 'val_losses' in history:\n",
    "        axes[0, 0].plot(history['val_losses'], label='éªŒè¯æŸå¤±', linewidth=2)\n",
    "    axes[0, 0].set_title('æŸå¤±æ›²çº¿')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # å›°æƒ‘åº¦\n",
    "    if 'train_perplexities' in history:\n",
    "        axes[0, 1].plot(history['train_perplexities'], label='è®­ç»ƒå›°æƒ‘åº¦', linewidth=2)\n",
    "        if 'val_perplexities' in history:\n",
    "            axes[0, 1].plot(history['val_perplexities'], label='éªŒè¯å›°æƒ‘åº¦', linewidth=2)\n",
    "        axes[0, 1].set_title('å›°æƒ‘åº¦æ›²çº¿')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Perplexity')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # å­¦ä¹ ç‡\n",
    "    if 'learning_rates' in history:\n",
    "        axes[1, 0].plot(history['learning_rates'], linewidth=2, color='orange')\n",
    "        axes[1, 0].set_title('å­¦ä¹ ç‡å˜åŒ–')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ¢¯åº¦èŒƒæ•°\n",
    "    if 'grad_norms' in history:\n",
    "        axes[1, 1].plot(history['grad_norms'], linewidth=2, color='red')\n",
    "        axes[1, 1].set_title('æ¢¯åº¦èŒƒæ•°')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Gradient Norm')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "# plot_training_history('checkpoints/training_history.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹å¯¹æ¯”\n",
    "\n",
    "æ¯”è¾ƒä¸åŒé‡‡æ ·ç­–ç•¥çš„ç”Ÿæˆæ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_strategies(checkpoint_path, vocab_path, start_text=\"The\", max_length=200):\n",
    "    \"\"\"\n",
    "    æ¯”è¾ƒä¸åŒç”Ÿæˆç­–ç•¥çš„æ•ˆæœ\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "        vocab_path (str): è¯æ±‡è¡¨è·¯å¾„\n",
    "        start_text (str): èµ·å§‹æ–‡æœ¬\n",
    "        max_length (int): ç”Ÿæˆé•¿åº¦\n",
    "    \"\"\"\n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    model, vocab, device = load_model_and_vocab(checkpoint_path, vocab_path)\n",
    "    \n",
    "    # ä¸åŒç­–ç•¥çš„å‚æ•°\n",
    "    strategies = {\n",
    "        'è´ªå¿ƒè§£ç ': {'temperature': 0.1},\n",
    "        'æ¸©åº¦é‡‡æ · (T=0.5)': {'temperature': 0.5},\n",
    "        'æ¸©åº¦é‡‡æ · (T=0.8)': {'temperature': 0.8},\n",
    "        'æ¸©åº¦é‡‡æ · (T=1.2)': {'temperature': 1.2},\n",
    "        'é«˜æ¸©é‡‡æ · (T=1.5)': {'temperature': 1.5}\n",
    "    }\n",
    "    \n",
    "    print(f\"èµ·å§‹æ–‡æœ¬: '{start_text}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {}\n",
    "    for strategy_name, params in strategies.items():\n",
    "        print(f\"\\nğŸ“ {strategy_name}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        generated = temperature_sampling(\n",
    "            model, vocab, device, start_text, max_length, \n",
    "            temperature=params['temperature']\n",
    "        )\n",
    "        \n",
    "        results[strategy_name] = generated\n",
    "        print(generated)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "# results = compare_generation_strategies(\n",
    "#     'checkpoints/best_lstm_char.pt',\n",
    "#     'checkpoints/char_vocabulary.pkl',\n",
    "#     start_text=\"Once upon a time\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. éšè—çŠ¶æ€å¯è§†åŒ–\n",
    "\n",
    "ä½¿ç”¨t-SNEå¯è§†åŒ–æ¨¡å‹çš„éšè—çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def extract_hidden_states(model, vocab, device, texts, max_seq_len=50):\n",
    "    \"\"\"\n",
    "    æå–æ¨¡å‹çš„éšè—çŠ¶æ€\n",
    "    \n",
    "    Args:\n",
    "        model: è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "        vocab: è¯æ±‡è¡¨\n",
    "        device: è®¡ç®—è®¾å¤‡\n",
    "        texts (list): æ–‡æœ¬åˆ—è¡¨\n",
    "        max_seq_len (int): æœ€å¤§åºåˆ—é•¿åº¦\n",
    "        \n",
    "    Returns:\n",
    "        numpy.array: éšè—çŠ¶æ€çŸ©é˜µ\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hidden_states = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # æ–‡æœ¬è½¬ç´¢å¼•\n",
    "            if isinstance(vocab, CharacterVocabulary):\n",
    "                indices = vocab.text_to_indices(text[:max_seq_len])\n",
    "            else:\n",
    "                words = text.split()[:max_seq_len]\n",
    "                indices = vocab.text_to_indices(words)\n",
    "            \n",
    "            if len(indices) < 5:  # è·³è¿‡å¤ªçŸ­çš„åºåˆ—\n",
    "                continue\n",
    "                \n",
    "            # è½¬æ¢ä¸ºtensor\n",
    "            input_tensor = torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            \n",
    "            # å‰å‘ä¼ æ’­è·å–éšè—çŠ¶æ€\n",
    "            if hasattr(model, 'get_hidden_states'):\n",
    "                hidden = model.get_hidden_states(input_tensor)\n",
    "            else:\n",
    "                # æ‰‹åŠ¨æå–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€\n",
    "                embeddings = model.embedding(input_tensor)\n",
    "                if isinstance(model, SimpleLSTM):\n",
    "                    output, (hidden, cell) = model.lstm(embeddings)\n",
    "                    hidden = hidden[-1, 0, :]  # å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€\n",
    "                else:\n",
    "                    output, hidden = model.rnn(embeddings)\n",
    "                    hidden = hidden[-1, 0, :]  # å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€\n",
    "            \n",
    "            hidden_states.append(hidden.cpu().numpy())\n",
    "    \n",
    "    return np.array(hidden_states)\n",
    "\n",
    "def visualize_hidden_states(hidden_states, labels=None, method='tsne', save_path=None):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–éšè—çŠ¶æ€\n",
    "    \n",
    "    Args:\n",
    "        hidden_states (numpy.array): éšè—çŠ¶æ€çŸ©é˜µ\n",
    "        labels (list): æ ‡ç­¾åˆ—è¡¨\n",
    "        method (str): é™ç»´æ–¹æ³• ('tsne' æˆ– 'pca')\n",
    "        save_path (str): ä¿å­˜è·¯å¾„\n",
    "    \"\"\"\n",
    "    if method == 'tsne':\n",
    "        # é¦–å…ˆç”¨PCAé™ç»´åˆ°50ç»´ï¼Œå†ç”¨t-SNE\n",
    "        if hidden_states.shape[1] > 50:\n",
    "            pca = PCA(n_components=50)\n",
    "            hidden_states = pca.fit_transform(hidden_states)\n",
    "        \n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(hidden_states)-1))\n",
    "        embedded = reducer.fit_transform(hidden_states)\n",
    "        title = 't-SNE éšè—çŠ¶æ€å¯è§†åŒ–'\n",
    "    else:\n",
    "        reducer = PCA(n_components=2)\n",
    "        embedded = reducer.fit_transform(hidden_states)\n",
    "        title = 'PCA éšè—çŠ¶æ€å¯è§†åŒ–'\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    if labels is not None:\n",
    "        unique_labels = list(set(labels))\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for i, label in enumerate(unique_labels):\n",
    "            mask = np.array(labels) == label\n",
    "            plt.scatter(embedded[mask, 0], embedded[mask, 1], \n",
    "                       c=[colors[i]], label=label, alpha=0.7, s=50)\n",
    "        \n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        plt.scatter(embedded[:, 0], embedded[:, 1], alpha=0.7, s=50)\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "# texts = [\"Hello world\", \"Good morning\", \"How are you\", ...]\n",
    "# hidden_states = extract_hidden_states(model, vocab, device, texts)\n",
    "# visualize_hidden_states(hidden_states, method='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨¡å‹å‚æ•°åˆ†æ\n",
    "\n",
    "åˆ†ææ¨¡å‹æƒé‡åˆ†å¸ƒå’Œæ¢¯åº¦æƒ…å†µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_parameters(checkpoint_path):\n",
    "    \"\"\"\n",
    "    åˆ†ææ¨¡å‹å‚æ•°\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "    \"\"\"\n",
    "    # åŠ è½½æ£€æŸ¥ç‚¹\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # æå–æ¨¡å‹å‚æ•°\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    \n",
    "    # è®¡ç®—å‚æ•°ç»Ÿè®¡ä¿¡æ¯\n",
    "    param_stats = {}\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, param in state_dict.items():\n",
    "        param_tensor = param.detach().numpy()\n",
    "        param_stats[name] = {\n",
    "            'shape': param_tensor.shape,\n",
    "            'mean': np.mean(param_tensor),\n",
    "            'std': np.std(param_tensor),\n",
    "            'min': np.min(param_tensor),\n",
    "            'max': np.max(param_tensor),\n",
    "            'num_params': param_tensor.size\n",
    "        }\n",
    "        total_params += param_tensor.size\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('æ¨¡å‹å‚æ•°åˆ†æ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # å‚æ•°æ•°é‡åˆ†å¸ƒ\n",
    "    layer_names = []\n",
    "    param_counts = []\n",
    "    \n",
    "    for name, stats in param_stats.items():\n",
    "        layer_names.append(name.split('.')[0])  # å–å±‚å\n",
    "        param_counts.append(stats['num_params'])\n",
    "    \n",
    "    # æŒ‰å±‚èšåˆ\n",
    "    layer_param_counts = {}\n",
    "    for layer, count in zip(layer_names, param_counts):\n",
    "        layer_param_counts[layer] = layer_param_counts.get(layer, 0) + count\n",
    "    \n",
    "    axes[0, 0].bar(layer_param_counts.keys(), layer_param_counts.values())\n",
    "    axes[0, 0].set_title('å„å±‚å‚æ•°æ•°é‡')\n",
    "    axes[0, 0].set_ylabel('å‚æ•°æ•°é‡')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # å‚æ•°å‡å€¼åˆ†å¸ƒ\n",
    "    means = [stats['mean'] for stats in param_stats.values()]\n",
    "    axes[0, 1].hist(means, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_title('å‚æ•°å‡å€¼åˆ†å¸ƒ')\n",
    "    axes[0, 1].set_xlabel('å‚æ•°å‡å€¼')\n",
    "    axes[0, 1].set_ylabel('é¢‘æ¬¡')\n",
    "    \n",
    "    # å‚æ•°æ ‡å‡†å·®åˆ†å¸ƒ\n",
    "    stds = [stats['std'] for stats in param_stats.values()]\n",
    "    axes[1, 0].hist(stds, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axes[1, 0].set_title('å‚æ•°æ ‡å‡†å·®åˆ†å¸ƒ')\n",
    "    axes[1, 0].set_xlabel('å‚æ•°æ ‡å‡†å·®')\n",
    "    axes[1, 0].set_ylabel('é¢‘æ¬¡')\n",
    "    \n",
    "    # å‚æ•°èŒƒå›´\n",
    "    param_names = list(param_stats.keys())[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª\n",
    "    mins = [param_stats[name]['min'] for name in param_names]\n",
    "    maxs = [param_stats[name]['max'] for name in param_names]\n",
    "    \n",
    "    x = np.arange(len(param_names))\n",
    "    axes[1, 1].bar(x, maxs, alpha=0.7, label='æœ€å¤§å€¼')\n",
    "    axes[1, 1].bar(x, mins, alpha=0.7, label='æœ€å°å€¼')\n",
    "    axes[1, 1].set_title('å‚æ•°èŒƒå›´ (å‰10å±‚)')\n",
    "    axes[1, 1].set_xlabel('å±‚å')\n",
    "    axes[1, 1].set_ylabel('å‚æ•°å€¼')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels([name.split('.')[0] for name in param_names], rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°æ€»ç»“ä¿¡æ¯\n",
    "    print(f\"\\nğŸ“Š æ¨¡å‹å‚æ•°æ€»ç»“:\")\n",
    "    print(f\"æ€»å‚æ•°æ•°é‡: {total_params:,}\")\n",
    "    print(f\"å±‚æ•°: {len(param_stats)}\")\n",
    "    \n",
    "    return param_stats\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "# param_stats = analyze_model_parameters('checkpoints/best_lstm_char.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ç”Ÿæˆè´¨é‡è¯„ä¼°\n",
    "\n",
    "è¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_text_metrics(text):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ–‡æœ¬è´¨é‡æŒ‡æ ‡\n",
    "    \n",
    "    Args:\n",
    "        text (str): ç”Ÿæˆçš„æ–‡æœ¬\n",
    "        \n",
    "    Returns:\n",
    "        dict: è´¨é‡æŒ‡æ ‡å­—å…¸\n",
    "    \"\"\"\n",
    "    # åŸºç¡€ç»Ÿè®¡\n",
    "    char_count = len(text)\n",
    "    word_count = len(text.split())\n",
    "    sentence_count = len(re.findall(r'[.!?]+', text))\n",
    "    \n",
    "    # è¯æ±‡å¤šæ ·æ€§\n",
    "    words = text.lower().split()\n",
    "    unique_words = len(set(words))\n",
    "    vocabulary_diversity = unique_words / max(word_count, 1)\n",
    "    \n",
    "    # é‡å¤åº¦åˆ†æ\n",
    "    word_freq = Counter(words)\n",
    "    most_common = word_freq.most_common(5)\n",
    "    \n",
    "    # å¹³å‡è¯é•¿\n",
    "    avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
    "    \n",
    "    # å¹³å‡å¥é•¿\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    avg_sentence_length = np.mean([len(sent.split()) for sent in sentences if sent.strip()]) if sentences else 0\n",
    "    \n",
    "    return {\n",
    "        'å­—ç¬¦æ•°': char_count,\n",
    "        'è¯æ•°': word_count,\n",
    "        'å¥æ•°': sentence_count,\n",
    "        'è¯æ±‡å¤šæ ·æ€§': vocabulary_diversity,\n",
    "        'ç‹¬ç‰¹è¯æ•°': unique_words,\n",
    "        'å¹³å‡è¯é•¿': avg_word_length,\n",
    "        'å¹³å‡å¥é•¿': avg_sentence_length,\n",
    "        'æœ€å¸¸è§è¯': most_common\n",
    "    }\n",
    "\n",
    "def compare_generation_quality(generated_texts):\n",
    "    \"\"\"\n",
    "    æ¯”è¾ƒä¸åŒç”Ÿæˆç­–ç•¥çš„æ–‡æœ¬è´¨é‡\n",
    "    \n",
    "    Args:\n",
    "        generated_texts (dict): ä¸åŒç­–ç•¥ç”Ÿæˆçš„æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    metrics_comparison = {}\n",
    "    \n",
    "    for strategy, text in generated_texts.items():\n",
    "        metrics = calculate_text_metrics(text)\n",
    "        metrics_comparison[strategy] = metrics\n",
    "    \n",
    "    # å¯è§†åŒ–æ¯”è¾ƒ\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('ç”Ÿæˆæ–‡æœ¬è´¨é‡å¯¹æ¯”', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    strategies = list(metrics_comparison.keys())\n",
    "    \n",
    "    # è¯æ±‡å¤šæ ·æ€§\n",
    "    diversity_scores = [metrics_comparison[s]['è¯æ±‡å¤šæ ·æ€§'] for s in strategies]\n",
    "    axes[0, 0].bar(strategies, diversity_scores)\n",
    "    axes[0, 0].set_title('è¯æ±‡å¤šæ ·æ€§')\n",
    "    axes[0, 0].set_ylabel('å¤šæ ·æ€§å¾—åˆ†')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # å¹³å‡è¯é•¿\n",
    "    word_lengths = [metrics_comparison[s]['å¹³å‡è¯é•¿'] for s in strategies]\n",
    "    axes[0, 1].bar(strategies, word_lengths, color='orange')\n",
    "    axes[0, 1].set_title('å¹³å‡è¯é•¿')\n",
    "    axes[0, 1].set_ylabel('å­—ç¬¦æ•°')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # å¹³å‡å¥é•¿\n",
    "    sentence_lengths = [metrics_comparison[s]['å¹³å‡å¥é•¿'] for s in strategies]\n",
    "    axes[0, 2].bar(strategies, sentence_lengths, color='green')\n",
    "    axes[0, 2].set_title('å¹³å‡å¥é•¿')\n",
    "    axes[0, 2].set_ylabel('è¯æ•°')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # è¯æ•°\n",
    "    word_counts = [metrics_comparison[s]['è¯æ•°'] for s in strategies]\n",
    "    axes[1, 0].bar(strategies, word_counts, color='red')\n",
    "    axes[1, 0].set_title('æ€»è¯æ•°')\n",
    "    axes[1, 0].set_ylabel('è¯æ•°')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # ç‹¬ç‰¹è¯æ•°\n",
    "    unique_word_counts = [metrics_comparison[s]['ç‹¬ç‰¹è¯æ•°'] for s in strategies]\n",
    "    axes[1, 1].bar(strategies, unique_word_counts, color='purple')\n",
    "    axes[1, 1].set_title('ç‹¬ç‰¹è¯æ•°')\n",
    "    axes[1, 1].set_ylabel('è¯æ•°')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # å¥æ•°\n",
    "    sentence_counts = [metrics_comparison[s]['å¥æ•°'] for s in strategies]\n",
    "    axes[1, 2].bar(strategies, sentence_counts, color='brown')\n",
    "    axes[1, 2].set_title('å¥æ•°')\n",
    "    axes[1, 2].set_ylabel('å¥æ•°')\n",
    "    axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°è¯¦ç»†å¯¹æ¯”\n",
    "    print(\"\\nğŸ“Š è¯¦ç»†è´¨é‡æŒ‡æ ‡å¯¹æ¯”:\")\n",
    "    print(\"=\" * 80)\n",
    "    for strategy, metrics in metrics_comparison.items():\n",
    "        print(f\"\\nğŸ¯ {strategy}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            if metric != 'æœ€å¸¸è§è¯':\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"  {metric}: {value:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {metric}: {value}\")\n",
    "        print(f\"  æœ€å¸¸è§è¯: {metrics['æœ€å¸¸è§è¯'][:3]}\")\n",
    "    \n",
    "    return metrics_comparison\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "# generated_texts = {\n",
    "#     'è´ªå¿ƒ': \"This is a sample text generated by greedy decoding.\",\n",
    "#     'æ¸©åº¦é‡‡æ ·': \"This is another sample text with more diversity and creativity.\"\n",
    "# }\n",
    "# quality_metrics = compare_generation_quality(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "ä»¥ä¸‹æ˜¯å®Œæ•´çš„åˆ†ææµç¨‹ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®è·¯å¾„\n",
    "checkpoint_path = 'checkpoints/best_lstm_char.pt'\n",
    "vocab_path = 'checkpoints/char_vocabulary.pkl'\n",
    "history_path = 'checkpoints/training_history.json'\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "from pathlib import Path\n",
    "if Path(checkpoint_path).exists():\n",
    "    print(\"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œå¼€å§‹åˆ†æ...\")\n",
    "    \n",
    "    # 1. è®­ç»ƒå†å²åˆ†æ\n",
    "    if Path(history_path).exists():\n",
    "        plot_training_history(history_path)\n",
    "    \n",
    "    # 2. ç”Ÿæˆç­–ç•¥å¯¹æ¯”\n",
    "    if Path(vocab_path).exists():\n",
    "        results = compare_generation_strategies(\n",
    "            checkpoint_path, vocab_path, \n",
    "            start_text=\"The quick brown fox\", \n",
    "            max_length=150\n",
    "        )\n",
    "        \n",
    "        # 3. è´¨é‡è¯„ä¼°\n",
    "        quality_metrics = compare_generation_quality(results)\n",
    "    \n",
    "    # 4. å‚æ•°åˆ†æ\n",
    "    param_stats = analyze_model_parameters(checkpoint_path)\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹\")\n",
    "    print(\"è¿è¡Œ: python train.py --model_type lstm --vocab_type char --epochs 10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}