{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 阶段2：RNN/LSTM 可视化分析\n",
    "\n",
    "本notebook用于可视化分析RNN/LSTM模型的训练过程和生成结果，包括：\n",
    "- 训练曲线分析\n",
    "- 生成文本示例\n",
    "- 隐藏状态可视化\n",
    "- 模型参数分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# 添加模块路径\n",
    "sys.path.append('.')\n",
    "from models.rnn import create_rnn_model\n",
    "from models.lstm import SimpleLSTM\n",
    "from utils.text_data import CharacterVocabulary, WordVocabulary\n",
    "from generate import load_model_and_vocab, temperature_sampling\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置图形样式\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 训练历史可视化\n",
    "\n",
    "分析训练过程中的损失和困惑度变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history_path, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制训练历史曲线\n",
    "    \n",
    "    Args:\n",
    "        history_path (str): 训练历史文件路径\n",
    "        save_path (str): 保存路径\n",
    "    \"\"\"\n",
    "    # 加载训练历史\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('RNN/LSTM 训练历史分析', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 训练损失\n",
    "    axes[0, 0].plot(history['train_losses'], label='训练损失', linewidth=2)\n",
    "    if 'val_losses' in history:\n",
    "        axes[0, 0].plot(history['val_losses'], label='验证损失', linewidth=2)\n",
    "    axes[0, 0].set_title('损失曲线')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 困惑度\n",
    "    if 'train_perplexities' in history:\n",
    "        axes[0, 1].plot(history['train_perplexities'], label='训练困惑度', linewidth=2)\n",
    "        if 'val_perplexities' in history:\n",
    "            axes[0, 1].plot(history['val_perplexities'], label='验证困惑度', linewidth=2)\n",
    "        axes[0, 1].set_title('困惑度曲线')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Perplexity')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 学习率\n",
    "    if 'learning_rates' in history:\n",
    "        axes[1, 0].plot(history['learning_rates'], linewidth=2, color='orange')\n",
    "        axes[1, 0].set_title('学习率变化')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 梯度范数\n",
    "    if 'grad_norms' in history:\n",
    "        axes[1, 1].plot(history['grad_norms'], linewidth=2, color='red')\n",
    "        axes[1, 1].set_title('梯度范数')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Gradient Norm')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 示例用法\n",
    "# plot_training_history('checkpoints/training_history.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 文本生成示例对比\n",
    "\n",
    "比较不同采样策略的生成效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_strategies(checkpoint_path, vocab_path, start_text=\"The\", max_length=200):\n",
    "    \"\"\"\n",
    "    比较不同生成策略的效果\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): 模型检查点路径\n",
    "        vocab_path (str): 词汇表路径\n",
    "        start_text (str): 起始文本\n",
    "        max_length (int): 生成长度\n",
    "    \"\"\"\n",
    "    # 加载模型\n",
    "    model, vocab, device = load_model_and_vocab(checkpoint_path, vocab_path)\n",
    "    \n",
    "    # 不同策略的参数\n",
    "    strategies = {\n",
    "        '贪心解码': {'temperature': 0.1},\n",
    "        '温度采样 (T=0.5)': {'temperature': 0.5},\n",
    "        '温度采样 (T=0.8)': {'temperature': 0.8},\n",
    "        '温度采样 (T=1.2)': {'temperature': 1.2},\n",
    "        '高温采样 (T=1.5)': {'temperature': 1.5}\n",
    "    }\n",
    "    \n",
    "    print(f\"起始文本: '{start_text}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {}\n",
    "    for strategy_name, params in strategies.items():\n",
    "        print(f\"\\n📝 {strategy_name}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        generated = temperature_sampling(\n",
    "            model, vocab, device, start_text, max_length, \n",
    "            temperature=params['temperature']\n",
    "        )\n",
    "        \n",
    "        results[strategy_name] = generated\n",
    "        print(generated)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 示例用法\n",
    "# results = compare_generation_strategies(\n",
    "#     'checkpoints/best_lstm_char.pt',\n",
    "#     'checkpoints/char_vocabulary.pkl',\n",
    "#     start_text=\"Once upon a time\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 隐藏状态可视化\n",
    "\n",
    "使用t-SNE可视化模型的隐藏状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def extract_hidden_states(model, vocab, device, texts, max_seq_len=50):\n",
    "    \"\"\"\n",
    "    提取模型的隐藏状态\n",
    "    \n",
    "    Args:\n",
    "        model: 训练好的模型\n",
    "        vocab: 词汇表\n",
    "        device: 计算设备\n",
    "        texts (list): 文本列表\n",
    "        max_seq_len (int): 最大序列长度\n",
    "        \n",
    "    Returns:\n",
    "        numpy.array: 隐藏状态矩阵\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hidden_states = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # 文本转索引\n",
    "            if isinstance(vocab, CharacterVocabulary):\n",
    "                indices = vocab.text_to_indices(text[:max_seq_len])\n",
    "            else:\n",
    "                words = text.split()[:max_seq_len]\n",
    "                indices = vocab.text_to_indices(words)\n",
    "            \n",
    "            if len(indices) < 5:  # 跳过太短的序列\n",
    "                continue\n",
    "                \n",
    "            # 转换为tensor\n",
    "            input_tensor = torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            \n",
    "            # 前向传播获取隐藏状态\n",
    "            if hasattr(model, 'get_hidden_states'):\n",
    "                hidden = model.get_hidden_states(input_tensor)\n",
    "            else:\n",
    "                # 手动提取最后一层的隐藏状态\n",
    "                embeddings = model.embedding(input_tensor)\n",
    "                if isinstance(model, SimpleLSTM):\n",
    "                    output, (hidden, cell) = model.lstm(embeddings)\n",
    "                    hidden = hidden[-1, 0, :]  # 取最后一层的隐藏状态\n",
    "                else:\n",
    "                    output, hidden = model.rnn(embeddings)\n",
    "                    hidden = hidden[-1, 0, :]  # 取最后一层的隐藏状态\n",
    "            \n",
    "            hidden_states.append(hidden.cpu().numpy())\n",
    "    \n",
    "    return np.array(hidden_states)\n",
    "\n",
    "def visualize_hidden_states(hidden_states, labels=None, method='tsne', save_path=None):\n",
    "    \"\"\"\n",
    "    可视化隐藏状态\n",
    "    \n",
    "    Args:\n",
    "        hidden_states (numpy.array): 隐藏状态矩阵\n",
    "        labels (list): 标签列表\n",
    "        method (str): 降维方法 ('tsne' 或 'pca')\n",
    "        save_path (str): 保存路径\n",
    "    \"\"\"\n",
    "    if method == 'tsne':\n",
    "        # 首先用PCA降维到50维，再用t-SNE\n",
    "        if hidden_states.shape[1] > 50:\n",
    "            pca = PCA(n_components=50)\n",
    "            hidden_states = pca.fit_transform(hidden_states)\n",
    "        \n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(hidden_states)-1))\n",
    "        embedded = reducer.fit_transform(hidden_states)\n",
    "        title = 't-SNE 隐藏状态可视化'\n",
    "    else:\n",
    "        reducer = PCA(n_components=2)\n",
    "        embedded = reducer.fit_transform(hidden_states)\n",
    "        title = 'PCA 隐藏状态可视化'\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    if labels is not None:\n",
    "        unique_labels = list(set(labels))\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for i, label in enumerate(unique_labels):\n",
    "            mask = np.array(labels) == label\n",
    "            plt.scatter(embedded[mask, 0], embedded[mask, 1], \n",
    "                       c=[colors[i]], label=label, alpha=0.7, s=50)\n",
    "        \n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        plt.scatter(embedded[:, 0], embedded[:, 1], alpha=0.7, s=50)\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 示例用法\n",
    "# texts = [\"Hello world\", \"Good morning\", \"How are you\", ...]\n",
    "# hidden_states = extract_hidden_states(model, vocab, device, texts)\n",
    "# visualize_hidden_states(hidden_states, method='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型参数分析\n",
    "\n",
    "分析模型权重分布和梯度情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_parameters(checkpoint_path):\n",
    "    \"\"\"\n",
    "    分析模型参数\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): 模型检查点路径\n",
    "    \"\"\"\n",
    "    # 加载检查点\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # 提取模型参数\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    \n",
    "    # 计算参数统计信息\n",
    "    param_stats = {}\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, param in state_dict.items():\n",
    "        param_tensor = param.detach().numpy()\n",
    "        param_stats[name] = {\n",
    "            'shape': param_tensor.shape,\n",
    "            'mean': np.mean(param_tensor),\n",
    "            'std': np.std(param_tensor),\n",
    "            'min': np.min(param_tensor),\n",
    "            'max': np.max(param_tensor),\n",
    "            'num_params': param_tensor.size\n",
    "        }\n",
    "        total_params += param_tensor.size\n",
    "    \n",
    "    # 可视化\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('模型参数分析', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 参数数量分布\n",
    "    layer_names = []\n",
    "    param_counts = []\n",
    "    \n",
    "    for name, stats in param_stats.items():\n",
    "        layer_names.append(name.split('.')[0])  # 取层名\n",
    "        param_counts.append(stats['num_params'])\n",
    "    \n",
    "    # 按层聚合\n",
    "    layer_param_counts = {}\n",
    "    for layer, count in zip(layer_names, param_counts):\n",
    "        layer_param_counts[layer] = layer_param_counts.get(layer, 0) + count\n",
    "    \n",
    "    axes[0, 0].bar(layer_param_counts.keys(), layer_param_counts.values())\n",
    "    axes[0, 0].set_title('各层参数数量')\n",
    "    axes[0, 0].set_ylabel('参数数量')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 参数均值分布\n",
    "    means = [stats['mean'] for stats in param_stats.values()]\n",
    "    axes[0, 1].hist(means, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_title('参数均值分布')\n",
    "    axes[0, 1].set_xlabel('参数均值')\n",
    "    axes[0, 1].set_ylabel('频次')\n",
    "    \n",
    "    # 参数标准差分布\n",
    "    stds = [stats['std'] for stats in param_stats.values()]\n",
    "    axes[1, 0].hist(stds, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axes[1, 0].set_title('参数标准差分布')\n",
    "    axes[1, 0].set_xlabel('参数标准差')\n",
    "    axes[1, 0].set_ylabel('频次')\n",
    "    \n",
    "    # 参数范围\n",
    "    param_names = list(param_stats.keys())[:10]  # 只显示前10个\n",
    "    mins = [param_stats[name]['min'] for name in param_names]\n",
    "    maxs = [param_stats[name]['max'] for name in param_names]\n",
    "    \n",
    "    x = np.arange(len(param_names))\n",
    "    axes[1, 1].bar(x, maxs, alpha=0.7, label='最大值')\n",
    "    axes[1, 1].bar(x, mins, alpha=0.7, label='最小值')\n",
    "    axes[1, 1].set_title('参数范围 (前10层)')\n",
    "    axes[1, 1].set_xlabel('层名')\n",
    "    axes[1, 1].set_ylabel('参数值')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels([name.split('.')[0] for name in param_names], rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印总结信息\n",
    "    print(f\"\\n📊 模型参数总结:\")\n",
    "    print(f\"总参数数量: {total_params:,}\")\n",
    "    print(f\"层数: {len(param_stats)}\")\n",
    "    \n",
    "    return param_stats\n",
    "\n",
    "# 示例用法\n",
    "# param_stats = analyze_model_parameters('checkpoints/best_lstm_char.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 生成质量评估\n",
    "\n",
    "评估生成文本的质量指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_text_metrics(text):\n",
    "    \"\"\"\n",
    "    计算文本质量指标\n",
    "    \n",
    "    Args:\n",
    "        text (str): 生成的文本\n",
    "        \n",
    "    Returns:\n",
    "        dict: 质量指标字典\n",
    "    \"\"\"\n",
    "    # 基础统计\n",
    "    char_count = len(text)\n",
    "    word_count = len(text.split())\n",
    "    sentence_count = len(re.findall(r'[.!?]+', text))\n",
    "    \n",
    "    # 词汇多样性\n",
    "    words = text.lower().split()\n",
    "    unique_words = len(set(words))\n",
    "    vocabulary_diversity = unique_words / max(word_count, 1)\n",
    "    \n",
    "    # 重复度分析\n",
    "    word_freq = Counter(words)\n",
    "    most_common = word_freq.most_common(5)\n",
    "    \n",
    "    # 平均词长\n",
    "    avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
    "    \n",
    "    # 平均句长\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    avg_sentence_length = np.mean([len(sent.split()) for sent in sentences if sent.strip()]) if sentences else 0\n",
    "    \n",
    "    return {\n",
    "        '字符数': char_count,\n",
    "        '词数': word_count,\n",
    "        '句数': sentence_count,\n",
    "        '词汇多样性': vocabulary_diversity,\n",
    "        '独特词数': unique_words,\n",
    "        '平均词长': avg_word_length,\n",
    "        '平均句长': avg_sentence_length,\n",
    "        '最常见词': most_common\n",
    "    }\n",
    "\n",
    "def compare_generation_quality(generated_texts):\n",
    "    \"\"\"\n",
    "    比较不同生成策略的文本质量\n",
    "    \n",
    "    Args:\n",
    "        generated_texts (dict): 不同策略生成的文本\n",
    "    \"\"\"\n",
    "    metrics_comparison = {}\n",
    "    \n",
    "    for strategy, text in generated_texts.items():\n",
    "        metrics = calculate_text_metrics(text)\n",
    "        metrics_comparison[strategy] = metrics\n",
    "    \n",
    "    # 可视化比较\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('生成文本质量对比', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    strategies = list(metrics_comparison.keys())\n",
    "    \n",
    "    # 词汇多样性\n",
    "    diversity_scores = [metrics_comparison[s]['词汇多样性'] for s in strategies]\n",
    "    axes[0, 0].bar(strategies, diversity_scores)\n",
    "    axes[0, 0].set_title('词汇多样性')\n",
    "    axes[0, 0].set_ylabel('多样性得分')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 平均词长\n",
    "    word_lengths = [metrics_comparison[s]['平均词长'] for s in strategies]\n",
    "    axes[0, 1].bar(strategies, word_lengths, color='orange')\n",
    "    axes[0, 1].set_title('平均词长')\n",
    "    axes[0, 1].set_ylabel('字符数')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 平均句长\n",
    "    sentence_lengths = [metrics_comparison[s]['平均句长'] for s in strategies]\n",
    "    axes[0, 2].bar(strategies, sentence_lengths, color='green')\n",
    "    axes[0, 2].set_title('平均句长')\n",
    "    axes[0, 2].set_ylabel('词数')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 词数\n",
    "    word_counts = [metrics_comparison[s]['词数'] for s in strategies]\n",
    "    axes[1, 0].bar(strategies, word_counts, color='red')\n",
    "    axes[1, 0].set_title('总词数')\n",
    "    axes[1, 0].set_ylabel('词数')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 独特词数\n",
    "    unique_word_counts = [metrics_comparison[s]['独特词数'] for s in strategies]\n",
    "    axes[1, 1].bar(strategies, unique_word_counts, color='purple')\n",
    "    axes[1, 1].set_title('独特词数')\n",
    "    axes[1, 1].set_ylabel('词数')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 句数\n",
    "    sentence_counts = [metrics_comparison[s]['句数'] for s in strategies]\n",
    "    axes[1, 2].bar(strategies, sentence_counts, color='brown')\n",
    "    axes[1, 2].set_title('句数')\n",
    "    axes[1, 2].set_ylabel('句数')\n",
    "    axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印详细对比\n",
    "    print(\"\\n📊 详细质量指标对比:\")\n",
    "    print(\"=\" * 80)\n",
    "    for strategy, metrics in metrics_comparison.items():\n",
    "        print(f\"\\n🎯 {strategy}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            if metric != '最常见词':\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"  {metric}: {value:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {metric}: {value}\")\n",
    "        print(f\"  最常见词: {metrics['最常见词'][:3]}\")\n",
    "    \n",
    "    return metrics_comparison\n",
    "\n",
    "# 示例用法\n",
    "# generated_texts = {\n",
    "#     '贪心': \"This is a sample text generated by greedy decoding.\",\n",
    "#     '温度采样': \"This is another sample text with more diversity and creativity.\"\n",
    "# }\n",
    "# quality_metrics = compare_generation_quality(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 使用示例\n",
    "\n",
    "以下是完整的分析流程示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置路径\n",
    "checkpoint_path = 'checkpoints/best_lstm_char.pt'\n",
    "vocab_path = 'checkpoints/char_vocabulary.pkl'\n",
    "history_path = 'checkpoints/training_history.json'\n",
    "\n",
    "# 检查文件是否存在\n",
    "from pathlib import Path\n",
    "if Path(checkpoint_path).exists():\n",
    "    print(\"✅ 找到模型文件，开始分析...\")\n",
    "    \n",
    "    # 1. 训练历史分析\n",
    "    if Path(history_path).exists():\n",
    "        plot_training_history(history_path)\n",
    "    \n",
    "    # 2. 生成策略对比\n",
    "    if Path(vocab_path).exists():\n",
    "        results = compare_generation_strategies(\n",
    "            checkpoint_path, vocab_path, \n",
    "            start_text=\"The quick brown fox\", \n",
    "            max_length=150\n",
    "        )\n",
    "        \n",
    "        # 3. 质量评估\n",
    "        quality_metrics = compare_generation_quality(results)\n",
    "    \n",
    "    # 4. 参数分析\n",
    "    param_stats = analyze_model_parameters(checkpoint_path)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 未找到模型文件，请先训练模型\")\n",
    "    print(\"运行: python train.py --model_type lstm --vocab_type char --epochs 10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}