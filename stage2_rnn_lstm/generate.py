#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬ç”Ÿæˆè„šæœ¬ - ä½¿ç”¨è®­ç»ƒå¥½çš„RNN/LSTMæ¨¡å‹ç”Ÿæˆæ–‡æœ¬

æ”¯æŒå¤šç§æ–‡æœ¬ç”Ÿæˆç­–ç•¥ï¼š
- è´ªå¿ƒè§£ç 
- éšæœºé‡‡æ ·  
- æ¸©åº¦é‡‡æ ·
- Top-ké‡‡æ ·
- Top-p (nucleus) é‡‡æ ·
"""

import torch
import torch.nn.functional as F
import argparse
import pickle
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from models.rnn import create_rnn_model
from models.lstm import SimpleLSTM, BiLSTM, LayerNormLSTMCell
from utils.text_data import CharacterVocabulary, WordVocabulary


def load_model_and_vocab(checkpoint_path, vocab_path):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œè¯æ±‡è¡¨
    
    Args:
        checkpoint_path (str): æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        vocab_path (str): è¯æ±‡è¡¨è·¯å¾„
        
    Returns:
        tuple: (model, vocabulary, device)
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    if not os.path.exists(vocab_path):
        raise FileNotFoundError(f"è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}")
    
    # åŠ è½½è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ä» {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # æå–æ¨¡å‹å‚æ•°
    model_config = checkpoint.get('model_config', {})
    model_type = model_config.get('model_type', 'lstm')
    vocab_type = model_config.get('vocab_type', 'char')
    
    # åŠ è½½è¯æ±‡è¡¨
    print(f"æ­£åœ¨åŠ è½½è¯æ±‡è¡¨ä» {vocab_path}")
    with open(vocab_path, 'rb') as f:
        vocab = pickle.load(f)
    
    # åˆ›å»ºæ¨¡å‹
    vocab_size = len(vocab)
    hidden_size = model_config.get('hidden_size', 256)
    num_layers = model_config.get('num_layers', 2)
    embedding_dim = model_config.get('embedding_dim', 128)
    dropout = model_config.get('dropout', 0.3)
    
    if model_type in ['rnn', 'gru']:
        model = create_rnn_model(
            model_type=model_type,
            vocab_size=vocab_size,
            embedding_dim=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout
        )
    else:  # lstm
        model = SimpleLSTM(
            vocab_size=vocab_size,
            embedding_dim=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout
        )
    
    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸ! è®¾å¤‡: {device}")
    print(f"æ¨¡å‹ç±»å‹: {model_type}, è¯æ±‡ç±»å‹: {vocab_type}")
    print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}, éšè—ç»´åº¦: {hidden_size}")
    
    return model, vocab, device


def greedy_decode(model, vocab, device, start_text="", max_length=200, end_token=None):
    """
    è´ªå¿ƒè§£ç  - æ¯æ¬¡é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        vocab: è¯æ±‡è¡¨
        device: è®¡ç®—è®¾å¤‡
        start_text (str): èµ·å§‹æ–‡æœ¬
        max_length (int): æœ€å¤§ç”Ÿæˆé•¿åº¦
        end_token (str): ç»“æŸç¬¦å·
        
    Returns:
        str: ç”Ÿæˆçš„æ–‡æœ¬
    """
    model.eval()
    
    # å‡†å¤‡èµ·å§‹åºåˆ—
    if start_text:
        if isinstance(vocab, CharacterVocabulary):
            tokens = vocab.text_to_indices(start_text)
        else:
            tokens = vocab.text_to_indices(start_text.split())
    else:
        tokens = [vocab.get_start_token()]
    
    with torch.no_grad():
        for _ in range(max_length):
            # å‡†å¤‡è¾“å…¥
            input_seq = torch.tensor(tokens[-100:], dtype=torch.long, device=device).unsqueeze(0)
            
            # å‰å‘ä¼ æ’­
            logits = model(input_seq)
            
            # è´ªå¿ƒé€‰æ‹©
            next_token = torch.argmax(logits[0, -1]).item()
            tokens.append(next_token)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if end_token and vocab.idx_to_token(next_token) == end_token:
                break
    
    # è½¬æ¢å›æ–‡æœ¬
    if isinstance(vocab, CharacterVocabulary):
        return vocab.indices_to_text(tokens)
    else:
        return ' '.join([vocab.idx_to_token(idx) for idx in tokens])


def temperature_sampling(model, vocab, device, start_text="", max_length=200, 
                        temperature=1.0, end_token=None):
    """
    æ¸©åº¦é‡‡æ · - ä½¿ç”¨æ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§
    
    Args:
        temperature (float): æ¸©åº¦å‚æ•°
            - < 1.0: æ›´ä¿å®ˆçš„é€‰æ‹©
            - = 1.0: æ ‡å‡†softmax  
            - > 1.0: æ›´éšæœºçš„é€‰æ‹©
    """
    model.eval()
    
    # å‡†å¤‡èµ·å§‹åºåˆ—
    if start_text:
        if isinstance(vocab, CharacterVocabulary):
            tokens = vocab.text_to_indices(start_text)
        else:
            tokens = vocab.text_to_indices(start_text.split())
    else:
        tokens = [vocab.get_start_token()]
    
    with torch.no_grad():
        for _ in range(max_length):
            # å‡†å¤‡è¾“å…¥
            input_seq = torch.tensor(tokens[-100:], dtype=torch.long, device=device).unsqueeze(0)
            
            # å‰å‘ä¼ æ’­
            logits = model(input_seq)
            
            # æ¸©åº¦ç¼©æ”¾
            scaled_logits = logits[0, -1] / temperature
            probabilities = F.softmax(scaled_logits, dim=-1)
            
            # é‡‡æ ·
            next_token = torch.multinomial(probabilities, 1).item()
            tokens.append(next_token)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if end_token and vocab.idx_to_token(next_token) == end_token:
                break
    
    # è½¬æ¢å›æ–‡æœ¬
    if isinstance(vocab, CharacterVocabulary):
        return vocab.indices_to_text(tokens)
    else:
        return ' '.join([vocab.idx_to_token(idx) for idx in tokens])


def top_k_sampling(model, vocab, device, start_text="", max_length=200, 
                   k=50, temperature=1.0, end_token=None):
    """
    Top-ké‡‡æ · - åªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„kä¸ªè¯
    
    Args:
        k (int): è€ƒè™‘çš„å€™é€‰è¯æ•°é‡
        temperature (float): æ¸©åº¦å‚æ•°
    """
    model.eval()
    
    # å‡†å¤‡èµ·å§‹åºåˆ—
    if start_text:
        if isinstance(vocab, CharacterVocabulary):
            tokens = vocab.text_to_indices(start_text)
        else:
            tokens = vocab.text_to_indices(start_text.split())
    else:
        tokens = [vocab.get_start_token()]
    
    with torch.no_grad():
        for _ in range(max_length):
            # å‡†å¤‡è¾“å…¥
            input_seq = torch.tensor(tokens[-100:], dtype=torch.long, device=device).unsqueeze(0)
            
            # å‰å‘ä¼ æ’­
            logits = model(input_seq)
            
            # æ¸©åº¦ç¼©æ”¾
            scaled_logits = logits[0, -1] / temperature
            
            # Top-kè¿‡æ»¤
            top_k_logits, top_k_indices = torch.topk(scaled_logits, k)
            probabilities = F.softmax(top_k_logits, dim=-1)
            
            # é‡‡æ ·
            sampled_index = torch.multinomial(probabilities, 1).item()
            next_token = top_k_indices[sampled_index].item()
            tokens.append(next_token)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if end_token and vocab.idx_to_token(next_token) == end_token:
                break
    
    # è½¬æ¢å›æ–‡æœ¬
    if isinstance(vocab, CharacterVocabulary):
        return vocab.indices_to_text(tokens)
    else:
        return ' '.join([vocab.idx_to_token(idx) for idx in tokens])


def top_p_sampling(model, vocab, device, start_text="", max_length=200, 
                   p=0.9, temperature=1.0, end_token=None):
    """
    Top-p (nucleus) é‡‡æ · - é€‰æ‹©ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°pçš„æœ€å°è¯é›†
    
    Args:
        p (float): ç´¯ç§¯æ¦‚ç‡é˜ˆå€¼ (0 < p <= 1)
        temperature (float): æ¸©åº¦å‚æ•°
    """
    model.eval()
    
    # å‡†å¤‡èµ·å§‹åºåˆ—
    if start_text:
        if isinstance(vocab, CharacterVocabulary):
            tokens = vocab.text_to_indices(start_text)
        else:
            tokens = vocab.text_to_indices(start_text.split())
    else:
        tokens = [vocab.get_start_token()]
    
    with torch.no_grad():
        for _ in range(max_length):
            # å‡†å¤‡è¾“å…¥
            input_seq = torch.tensor(tokens[-100:], dtype=torch.long, device=device).unsqueeze(0)
            
            # å‰å‘ä¼ æ’­
            logits = model(input_seq)
            
            # æ¸©åº¦ç¼©æ”¾
            scaled_logits = logits[0, -1] / temperature
            probabilities = F.softmax(scaled_logits, dim=-1)
            
            # Top-pè¿‡æ»¤
            sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)
            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
            
            # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡pçš„ä½ç½®
            sorted_indices_to_remove = cumulative_probs > p
            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
            sorted_indices_to_remove[0] = 0
            
            # è¿‡æ»¤æ¦‚ç‡
            filtered_probs = sorted_probs.clone()
            filtered_probs[sorted_indices_to_remove] = 0
            filtered_probs = filtered_probs / filtered_probs.sum()
            
            # é‡‡æ ·
            sampled_index = torch.multinomial(filtered_probs, 1).item()
            next_token = sorted_indices[sampled_index].item()
            tokens.append(next_token)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if end_token and vocab.idx_to_token(next_token) == end_token:
                break
    
    # è½¬æ¢å›æ–‡æœ¬
    if isinstance(vocab, CharacterVocabulary):
        return vocab.indices_to_text(tokens)
    else:
        return ' '.join([vocab.idx_to_token(idx) for idx in tokens])


def interactive_generation(model, vocab, device):
    """
    äº¤äº’å¼æ–‡æœ¬ç”Ÿæˆ - ç”¨æˆ·å¯ä»¥è¾“å…¥èµ·å§‹æ–‡æœ¬å¹¶é€‰æ‹©ç”Ÿæˆç­–ç•¥
    """
    print("\n" + "="*60)
    print("ğŸ­ äº¤äº’å¼æ–‡æœ¬ç”Ÿæˆ")
    print("="*60)
    
    while True:
        print("\nè¯·é€‰æ‹©ç”Ÿæˆç­–ç•¥:")
        print("1. è´ªå¿ƒè§£ç  (Greedy)")
        print("2. æ¸©åº¦é‡‡æ · (Temperature)")
        print("3. Top-k é‡‡æ ·")
        print("4. Top-p (Nucleus) é‡‡æ ·")
        print("0. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-4): ").strip()
        
        if choice == '0':
            break
        
        if choice not in ['1', '2', '3', '4']:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        # è·å–å‚æ•°
        start_text = input("\nè¯·è¾“å…¥èµ·å§‹æ–‡æœ¬ (ç•™ç©ºä½¿ç”¨é»˜è®¤): ").strip()
        try:
            max_length = int(input("ç”Ÿæˆé•¿åº¦ (é»˜è®¤200): ") or "200")
        except ValueError:
            max_length = 200
        
        print(f"\nğŸš€ æ­£åœ¨ç”Ÿæˆæ–‡æœ¬...")
        print("-" * 60)
        
        try:
            if choice == '1':
                # è´ªå¿ƒè§£ç 
                generated_text = greedy_decode(
                    model, vocab, device, start_text, max_length
                )
                print("ğŸ“ è´ªå¿ƒè§£ç ç»“æœ:")
                
            elif choice == '2':
                # æ¸©åº¦é‡‡æ ·
                try:
                    temperature = float(input("æ¸©åº¦å‚æ•° (é»˜è®¤0.8): ") or "0.8")
                except ValueError:
                    temperature = 0.8
                
                generated_text = temperature_sampling(
                    model, vocab, device, start_text, max_length, temperature
                )
                print(f"ğŸ“ æ¸©åº¦é‡‡æ ·ç»“æœ (T={temperature}):")
                
            elif choice == '3':
                # Top-ké‡‡æ ·
                try:
                    k = int(input("kå€¼ (é»˜è®¤50): ") or "50")
                    temperature = float(input("æ¸©åº¦å‚æ•° (é»˜è®¤0.8): ") or "0.8")
                except ValueError:
                    k = 50
                    temperature = 0.8
                
                generated_text = top_k_sampling(
                    model, vocab, device, start_text, max_length, k, temperature
                )
                print(f"ğŸ“ Top-ké‡‡æ ·ç»“æœ (k={k}, T={temperature}):")
                
            elif choice == '4':
                # Top-pé‡‡æ ·
                try:
                    p = float(input("på€¼ (é»˜è®¤0.9): ") or "0.9")
                    temperature = float(input("æ¸©åº¦å‚æ•° (é»˜è®¤0.8): ") or "0.8")
                except ValueError:
                    p = 0.9
                    temperature = 0.8
                
                generated_text = top_p_sampling(
                    model, vocab, device, start_text, max_length, p, temperature
                )
                print(f"ğŸ“ Top-pé‡‡æ ·ç»“æœ (p={p}, T={temperature}):")
            
            print(f"\n{generated_text}")
            print("-" * 60)
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")


def main():
    parser = argparse.ArgumentParser(description='RNN/LSTMæ–‡æœ¬ç”Ÿæˆè„šæœ¬')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--vocab', type=str, required=True,
                       help='è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--strategy', type=str, default='temperature',
                       choices=['greedy', 'temperature', 'top_k', 'top_p'],
                       help='ç”Ÿæˆç­–ç•¥')
    parser.add_argument('--start_text', type=str, default='',
                       help='èµ·å§‹æ–‡æœ¬')
    parser.add_argument('--max_length', type=int, default=200,
                       help='æœ€å¤§ç”Ÿæˆé•¿åº¦')
    parser.add_argument('--temperature', type=float, default=0.8,
                       help='æ¸©åº¦å‚æ•° (ç”¨äºtemperature, top_k, top_p)')
    parser.add_argument('--k', type=int, default=50,
                       help='Top-ké‡‡æ ·çš„kå€¼')
    parser.add_argument('--p', type=float, default=0.9,
                       help='Top-pé‡‡æ ·çš„på€¼')
    parser.add_argument('--interactive', action='store_true',
                       help='å¯ç”¨äº¤äº’æ¨¡å¼')
    parser.add_argument('--num_samples', type=int, default=1,
                       help='ç”Ÿæˆæ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½æ¨¡å‹å’Œè¯æ±‡è¡¨
        model, vocab, device = load_model_and_vocab(args.checkpoint, args.vocab)
        
        if args.interactive:
            # äº¤äº’å¼ç”Ÿæˆ
            interactive_generation(model, vocab, device)
        else:
            # æ‰¹é‡ç”Ÿæˆ
            print(f"\nğŸš€ ä½¿ç”¨ {args.strategy} ç­–ç•¥ç”Ÿæˆ {args.num_samples} ä¸ªæ ·æœ¬")
            print("="*60)
            
            for i in range(args.num_samples):
                print(f"\nğŸ“ æ ·æœ¬ {i+1}:")
                print("-" * 40)
                
                if args.strategy == 'greedy':
                    generated_text = greedy_decode(
                        model, vocab, device, args.start_text, args.max_length
                    )
                elif args.strategy == 'temperature':
                    generated_text = temperature_sampling(
                        model, vocab, device, args.start_text, args.max_length, 
                        args.temperature
                    )
                elif args.strategy == 'top_k':
                    generated_text = top_k_sampling(
                        model, vocab, device, args.start_text, args.max_length,
                        args.k, args.temperature
                    )
                elif args.strategy == 'top_p':
                    generated_text = top_p_sampling(
                        model, vocab, device, args.start_text, args.max_length,
                        args.p, args.temperature
                    )
                
                print(generated_text)
        
        print(f"\nâœ… ç”Ÿæˆå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())