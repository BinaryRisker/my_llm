"""
Gradioæ¼”ç¤ºç•Œé¢
=============

ä½¿ç”¨Gradioåˆ›å»ºäº¤äº’å¼Webç•Œé¢ï¼Œå±•ç¤ºé¡¹ç›®çš„å„ç§åŠŸèƒ½ï¼š
- æ–‡æœ¬ç”Ÿæˆå’Œåˆ†æ
- æ¨¡å‹å¯¹æ¯”
- åˆ†è¯å™¨æ¼”ç¤º
- æ•°æ®é¢„å¤„ç†å±•ç¤º
- å®éªŒç»“æœå¯è§†åŒ–

ä½¿ç”¨æ–¹æ³•:
    python web_interface/gradio_demo.py
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import tempfile
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

try:
    import gradio as gr
    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False
    gr = None

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    pd = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from utils.tokenizers.bpe_tokenizer import BPETokenizer
    from utils.data_processing.preprocessing_pipeline import DataPreprocessor
    from utils.experiment_tracking.mlflow_tracker import MLflowTracker
    from utils.hyperparameter_optimization.grid_search import GridSearchOptimizer
    MODULES_AVAILABLE = True
except ImportError as e:
    MODULES_AVAILABLE = False
    print(f"Warning: Could not import project modules: {e}")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œæ•°æ®
global_state = {
    'bpe_tokenizer': None,
    'preprocessor': None,
    'sample_data': [
        "Hello world! This is a great example of natural language processing.",
        "è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡ç¤ºä¾‹æ–‡æœ¬ï¼Œå±•ç¤ºå¤šè¯­è¨€å¤„ç†èƒ½åŠ›ã€‚",
        "Contact us at example@test.com or visit https://example.com for more info.",
        "<p>HTML content with <b>bold</b> tags should be cleaned.</p>",
        "Very very very long repeated characters!!!!!!!",
        "Short text",
        "Another example with    extra    whitespace    everywhere.",
        "æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚è‡ªç„¶è¨€èªå‡¦ç†ã®ãƒ‡ãƒ¢ã€‚",
    ]
}


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–é¡¹æ˜¯å¦å¯ç”¨"""
    missing_deps = []
    
    if not GRADIO_AVAILABLE:
        missing_deps.append("gradio")
    if not PANDAS_AVAILABLE:
        missing_deps.append("pandas") 
    if not NUMPY_AVAILABLE:
        missing_deps.append("numpy")
    if not MODULES_AVAILABLE:
        missing_deps.append("project modules")
    
    return missing_deps


def initialize_components():
    """åˆå§‹åŒ–ç»„ä»¶"""
    if not MODULES_AVAILABLE:
        return False
    
    try:
        # åˆå§‹åŒ–BPEåˆ†è¯å™¨
        global_state['bpe_tokenizer'] = BPETokenizer(vocab_size=100, min_frequency=1)
        
        # åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
        global_state['preprocessor'] = DataPreprocessor()
        global_state['preprocessor'].setup_default_pipeline(
            clean_text=True,
            detect_language=True,
            filter_by_length=True,
            min_length=2,
            max_length=100
        )
        
        logger.info("ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def train_bpe_tokenizer(texts: str, vocab_size: int) -> Tuple[str, str]:
    """è®­ç»ƒBPEåˆ†è¯å™¨"""
    try:
        if not texts.strip():
            return "âŒ è¯·è¾“å…¥è®­ç»ƒæ–‡æœ¬", ""
        
        # è§£æè¾“å…¥æ–‡æœ¬
        text_list = [line.strip() for line in texts.split('\n') if line.strip()]
        
        if len(text_list) == 0:
            return "âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ–‡æœ¬", ""
        
        # åˆ›å»ºå’Œè®­ç»ƒåˆ†è¯å™¨
        tokenizer = BPETokenizer(vocab_size=max(50, min(vocab_size, 1000)), min_frequency=1)
        tokenizer.train(text_list)
        
        # æ›´æ–°å…¨å±€çŠ¶æ€
        global_state['bpe_tokenizer'] = tokenizer
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""âœ… BPEåˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼

ğŸ“Š è®­ç»ƒç»Ÿè®¡:
- è®­ç»ƒæ–‡æœ¬æ•°é‡: {len(text_list)}
- ç›®æ ‡è¯æ±‡è¡¨å¤§å°: {vocab_size}
- å®é™…è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}

ğŸ“– è¯æ±‡è¡¨ç¤ºä¾‹ (å‰20ä¸ª):"""
        
        vocab = tokenizer.get_vocab()
        for i, (token, id_) in enumerate(list(vocab.items())[:20]):
            report += f"\n  {id_:3d}: '{token}'"
        
        return report, "åˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼Œå¯ä»¥åœ¨ä¸‹æ–¹æµ‹è¯•ç¼–ç åŠŸèƒ½"
        
    except Exception as e:
        return f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}", ""


def test_bpe_encoding(text: str) -> str:
    """æµ‹è¯•BPEç¼–ç """
    try:
        if not global_state['bpe_tokenizer']:
            return "âŒ è¯·å…ˆè®­ç»ƒBPEåˆ†è¯å™¨"
        
        if not text.strip():
            return "âŒ è¯·è¾“å…¥è¦ç¼–ç çš„æ–‡æœ¬"
        
        tokenizer = global_state['bpe_tokenizer']
        
        # ç¼–ç 
        token_ids = tokenizer.encode(text, add_special_tokens=True)
        
        # è§£ç éªŒè¯
        decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)
        
        # ç”Ÿæˆç»“æœ
        result = f"""ğŸ”¤ BPEç¼–ç ç»“æœ:

ğŸ“ åŸæ–‡: {text}
ğŸ”¢ Token IDs: {token_ids}
ğŸ“„ è§£ç ç»“æœ: {decoded_text}

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
- åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦
- Tokenæ•°é‡: {len(token_ids)}
- å‹ç¼©æ¯”: {len(token_ids) / len(text.split()) if text.split() else 0:.2f} tokens/word"""
        
        return result
        
    except Exception as e:
        return f"âŒ ç¼–ç å¤±è´¥: {str(e)}"


def preprocess_data(texts: str, clean_text: bool, detect_language: bool, 
                   filter_length: bool, min_length: int, max_length: int) -> Tuple[str, str]:
    """æ•°æ®é¢„å¤„ç†æ¼”ç¤º"""
    try:
        if not texts.strip():
            return "âŒ è¯·è¾“å…¥è¦å¤„ç†çš„æ–‡æœ¬", ""
        
        # è§£æè¾“å…¥
        text_list = [line.strip() for line in texts.split('\n') if line.strip()]
        
        if not text_list:
            return "âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®", ""
        
        # è®¾ç½®é¢„å¤„ç†ç®¡é“
        preprocessor = DataPreprocessor()
        preprocessor.clear_steps()
        
        if clean_text:
            preprocessor.add_step('clean_text')
        if detect_language:
            preprocessor.add_step('detect_language')
        if filter_length:
            preprocessor.add_step('filter_by_length', 
                                min_length=min_length, max_length=max_length)
        preprocessor.add_step('compute_stats')
        
        # æ‰§è¡Œé¢„å¤„ç†
        processed_data, stats = preprocessor.process(text_list, return_stats=True)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""ğŸ“Š æ•°æ®é¢„å¤„ç†ç»“æœ:

ğŸ“ˆ å¤„ç†ç»Ÿè®¡:
- åŸå§‹æ ·æœ¬æ•°: {stats['original_count']}
- æœ€ç»ˆæ ·æœ¬æ•°: {stats['final_count']}
- è¿‡æ»¤ç‡: {(1 - stats['final_count'] / stats['original_count']) * 100:.1f}%

ğŸ“‹ å¤„ç†æ­¥éª¤è¯¦æƒ…:"""
        
        for step_name, step_stats in stats['step_results'].items():
            report += f"\n\nğŸ”¸ {step_name}:"
            if isinstance(step_stats, dict):
                for key, value in step_stats.items():
                    if isinstance(value, dict) and len(value) <= 10:
                        report += f"\n  {key}: {value}"
                    else:
                        report += f"\n  {key}: {value}"
        
        # æ˜¾ç¤ºå¤„ç†åçš„æ•°æ®
        processed_display = ""
        for i, text in enumerate(processed_data[:10]):
            display_text = text if isinstance(text, str) else text.get('text', str(text))
            processed_display += f"{i+1}. {display_text}\n"
        
        if len(processed_data) > 10:
            processed_display += f"... (å…± {len(processed_data)} æ¡)"
        
        return report, processed_display
        
    except Exception as e:
        return f"âŒ é¢„å¤„ç†å¤±è´¥: {str(e)}", ""


def compare_models() -> str:
    """æ¨¡å‹å¯¹æ¯”æ¼”ç¤º"""
    try:
        # æ¨¡æ‹Ÿæ¨¡å‹å¯¹æ¯”æ•°æ®
        models = ['MLP', 'LSTM', 'Attention', 'Transformer', 'GPT']
        metrics = {
            'BLEU Score': [15.2, 23.4, 28.7, 35.2, 38.9],
            'Perplexity': [85.4, 42.1, 28.3, 18.7, 12.4],
            'Training Time (hours)': [0.5, 2.1, 4.5, 8.2, 15.6]
        }
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        report = "ğŸ† æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n\n"
        report += "| æ¨¡å‹ | BLEU Score | Perplexity | Training Time (h) |\n"
        report += "|------|------------|------------|-------------------|\n"
        
        for i, model in enumerate(models):
            report += f"| {model} | {metrics['BLEU Score'][i]:.1f} | {metrics['Perplexity'][i]:.1f} | {metrics['Training Time (hours)'][i]:.1f} |\n"
        
        report += "\nğŸ“ˆ æ€§èƒ½è¶‹åŠ¿:\n"
        report += "- BLEU Score: éšç€æ¨¡å‹å¤æ‚åº¦å¢åŠ è€Œæå‡\n"
        report += "- Perplexity: å‘ˆç°ä¸‹é™è¶‹åŠ¿ï¼Œæ¨¡å‹è¡¨ç°è¶Šæ¥è¶Šå¥½\n"
        report += "- Training Time: å¤æ‚æ¨¡å‹éœ€è¦æ›´é•¿è®­ç»ƒæ—¶é—´\n"
        
        report += "\nğŸ¯ æ¨èå»ºè®®:\n"
        report += "- å¿«é€ŸåŸå‹: é€‰æ‹©MLPæˆ–LSTM\n"
        report += "- å¹³è¡¡æ€§èƒ½: é€‰æ‹©Attentionæˆ–Transformer\n"
        report += "- æœ€ä½³æ•ˆæœ: é€‰æ‹©GPT (å¦‚æœ‰å……è¶³èµ„æº)"
        
        return report
        
    except Exception as e:
        return f"âŒ å¯¹æ¯”ç”Ÿæˆå¤±è´¥: {str(e)}"


def hyperparameter_optimization_demo(param_ranges: str) -> str:
    """è¶…å‚æ•°ä¼˜åŒ–æ¼”ç¤º"""
    try:
        if not param_ranges.strip():
            return "âŒ è¯·è¾“å…¥å‚æ•°èŒƒå›´ï¼Œæ ¼å¼ï¼šlearning_rate=[0.001,0.01,0.1]"
        
        # è§£æå‚æ•°èŒƒå›´
        param_grid = {}
        try:
            for line in param_ranges.strip().split('\n'):
                if '=' in line:
                    key, value = line.split('=', 1)
                    param_grid[key.strip()] = eval(value.strip())
        except:
            return "âŒ å‚æ•°æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨æ­£ç¡®çš„Pythonåˆ—è¡¨æ ¼å¼"
        
        if not param_grid:
            return "âŒ æ²¡æœ‰æœ‰æ•ˆçš„å‚æ•°å®šä¹‰"
        
        # æ¨¡æ‹Ÿä¼˜åŒ–è¿‡ç¨‹
        def mock_objective(params):
            import random
            # æ¨¡æ‹Ÿä¸€äº›å‚æ•°çš„å½±å“
            score = 0.8
            for key, value in params.items():
                if 'learning_rate' in key:
                    score -= abs(value - 0.01) * 10
                elif 'batch_size' in key:
                    score -= abs(value - 64) * 0.001
            
            score += random.uniform(-0.1, 0.1)
            return max(0.1, score)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = GridSearchOptimizer(n_jobs=1, verbose=False)
        
        # è¿è¡Œæœç´¢
        result = optimizer.search(
            param_grid=param_grid,
            objective_function=mock_objective,
            maximize=True
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""ğŸ” è¶…å‚æ•°ä¼˜åŒ–ç»“æœ:

ğŸ† æœ€ä½³é…ç½®:
"""
        for key, value in result.best_params.items():
            report += f"  {key}: {value}\n"
        
        report += f"""
ğŸ“Š ä¼˜åŒ–ç»Ÿè®¡:
- æœ€ä½³å¾—åˆ†: {result.best_score:.4f}
- æ€»è¯•éªŒæ¬¡æ•°: {result.total_trials}
- æˆåŠŸè¯•éªŒ: {result.successful_trials}
- æœç´¢æ—¶é—´: {result.search_time:.2f}ç§’

ğŸ… å‰3åç»“æœ:"""
        
        top_results = optimizer.get_top_results(n=3)
        for i, trial in enumerate(top_results):
            report += f"\n  {i+1}. å¾—åˆ†: {trial.score:.4f}, å‚æ•°: {trial.params}"
        
        return report
        
    except Exception as e:
        return f"âŒ ä¼˜åŒ–å¤±è´¥: {str(e)}"


def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    # æ£€æŸ¥ä¾èµ–
    missing_deps = check_dependencies()
    if missing_deps:
        def show_error():
            return f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing_deps)}\nè¯·è¿è¡Œ: pip install {' '.join(missing_deps)}"
        
        with gr.Blocks(title="ä¾èµ–ç¼ºå¤±") as demo:
            gr.Markdown("# âŒ ä¾èµ–åŒ…ç¼ºå¤±")
            gr.Textbox(value=show_error(), interactive=False)
        
        return demo
    
    # åˆå§‹åŒ–ç»„ä»¶
    init_success = initialize_components()
    if not init_success:
        def show_init_error():
            return "âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é¡¹ç›®æ¨¡å—æ˜¯å¦æ­£ç¡®å®‰è£…"
        
        with gr.Blocks(title="åˆå§‹åŒ–å¤±è´¥") as demo:
            gr.Markdown("# âŒ åˆå§‹åŒ–å¤±è´¥")  
            gr.Textbox(value=show_init_error(), interactive=False)
        
        return demo
    
    # åˆ›å»ºä¸»ç•Œé¢
    with gr.Blocks(
        title="LLMå­¦ä¹ é¡¹ç›®æ¼”ç¤º",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        .gr-button {
            background: linear-gradient(45deg, #FF6B6B, #4ECDC4);
            border: none;
            color: white;
        }
        """
    ) as demo:
        
        gr.Markdown("""
        # ğŸš€ å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ é¡¹ç›® - äº¤äº’å¼æ¼”ç¤º
        
        æ¬¢è¿ä½¿ç”¨æˆ‘ä»¬çš„LLMå­¦ä¹ é¡¹ç›®æ¼”ç¤ºï¼è¿™é‡ŒåŒ…å«äº†é¡¹ç›®çš„å„ç§æ ¸å¿ƒåŠŸèƒ½ã€‚
        """)
        
        with gr.Tabs():
            
            # Tab 1: BPEåˆ†è¯å™¨
            with gr.TabItem("ğŸ”¤ BPEåˆ†è¯å™¨"):
                gr.Markdown("## BPE (Byte Pair Encoding) åˆ†è¯å™¨æ¼”ç¤º")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### 1. è®­ç»ƒåˆ†è¯å™¨")
                        train_texts = gr.Textbox(
                            label="è®­ç»ƒæ–‡æœ¬ (æ¯è¡Œä¸€ä¸ªå¥å­)",
                            placeholder="Hello world!\nè¿™æ˜¯ä¸­æ–‡ç¤ºä¾‹\nMore training text...",
                            lines=8,
                            value="\n".join(global_state['sample_data'][:6])
                        )
                        vocab_size = gr.Slider(
                            minimum=50, maximum=1000, value=200, step=50,
                            label="è¯æ±‡è¡¨å¤§å°"
                        )
                        train_btn = gr.Button("è®­ç»ƒBPEåˆ†è¯å™¨", variant="primary")
                    
                    with gr.Column():
                        train_output = gr.Textbox(
                            label="è®­ç»ƒç»“æœ", 
                            lines=15,
                            max_lines=15
                        )
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### 2. æµ‹è¯•ç¼–ç ")
                        test_text = gr.Textbox(
                            label="æµ‹è¯•æ–‡æœ¬",
                            placeholder="è¾“å…¥è¦ç¼–ç çš„æ–‡æœ¬...",
                            value="Hello BPE tokenizer! è¿™æ˜¯æµ‹è¯•æ–‡æœ¬ã€‚"
                        )
                        encode_btn = gr.Button("ç¼–ç æµ‹è¯•")
                    
                    with gr.Column():
                        encode_output = gr.Textbox(
                            label="ç¼–ç ç»“æœ",
                            lines=10
                        )
                
                train_btn.click(
                    fn=train_bpe_tokenizer,
                    inputs=[train_texts, vocab_size],
                    outputs=[train_output, gr.Textbox(visible=False)]
                )
                
                encode_btn.click(
                    fn=test_bpe_encoding,
                    inputs=[test_text],
                    outputs=[encode_output]
                )
            
            # Tab 2: æ•°æ®é¢„å¤„ç†
            with gr.TabItem("ğŸ§¹ æ•°æ®é¢„å¤„ç†"):
                gr.Markdown("## æ•°æ®é¢„å¤„ç†ç®¡é“æ¼”ç¤º")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### è¾“å…¥æ•°æ®")
                        input_texts = gr.Textbox(
                            label="åŸå§‹æ–‡æœ¬ (æ¯è¡Œä¸€ä¸ª)",
                            lines=8,
                            value="\n".join(global_state['sample_data'])
                        )
                        
                        gr.Markdown("### é¢„å¤„ç†é€‰é¡¹")
                        clean_text = gr.Checkbox(label="æ–‡æœ¬æ¸…æ´—", value=True)
                        detect_language = gr.Checkbox(label="è¯­è¨€æ£€æµ‹", value=True)
                        filter_length = gr.Checkbox(label="é•¿åº¦è¿‡æ»¤", value=True)
                        
                        with gr.Row():
                            min_length = gr.Number(label="æœ€å°é•¿åº¦", value=2)
                            max_length = gr.Number(label="æœ€å¤§é•¿åº¦", value=100)
                        
                        process_btn = gr.Button("å¼€å§‹é¢„å¤„ç†", variant="primary")
                    
                    with gr.Column():
                        gr.Markdown("### å¤„ç†ç»“æœ")
                        process_stats = gr.Textbox(
                            label="å¤„ç†ç»Ÿè®¡",
                            lines=15
                        )
                        
                        process_data = gr.Textbox(
                            label="å¤„ç†åæ•°æ®",
                            lines=10
                        )
                
                process_btn.click(
                    fn=preprocess_data,
                    inputs=[input_texts, clean_text, detect_language, 
                           filter_length, min_length, max_length],
                    outputs=[process_stats, process_data]
                )
            
            # Tab 3: æ¨¡å‹å¯¹æ¯”
            with gr.TabItem("ğŸ“Š æ¨¡å‹å¯¹æ¯”"):
                gr.Markdown("## æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ")
                
                with gr.Row():
                    with gr.Column():
                        compare_btn = gr.Button("ç”Ÿæˆæ¨¡å‹å¯¹æ¯”", variant="primary")
                        gr.Markdown("""
                        ### å¯¹æ¯”è¯´æ˜
                        - **MLP**: å¤šå±‚æ„ŸçŸ¥æœºï¼ŒåŸºç¡€ç¥ç»ç½‘ç»œ
                        - **LSTM**: é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼Œå¤„ç†åºåˆ—æ•°æ®
                        - **Attention**: æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ¨æ€æƒé‡åˆ†é…
                        - **Transformer**: è‡ªæ³¨æ„åŠ›æ¶æ„ï¼Œç°ä»£NLPåŸºçŸ³
                        - **GPT**: ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ€å…ˆè¿›çš„æ¶æ„
                        """)
                    
                    with gr.Column():
                        compare_output = gr.Textbox(
                            label="å¯¹æ¯”ç»“æœ",
                            lines=20
                        )
                
                compare_btn.click(
                    fn=compare_models,
                    outputs=[compare_output]
                )
            
            # Tab 4: è¶…å‚æ•°ä¼˜åŒ–
            with gr.TabItem("ğŸ” è¶…å‚æ•°ä¼˜åŒ–"):
                gr.Markdown("## è¶…å‚æ•°ä¼˜åŒ–æ¼”ç¤º")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### å‚æ•°æœç´¢ç©ºé—´")
                        param_input = gr.Textbox(
                            label="å‚æ•°å®šä¹‰ (Pythonæ ¼å¼)",
                            lines=5,
                            value="""learning_rate=[0.001, 0.01, 0.1]
batch_size=[16, 32, 64]
hidden_size=[128, 256, 512]""",
                            placeholder="param_name=[value1, value2, value3]"
                        )
                        
                        optimize_btn = gr.Button("å¼€å§‹ä¼˜åŒ–æœç´¢", variant="primary")
                        
                        gr.Markdown("""
                        ### ä½¿ç”¨è¯´æ˜
                        1. æ¯è¡Œå®šä¹‰ä¸€ä¸ªå‚æ•°
                        2. æ ¼å¼ï¼šå‚æ•°å=[å€¼1,å€¼2,å€¼3]
                        3. æ”¯æŒæ•°å­—å’Œå­—ç¬¦ä¸²å€¼
                        4. ç³»ç»Ÿä¼šè‡ªåŠ¨æœç´¢æœ€ä½³ç»„åˆ
                        """)
                    
                    with gr.Column():
                        optimize_output = gr.Textbox(
                            label="ä¼˜åŒ–ç»“æœ",
                            lines=20
                        )
                
                optimize_btn.click(
                    fn=hyperparameter_optimization_demo,
                    inputs=[param_input],
                    outputs=[optimize_output]
                )
            
            # Tab 5: é¡¹ç›®ä¿¡æ¯
            with gr.TabItem("â„¹ï¸ é¡¹ç›®ä¿¡æ¯"):
                gr.Markdown("""
                ## ğŸ“ å…³äºæœ¬é¡¹ç›®
                
                è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ é¡¹ç›®ï¼ŒåŒ…å«ä»åŸºç¡€ç¥ç»ç½‘ç»œåˆ°GPTçš„å®Œæ•´å®ç°è·¯å¾„ã€‚
                
                ### ğŸ—ï¸ é¡¹ç›®ç‰¹è‰²
                - ğŸ“š **5ä¸ªæ¸è¿›é˜¶æ®µ**: MLP â†’ RNN/LSTM â†’ Attention â†’ Transformer â†’ GPT
                - ğŸ”§ **å®Œæ•´å·¥å…·é“¾**: åˆ†è¯å™¨ã€é¢„å¤„ç†ã€è®­ç»ƒã€è¯„ä¼°ã€ä¼˜åŒ–
                - ğŸ“Š **å®éªŒç®¡ç†**: MLflowé›†æˆã€è¶…å‚æ•°æœç´¢ã€ç»“æœå¯¹æ¯”
                - ğŸŒ **Webç•Œé¢**: Gradio/Streamlitäº¤äº’å¼æ¼”ç¤º
                - ğŸ“– **è¯¦ç»†æ–‡æ¡£**: ç†è®ºè§£é‡Šã€ä»£ç æ³¨é‡Šã€ä½¿ç”¨æŒ‡å—
                
                ### ğŸš€ å¿«é€Ÿå¼€å§‹
                1. **ç¯å¢ƒé…ç½®**: `python scripts/setup_environment.py --full`
                2. **æ•°æ®ä¸‹è½½**: `python scripts/download_datasets.py --all`
                3. **æ¨¡å‹è®­ç»ƒ**: `python training/stage4_transformer/train_transformer.py`
                4. **Webæ¼”ç¤º**: `python web_interface/gradio_demo.py`
                
                ### ğŸ“ é¡¹ç›®ç»“æ„
                ```
                my_llm/
                â”œâ”€â”€ models/           # æ¨¡å‹å®ç°
                â”œâ”€â”€ training/         # è®­ç»ƒè„šæœ¬
                â”œâ”€â”€ evaluation/       # è¯„ä¼°å·¥å…·
                â”œâ”€â”€ utils/           # å·¥å…·æ¨¡å—
                â”œâ”€â”€ web_interface/   # Webç•Œé¢
                â”œâ”€â”€ data/            # æ•°æ®é›†
                â””â”€â”€ experiments/     # å®éªŒç»“æœ
                ```
                
                ### ğŸ”— ç›¸å…³èµ„æº
                - ğŸ“– [ROADMAP.md](../ROADMAP.md) - å®Œæ•´å¼€å‘è·¯çº¿å›¾
                - ğŸ“‹ [TODO_IMPROVEMENTS.md](../TODO_IMPROVEMENTS.md) - æ”¹è¿›è®¡åˆ’
                - ğŸ—‚ï¸ [docs/](../docs/) - ç†è®ºæ–‡æ¡£
                
                ### ğŸ¤ å‚ä¸è´¡çŒ®
                æ¬¢è¿æäº¤Issueså’ŒPull Requestsï¼
                """)
        
        gr.Markdown("""
        ---
        ğŸ’¡ **æç¤º**: è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºç•Œé¢ï¼Œå±•ç¤ºäº†é¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½ã€‚å®Œæ•´åŠŸèƒ½è¯·å‚è€ƒé¡¹ç›®æ–‡æ¡£ã€‚
        """)
    
    return demo


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨Gradioæ¼”ç¤ºç•Œé¢...")
    
    if not GRADIO_AVAILABLE:
        print("âŒ Gradioæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install gradio")
        return
    
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    try:
        demo.launch(
            server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
            server_port=7860,       # é»˜è®¤ç«¯å£
            share=False,            # ä¸åˆ›å»ºå…¬å…±é“¾æ¥
            debug=True,             # è°ƒè¯•æ¨¡å¼
            show_error=True,        # æ˜¾ç¤ºé”™è¯¯
            quiet=False             # ä¸é™é»˜
        )
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("ğŸ’¡ å°è¯•ä½¿ç”¨ä¸åŒç«¯å£æˆ–æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")


if __name__ == "__main__":
    main()