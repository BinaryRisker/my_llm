# é˜¶æ®µ3ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸åºåˆ—åˆ°åºåˆ—æ¨¡å‹ (Seq2Seq)

## ğŸ“‹ å­¦ä¹ ç›®æ ‡

åœ¨é˜¶æ®µ3ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢ç´¢æ³¨æ„åŠ›æœºåˆ¶å’Œåºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œè¿™äº›æŠ€æœ¯ä¸ºåç»­ç†è§£Transformeræ¶æ„å¥ å®šåŸºç¡€ã€‚é€šè¿‡æœ¬é˜¶æ®µå­¦ä¹ ï¼Œæ‚¨å°†æŒæ¡ï¼š

- æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³å’Œè®¡ç®—æ–¹æ³•
- Bahdanauæ³¨æ„åŠ›ä¸Luongæ³¨æ„åŠ›çš„åŒºåˆ«
- Seq2Seqæ¶æ„çš„ç¼–ç å™¨-è§£ç å™¨è®¾è®¡
- æœºå™¨ç¿»è¯‘ä»»åŠ¡çš„å®ç°ä¸è¯„ä¼°
- æ³¨æ„åŠ›å¯è§†åŒ–ä¸è§£é‡Šæ€§åˆ†æ

## ğŸ§  ç†è®ºåŸºç¡€

### 1. åºåˆ—åˆ°åºåˆ—æ¨¡å‹æ¦‚è¿°

**ä¼ ç»ŸRNNçš„å±€é™æ€§ï¼š**
- è¾“å…¥åºåˆ—è¢«å‹ç¼©åˆ°å›ºå®šé•¿åº¦çš„éšè—çŠ¶æ€å‘é‡
- é•¿åºåˆ—å®¹æ˜“ä¸¢å¤±å‰æœŸä¿¡æ¯ï¼ˆä¿¡æ¯ç“¶é¢ˆï¼‰
- æ— æ³•åŠ¨æ€å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†

**Seq2Seqæ¨¡å‹çš„ä¼˜åŠ¿ï¼š**
```
ç¼–ç å™¨ (Encoder)ï¼š
Input: xâ‚, xâ‚‚, ..., xâ‚™ â†’ Hidden States: hâ‚, hâ‚‚, ..., hâ‚™

è§£ç å™¨ (Decoder)ï¼š
Hidden States + Context â†’ Output: yâ‚, yâ‚‚, ..., yâ‚˜
```

### 2. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)

#### 2.1 æ ¸å¿ƒæ€æƒ³

æ³¨æ„åŠ›æœºåˆ¶å…è®¸è§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¾“å‡ºè¯æ—¶ï¼ŒåŠ¨æ€åœ°"å…³æ³¨"è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ä»…ä¾èµ–äºç¼–ç å™¨çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚

**æ•°å­¦è¡¨ç¤ºï¼š**
```
Context Vector: cáµ¢ = Î£â±¼ Î±áµ¢â±¼ * hâ±¼
Attention Weights: Î±áµ¢â±¼ = softmax(eáµ¢â±¼)
Attention Energy: eáµ¢â±¼ = f(sáµ¢â‚‹â‚, hâ±¼)
```

å…¶ä¸­ï¼š
- `sáµ¢â‚‹â‚`ï¼šè§£ç å™¨åœ¨æ—¶åˆ»i-1çš„éšè—çŠ¶æ€
- `hâ±¼`ï¼šç¼–ç å™¨åœ¨æ—¶åˆ»jçš„éšè—çŠ¶æ€
- `Î±áµ¢â±¼`ï¼šæ³¨æ„åŠ›æƒé‡ï¼Œè¡¨ç¤ºç”Ÿæˆç¬¬iä¸ªè¾“å‡ºæ—¶å¯¹ç¬¬jä¸ªè¾“å…¥çš„å…³æ³¨ç¨‹åº¦
- `cáµ¢`ï¼šä¸Šä¸‹æ–‡å‘é‡ï¼Œè¾“å…¥ä¿¡æ¯çš„åŠ æƒå¹³å‡

#### 2.2 Bahdanauæ³¨æ„åŠ› (Additive Attention)

**æå‡ºèƒŒæ™¯ï¼š** Bahdanauç­‰äººåœ¨2014å¹´æå‡ºï¼Œé¦–æ¬¡åœ¨ç¥ç»æœºå™¨ç¿»è¯‘ä¸­å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ã€‚

**è®¡ç®—æ–¹å¼ï¼š**
```python
# åŠ æ€§æ³¨æ„åŠ›
e_ij = v_a^T * tanh(W_a * s_{i-1} + U_a * h_j)
alpha_ij = softmax(e_ij)
c_i = sum(alpha_ij * h_j)
```

**ç‰¹ç‚¹ï¼š**
- ä½¿ç”¨å‰é¦ˆç¥ç»ç½‘ç»œè®¡ç®—æ³¨æ„åŠ›å¾—åˆ†
- å‚æ•°é‡ï¼š`v_a`, `W_a`, `U_a`
- è®¡ç®—å¤æ‚åº¦ç›¸å¯¹è¾ƒé«˜ï¼Œä½†è¡¨è¾¾èƒ½åŠ›å¼º

#### 2.3 Luongæ³¨æ„åŠ› (Multiplicative Attention)

**æå‡ºèƒŒæ™¯ï¼š** Luongç­‰äººåœ¨2015å¹´æå‡ºï¼Œç®€åŒ–äº†æ³¨æ„åŠ›çš„è®¡ç®—è¿‡ç¨‹ã€‚

**ä¸‰ç§è®¡ç®—æ–¹å¼ï¼š**

1. **Dot Productï¼ˆç‚¹ç§¯ï¼‰ï¼š**
   ```python
   e_ij = s_{i-1}^T * h_j
   ```

2. **Generalï¼ˆé€šç”¨ï¼‰ï¼š**
   ```python
   e_ij = s_{i-1}^T * W_a * h_j
   ```

3. **Concatï¼ˆè¿æ¥ï¼‰ï¼š**
   ```python
   e_ij = v_a^T * tanh(W_a * [s_{i-1}; h_j])
   ```

**ç‰¹ç‚¹ï¼š**
- è®¡ç®—æ•ˆç‡æ›´é«˜
- å‚æ•°é‡ç›¸å¯¹è¾ƒå°‘
- åœ¨å®è·µä¸­å¹¿æ³›ä½¿ç”¨

#### 2.4 æ³¨æ„åŠ›å¯¹é½ (Attention Alignment)

æ³¨æ„åŠ›æƒé‡`Î±áµ¢â±¼`å½¢æˆäº†ä¸€ä¸ªå¯¹é½çŸ©é˜µï¼Œå¯è§†åŒ–åœ°å±•ç¤ºè¾“å…¥è¾“å‡ºåºåˆ—ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼š

```
      Source: "I love machine learning"
Target   I    love  machine  learning
æˆ‘      0.8   0.1    0.05     0.05
å–œæ¬¢    0.1   0.7    0.1      0.1  
æœºå™¨    0.05  0.05   0.8      0.1
å­¦ä¹     0.05  0.05   0.1      0.8
```

### 3. Seq2Seqæ¶æ„è¯¦è§£

#### 3.1 ç¼–ç å™¨ (Encoder)

**èŒè´£ï¼š** å°†å˜é•¿è¾“å…¥åºåˆ—ç¼–ç ä¸ºå›ºå®šé•¿åº¦çš„è¡¨ç¤º

```python
class Encoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, 
                           batch_first=True, bidirectional=True)
    
    def forward(self, x, lengths):
        embedded = self.embedding(x)
        packed = pack_padded_sequence(embedded, lengths, batch_first=True)
        output, (hidden, cell) = self.lstm(packed)
        output, _ = pad_packed_sequence(output, batch_first=True)
        return output, (hidden, cell)
```

**å…³é”®æŠ€æœ¯ï¼š**
- **åŒå‘LSTMï¼š** åŒæ—¶æ•è·å‰å‘å’Œåå‘ä¿¡æ¯
- **Sequence Packingï¼š** å¤„ç†å˜é•¿åºåˆ—ï¼Œæé«˜è®¡ç®—æ•ˆç‡
- **å¤šå±‚ç»“æ„ï¼š** å¢å¼ºè¡¨ç¤ºèƒ½åŠ›

#### 3.2 è§£ç å™¨ (Decoder)

**èŒè´£ï¼š** åŸºäºç¼–ç å™¨çš„è¾“å‡ºå’Œæ³¨æ„åŠ›æœºåˆ¶ç”Ÿæˆç›®æ ‡åºåˆ—

```python
class AttentionDecoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, attention):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size + hidden_size*2, hidden_size, 
                           num_layers, batch_first=True)
        self.attention = attention
        self.output_proj = nn.Linear(hidden_size, vocab_size)
    
    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input)
        context = self.attention(hidden[0][-1], encoder_outputs)
        lstm_input = torch.cat([embedded, context], dim=-1)
        output, hidden = self.lstm(lstm_input, hidden)
        logits = self.output_proj(output)
        return logits, hidden
```

#### 3.3 Teacher Forcing vs è‡ªå›å½’ç”Ÿæˆ

**è®­ç»ƒæ—¶ï¼ˆTeacher Forcingï¼‰ï¼š**
```python
for t in range(target_length):
    output, hidden = decoder(target[:, t:t+1], hidden, encoder_outputs)
    loss += criterion(output, target[:, t+1:t+2])
```

**æ¨ç†æ—¶ï¼ˆè‡ªå›å½’ï¼‰ï¼š**
```python
input = start_token
for t in range(max_length):
    output, hidden = decoder(input, hidden, encoder_outputs)
    input = torch.argmax(output, dim=-1)
    if input == end_token:
        break
```

**æ›å…‰åå·® (Exposure Bias)ï¼š**
è®­ç»ƒå’Œæ¨ç†çš„å·®å¼‚å¯èƒ½å¯¼è‡´é”™è¯¯ç´¯ç§¯ï¼Œå¯é€šè¿‡ä»¥ä¸‹æ–¹æ³•ç¼“è§£ï¼š
- **Scheduled Samplingï¼š** è®­ç»ƒæ—¶éšæœºä½¿ç”¨çœŸå®æ ‡ç­¾æˆ–æ¨¡å‹é¢„æµ‹
- **Curriculum Learningï¼š** é€æ­¥å‡å°‘teacher forcingæ¯”ä¾‹

### 4. æœºå™¨ç¿»è¯‘ä»»åŠ¡

#### 4.1 æ•°æ®é¢„å¤„ç†

**åˆ†è¯ (Tokenization)ï¼š**
```python
# è‹±æ–‡ï¼šåŸºäºç©ºæ ¼å’Œæ ‡ç‚¹
"Hello, world!" â†’ ["Hello", ",", "world", "!"]

# æ³•æ–‡ï¼šå¤„ç†é‡éŸ³ç¬¦å·å’Œè¿å­—ç¬¦
"C'est trÃ¨s bien." â†’ ["C'", "est", "trÃ¨s", "bien", "."]
```

**è¯æ±‡è¡¨æ„å»ºï¼š**
```python
class Vocabulary:
    def __init__(self):
        self.word2idx = {"<pad>": 0, "<sos>": 1, "<eos>": 2, "<unk>": 3}
        self.idx2word = {0: "<pad>", 1: "<sos>", 2: "<eos>", 3: "<unk>"}
        
    def add_word(self, word):
        if word not in self.word2idx:
            idx = len(self.word2idx)
            self.word2idx[word] = idx
            self.idx2word[idx] = word
```

**åºåˆ—å¡«å…… (Padding)ï¼š**
```python
# æ‰¹æ¬¡å†…åºåˆ—é•¿åº¦å¯¹é½
batch = [
    [1, 2, 3],           # åŸå§‹åºåˆ—
    [4, 5, 6, 7, 8],     # è¾ƒé•¿åºåˆ—
    [9, 10]              # è¾ƒçŸ­åºåˆ—
]

padded_batch = [
    [1, 2, 3, 0, 0],     # å¡«å……åˆ°æœ€å¤§é•¿åº¦
    [4, 5, 6, 7, 8],
    [9, 10, 0, 0, 0]
]
```

#### 4.2 BLEUè¯„ä¼°æŒ‡æ ‡

**BLEU (Bilingual Evaluation Understudy)** æ˜¯æœºå™¨ç¿»è¯‘è´¨é‡çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š** è®¡ç®—æœºå™¨ç¿»è¯‘ä¸å‚è€ƒç¿»è¯‘ä¹‹é—´çš„n-gramé‡å ç¨‹åº¦

**è®¡ç®—å…¬å¼ï¼š**
```
BLEU = BP Ã— exp(Î£á´ºâ‚™â‚Œâ‚ wâ‚™ Ã— log pâ‚™)

å…¶ä¸­ï¼š
- pâ‚™ï¼šn-gramç²¾ç¡®åº¦
- wâ‚™ï¼šæƒé‡ (é€šå¸¸ wâ‚=wâ‚‚=wâ‚ƒ=wâ‚„=0.25)
- BPï¼šç®€æ´æ€§æƒ©ç½š (Brevity Penalty)
```

**ç®€æ´æ€§æƒ©ç½šï¼š**
```python
if len(candidate) > len(reference):
    BP = 1
else:
    BP = exp(1 - len(reference) / len(candidate))
```

**ç¤ºä¾‹è®¡ç®—ï¼š**
```python
å‚è€ƒç¿»è¯‘: "The cat is on the mat"
å€™é€‰ç¿»è¯‘: "The cat is on mat"

1-gram: 5/5 = 1.0    (The, cat, is, on, mat)
2-gram: 3/4 = 0.75   (The cat, cat is, is on)
3-gram: 2/3 = 0.67   (The cat is, cat is on)
4-gram: 1/2 = 0.5    (The cat is on)

BP = exp(1 - 6/5) = 0.819
BLEU = 0.819 Ã— exp(0.25Ã—log(1.0) + 0.25Ã—log(0.75) + 0.25Ã—log(0.67) + 0.25Ã—log(0.5))
     â‰ˆ 0.61
```

### 5. æ³¨æ„åŠ›å¯è§†åŒ–

#### 5.1 æ³¨æ„åŠ›çŸ©é˜µçƒ­å›¾

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, source_words, target_words):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    plt.figure(figsize=(10, 8))
    
    # åˆ›å»ºçƒ­å›¾
    sns.heatmap(attention_weights, 
                xticklabels=source_words, 
                yticklabels=target_words,
                cmap='Blues', 
                cbar=True)
    
    plt.xlabel('Source Sequence')
    plt.ylabel('Target Sequence')
    plt.title('Attention Weights Visualization')
    plt.show()
```

#### 5.2 æ³¨æ„åŠ›æ¨¡å¼åˆ†æ

**å¸¸è§çš„æ³¨æ„åŠ›æ¨¡å¼ï¼š**

1. **å•è°ƒå¯¹é½ (Monotonic Alignment)ï¼š**
   - æ³¨æ„åŠ›æƒé‡æ²¿å¯¹è§’çº¿åˆ†å¸ƒ
   - é€‚ç”¨äºè¯­åºç›¸è¿‘çš„è¯­è¨€å¯¹

2. **å€’åºå¯¹é½ (Inverted Alignment)ï¼š**
   - æ³¨æ„åŠ›æƒé‡å‘ˆåå¯¹è§’çº¿åˆ†å¸ƒ
   - é€‚ç”¨äºè¯­åºç›¸åçš„è¯­è¨€å¯¹

3. **å¤šå¯¹ä¸€/ä¸€å¯¹å¤šå¯¹é½ï¼š**
   - ä¸€ä¸ªæºè¯å¯¹åº”å¤šä¸ªç›®æ ‡è¯ï¼Œæˆ–åä¹‹
   - å¤„ç†è¯­è¨€é—´çš„ç»“æ„å·®å¼‚

### 6. é«˜çº§æŠ€å·§ä¸ä¼˜åŒ–

#### 6.1 Coverageæœºåˆ¶

**é—®é¢˜ï¼š** æ ‡å‡†æ³¨æ„åŠ›å¯èƒ½é‡å¤å…³æ³¨æŸäº›æºè¯ï¼Œå¿½ç•¥å…¶ä»–é‡è¦ä¿¡æ¯

**è§£å†³æ–¹æ¡ˆï¼š** å¼•å…¥è¦†ç›–å‘é‡ (Coverage Vector)

```python
class CoverageAttention(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.coverage_proj = nn.Linear(1, hidden_size)
        self.v = nn.Parameter(torch.randn(hidden_size))
        
    def forward(self, hidden, encoder_outputs, coverage):
        # coverage: ç´¯ç§¯çš„æ³¨æ„åŠ›æƒé‡
        energy = torch.tanh(
            self.hidden_proj(hidden) +
            self.encoder_proj(encoder_outputs) +
            self.coverage_proj(coverage.unsqueeze(-1))
        )
        energy = torch.matmul(energy, self.v)
        attention = F.softmax(energy, dim=-1)
        
        # æ›´æ–°è¦†ç›–å‘é‡
        coverage = coverage + attention
        return attention, coverage
```

#### 6.2 Beam Searchè§£ç 

**é—®é¢˜ï¼š** è´ªå¿ƒè§£ç å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜

**è§£å†³æ–¹æ¡ˆï¼š** ä¿æŒå¤šä¸ªå€™é€‰åºåˆ—ï¼Œé€‰æ‹©å…¨å±€æœ€ä¼˜

```python
def beam_search(model, encoder_outputs, beam_size=5, max_length=50):
    """Beam Searchè§£ç """
    beam = [(0.0, [start_token], initial_hidden)]  # (score, sequence, hidden)
    
    for _ in range(max_length):
        candidates = []
        
        for score, sequence, hidden in beam:
            if sequence[-1] == end_token:
                candidates.append((score, sequence, hidden))
                continue
                
            input_token = sequence[-1]
            logits, new_hidden = model.decoder(input_token, hidden, encoder_outputs)
            log_probs = F.log_softmax(logits, dim=-1)
            
            # æ‰©å±•å€™é€‰
            top_tokens = torch.topk(log_probs, beam_size)
            for token_score, token_idx in zip(top_tokens.values, top_tokens.indices):
                new_score = score + token_score.item()
                new_sequence = sequence + [token_idx.item()]
                candidates.append((new_score, new_sequence, new_hidden))
        
        # é€‰æ‹©top-kå€™é€‰
        beam = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_size]
    
    return beam[0][1]  # è¿”å›æœ€ä½³åºåˆ—
```

### 7. å®éªŒè®¾è®¡ä¸è¯„ä¼°

#### 7.1 æ•°æ®é›†

**æ¨èæ•°æ®é›†ï¼š**
- **IWSLT14 (è‹±-å¾·)ï¼š** 160Kå¥å¯¹ï¼Œé€‚åˆå¿«é€Ÿå®éªŒ
- **WMT14 (è‹±-æ³•)ï¼š** 36Må¥å¯¹ï¼Œå¤§è§„æ¨¡ç¿»è¯‘ä»»åŠ¡
- **Multi30Kï¼š** å¤šè¯­è¨€å›¾åƒæè¿°ï¼Œ31Kå¥å¯¹

#### 7.2 å®éªŒé…ç½®

**åŸºçº¿æ¨¡å‹å¯¹æ¯”ï¼š**
```python
experiments = {
    "Seq2Seq (æ— æ³¨æ„åŠ›)": {
        "encoder": "BiLSTM",
        "decoder": "LSTM", 
        "attention": None
    },
    "Seq2Seq + Bahdanauæ³¨æ„åŠ›": {
        "encoder": "BiLSTM",
        "decoder": "LSTM",
        "attention": "Bahdanau"
    },
    "Seq2Seq + Luongæ³¨æ„åŠ›": {
        "encoder": "BiLSTM", 
        "decoder": "LSTM",
        "attention": "Luong"
    }
}
```

**è¯„ä¼°æŒ‡æ ‡ï¼š**
- **BLEU-1, 2, 3, 4ï¼š** ä¸åŒn-gramçš„BLEUåˆ†æ•°
- **METEORï¼š** è€ƒè™‘åŒä¹‰è¯å’Œè¯å¹²çš„è¯„ä¼°æŒ‡æ ‡
- **CIDErï¼š** åŸºäºTF-IDFæƒé‡çš„è¯„ä¼°æŒ‡æ ‡
- **äººå·¥è¯„ä¼°ï¼š** æµç•…æ€§å’Œå‡†ç¡®æ€§è¯„åˆ†

#### 7.3 æ¶ˆèç ”ç©¶ (Ablation Study)

**ç ”ç©¶é—®é¢˜ï¼š**
1. æ³¨æ„åŠ›æœºåˆ¶å¯¹ç¿»è¯‘è´¨é‡çš„å½±å“ï¼Ÿ
2. ä¸åŒæ³¨æ„åŠ›ç±»å‹çš„æ€§èƒ½å·®å¼‚ï¼Ÿ
3. ç¼–ç å™¨å±‚æ•°å¯¹æ€§èƒ½çš„å½±å“ï¼Ÿ
4. Teacher forcingæ¯”ä¾‹çš„å½±å“ï¼Ÿ

### 8. å½“å‰æŒ‘æˆ˜ä¸å‘å±•æ–¹å‘

#### 8.1 å±€é™æ€§

**è®¡ç®—å¤æ‚åº¦ï¼š**
- æ³¨æ„åŠ›è®¡ç®—çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(nÂ²)
- é•¿åºåˆ—å¤„ç†æ•ˆç‡ä½ä¸‹

**å¯¹é½è´¨é‡ï¼š**
- ç¡¬æ³¨æ„åŠ›vsè½¯æ³¨æ„åŠ›çš„æƒè¡¡
- ç¨€ç–æ³¨æ„åŠ›çš„æ¢ç´¢

#### 8.2 å‘å±•è¶‹åŠ¿

**è‡ªæ³¨æ„åŠ› (Self-Attention)ï¼š**
- åºåˆ—å†…éƒ¨çš„æ³¨æ„åŠ›è®¡ç®—
- Transformerçš„æ ¸å¿ƒç»„ä»¶

**å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)ï¼š**
- å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›è¡¨ç¤º
- æ•è·ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»

**ä½ç½®ç¼–ç  (Positional Encoding)ï¼š**
- è§£å†³æ³¨æ„åŠ›æœºåˆ¶ç¼ºä¹ä½ç½®ä¿¡æ¯çš„é—®é¢˜
- ä¸ºTransformeræ¨¡å‹å¥ å®šåŸºç¡€

## ğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“

1. **æ³¨æ„åŠ›æœºåˆ¶çš„æœ¬è´¨ï¼š** åŠ¨æ€æƒé‡åˆ†é…ï¼Œè§£å†³ä¿¡æ¯ç“¶é¢ˆé—®é¢˜

2. **Seq2Seqæ¶æ„ï¼š** ç¼–ç å™¨-è§£ç å™¨èŒƒå¼ï¼Œé€‚ç”¨äºå¤šç§åºåˆ—è½¬æ¢ä»»åŠ¡

3. **æ³¨æ„åŠ›ç±»å‹ï¼š** Bahdanauï¼ˆåŠ æ€§ï¼‰vs Luongï¼ˆä¹˜æ€§ï¼‰ï¼Œå„æœ‰ä¼˜åŠ£

4. **è®­ç»ƒç­–ç•¥ï¼š** Teacher forcingä¸æ›å…‰åå·®çš„æƒè¡¡

5. **è¯„ä¼°æ–¹æ³•ï¼š** BLEUç­‰è‡ªåŠ¨æŒ‡æ ‡ç»“åˆäººå·¥è¯„ä¼°

6. **å¯è§†åŒ–åˆ†æï¼š** æ³¨æ„åŠ›çŸ©é˜µæ­ç¤ºæ¨¡å‹çš„å¯¹é½æ¨¡å¼

7. **æŠ€æœ¯æ¼”è¿›ï¼š** ä»RNNæ³¨æ„åŠ›åˆ°Transformerçš„è‡ªæ³¨æ„åŠ›

é€šè¿‡æŒæ¡æ³¨æ„åŠ›æœºåˆ¶ä¸Seq2Seqæ¨¡å‹ï¼Œæ‚¨å·²ç»ä¸ºç†è§£ç°ä»£æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæŠ€æœ¯â€”â€”Transformeræ¶æ„â€”â€”åšå¥½äº†å……åˆ†å‡†å¤‡ï¼