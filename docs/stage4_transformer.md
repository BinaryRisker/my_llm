# Stage 4: Transformer Architecture - ç†è®ºä¸å®è·µæŒ‡å— ğŸ”€

[![Theory](https://img.shields.io/badge/Level-Advanced-red.svg)](https://github.com/your-username/my_llm)
[![Architecture](https://img.shields.io/badge/Architecture-Transformer-blue.svg)](https://arxiv.org/abs/1706.03762)
[![Applications](https://img.shields.io/badge/Applications-NLP-green.svg)](https://github.com/your-username/my_llm)

## ğŸ“– æ¦‚è¿°

**Transformer** æ˜¯ç”± Vaswani ç­‰äººåœ¨ 2017 å¹´è®ºæ–‡ "Attention Is All You Need" ä¸­æå‡ºçš„é©å‘½æ€§æ¶æ„ã€‚å®ƒå®Œå…¨æŠ›å¼ƒäº†å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œä»…ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†åºåˆ—æ•°æ®ï¼Œæˆä¸ºäº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„ã€‚

### ğŸ¯ æœ¬é˜¶æ®µå­¦ä¹ ç›®æ ‡

- ç†è§£ Self-Attention å’Œ Multi-Head Attention æœºåˆ¶
- æŒæ¡ Transformer çš„å®Œæ•´æ¶æ„è®¾è®¡
- å®ç°ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰
- ç†è§£å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰å’Œæ®‹å·®è¿æ¥
- æŒæ¡ Transformer çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹
- æ¯”è¾ƒ Transformer ä¸ä¼ ç»Ÿ RNN/CNN çš„ä¼˜åŠ£

## ğŸ—ï¸ Transformer æ¶æ„è¯¦è§£

### æ•´ä½“æ¶æ„æ¦‚è§ˆ

```
è¾“å…¥åºåˆ— â†’ Embedding + Position Encoding
    â†“
Encoder Stack (N=6å±‚)
â”œâ”€â”€ Multi-Head Self-Attention
â”œâ”€â”€ Add & Norm
â”œâ”€â”€ Feed Forward Network  
â””â”€â”€ Add & Norm
    â†“
Decoder Stack (N=6å±‚)
â”œâ”€â”€ Masked Multi-Head Self-Attention
â”œâ”€â”€ Add & Norm
â”œâ”€â”€ Multi-Head Cross-Attention (ä¸Encoderè¿æ¥)
â”œâ”€â”€ Add & Norm
â”œâ”€â”€ Feed Forward Network
â””â”€â”€ Add & Norm
    â†“
Linear + Softmax â†’ è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
```

### 1. Multi-Head Attention æœºåˆ¶

#### 1.1 Self-Attention åŸºç¡€

Self-Attention å…è®¸åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®å…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼ŒåŒ…æ‹¬è‡ªèº«ã€‚

**æ•°å­¦å…¬å¼ï¼š**
```python
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V

å…¶ä¸­ï¼š
- Q (Query): æŸ¥è¯¢çŸ©é˜µï¼Œå½¢çŠ¶ [seq_len, d_k]
- K (Key): é”®çŸ©é˜µï¼Œå½¢çŠ¶ [seq_len, d_k] 
- V (Value): å€¼çŸ©é˜µï¼Œå½¢çŠ¶ [seq_len, d_v]
- d_k: é”®ç»´åº¦ï¼Œç”¨äºç¼©æ”¾ç‚¹ç§¯
```

**ç›´è§‚ç†è§£ï¼š**
- Query: "æˆ‘æƒ³æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
- Key: "æˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯å¯ä»¥è¢«æ‰¾åˆ°ï¼Ÿ"
- Value: "æ‰¾åˆ°åï¼Œå®é™…è¦ä¼ é€’çš„ä¿¡æ¯"

#### 1.2 Multi-Head Attention

å°†æ³¨æ„åŠ›æœºåˆ¶æ‰©å±•åˆ°å¤šä¸ª"å¤´"ï¼ˆheadsï¼‰ï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´ã€‚

```python
MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O

å…¶ä¸­æ¯ä¸ª head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

å‚æ•°çŸ©é˜µï¼š
- W_i^Q âˆˆ R^{d_model Ã— d_k}  # QueryæŠ•å½±çŸ©é˜µ
- W_i^K âˆˆ R^{d_model Ã— d_k}  # KeyæŠ•å½±çŸ©é˜µ  
- W_i^V âˆˆ R^{d_model Ã— d_v}  # ValueæŠ•å½±çŸ©é˜µ
- W^O âˆˆ R^{hd_v Ã— d_model}   # è¾“å‡ºæŠ•å½±çŸ©é˜µ
```

**ä»£ç å®ç°ç¤ºä¾‹ï¼š**
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads, d_k=None, d_v=None, dropout=0.1):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_k or d_model // n_heads
        self.d_v = d_v or d_model // n_heads
        
        # çº¿æ€§æŠ•å½±å±‚
        self.w_q = nn.Linear(d_model, n_heads * self.d_k, bias=False)
        self.w_k = nn.Linear(d_model, n_heads * self.d_k, bias=False)
        self.w_v = nn.Linear(d_model, n_heads * self.d_v, bias=False)
        self.w_o = nn.Linear(n_heads * self.d_v, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = self.d_k ** -0.5
    
    def forward(self, query, key, value, mask=None):
        batch_size, seq_len = query.size(0), query.size(1)
        
        # 1. çº¿æ€§æŠ•å½±å¹¶é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_v).transpose(1, 2)
        
        # 2. è®¡ç®—æ³¨æ„åŠ›
        attention_output, attention_weights = self.attention(Q, K, V, mask)
        
        # 3. è¿æ¥å¤šå¤´å¹¶æŠ•å½±
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.n_heads * self.d_v
        )
        
        return self.w_o(attention_output), attention_weights
```

### 2. ä½ç½®ç¼–ç  (Positional Encoding)

Transformer æ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œå› æ­¤éœ€è¦æ˜¾å¼åœ°ä¸ºåºåˆ—ä¸­çš„ä½ç½®æ·»åŠ ä¿¡æ¯ã€‚

#### 2.1 ç»å¯¹ä½ç½®ç¼–ç 

**æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç ï¼š**
```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

å…¶ä¸­ï¼š
- pos: åºåˆ—ä¸­çš„ä½ç½® (0, 1, 2, ...)
- i: ç»´åº¦ç´¢å¼• (0, 1, 2, ..., d_model/2-1)  
- d_model: æ¨¡å‹ç»´åº¦
```

**ä»£ç å®ç°ï¼š**
```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        
        # åˆ›å»ºä½ç½®ç¼–ç è¡¨
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        # è®¡ç®—é™¤æ•°é¡¹
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        # åº”ç”¨æ­£å¼¦å’Œä½™å¼¦
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ç»´åº¦
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ç»´åº¦
        
        pe = pe.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x: [batch_size, seq_len, d_model]
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
```

#### 2.2 ä½ç½®ç¼–ç çš„ä¼˜åŠ¿

1. **é•¿åº¦æ— å…³**: å¯ä»¥å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„åºåˆ—é•¿åº¦
2. **å‘¨æœŸæ€§**: ç›¸å¯¹ä½ç½®å…³ç³»è¢«ç¼–ç åœ¨å‡½æ•°ä¸­
3. **å¯å­¦ä¹ æ€§**: æ¨¡å‹å¯ä»¥å­¦ä¹ å¦‚ä½•åˆ©ç”¨ä½ç½®ä¿¡æ¯

### 3. Layer Normalization & æ®‹å·®è¿æ¥

#### 3.1 Layer Normalization

ä¸ Batch Normalization ä¸åŒï¼ŒLayer Normalization åœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼š

```python
LayerNorm(x) = Î³ * (x - Î¼) / Ïƒ + Î²

å…¶ä¸­ï¼š
- Î¼ = mean(x, dim=-1)     # åœ¨æœ€åä¸€ä¸ªç»´åº¦è®¡ç®—å‡å€¼
- Ïƒ = std(x, dim=-1)      # åœ¨æœ€åä¸€ä¸ªç»´åº¦è®¡ç®—æ ‡å‡†å·®
- Î³, Î²: å¯å­¦ä¹ å‚æ•°
```

**ä»£ç å®ç°ï¼š**
```python
class LayerNorm(nn.Module):
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps
    
    def forward(self, x):
        # x: [batch_size, seq_len, d_model]
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
```

#### 3.2 æ®‹å·®è¿æ¥ (Residual Connection)

```python
output = LayerNorm(x + Sublayer(x))
```

è¿™ç§è®¾è®¡æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- æ¢¯åº¦æµæ›´é¡ºç•…ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±
- è®­ç»ƒæ›´ç¨³å®š
- å…è®¸æ„å»ºæ›´æ·±çš„ç½‘ç»œ

### 4. Feed Forward Network (FFN)

æ¯ä¸ª Transformer å±‚åŒ…å«ä¸€ä¸ªå‰é¦ˆç½‘ç»œï¼š

```python
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2

å…¶ä¸­ï¼š
- W_1 âˆˆ R^{d_model Ã— d_ff}  # ç¬¬ä¸€å±‚æƒé‡ï¼Œé€šå¸¸ d_ff = 4 * d_model
- W_2 âˆˆ R^{d_ff Ã— d_model}  # ç¬¬äºŒå±‚æƒé‡  
- ReLU æ¿€æ´»å‡½æ•°
```

**ä»£ç å®ç°ï¼š**
```python
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        return self.linear2(self.dropout(torch.relu(self.linear1(x))))
```

## ğŸ­ Encoder-Decoder æ¶æ„è¯¦è§£

### 1. Transformer Encoder

Encoder è´Ÿè´£ç†è§£è¾“å…¥åºåˆ—ï¼Œæ¯å±‚åŒ…å«ï¼š
1. Multi-Head Self-Attention
2. Add & Norm
3. Feed Forward Network  
4. Add & Norm

```python
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout=dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # Self-attention + æ®‹å·®è¿æ¥ + LayerNorm
        attn_output, _ = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed forward + æ®‹å·®è¿æ¥ + LayerNorm  
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x
```

### 2. Transformer Decoder

Decoder è´Ÿè´£ç”Ÿæˆè¾“å‡ºåºåˆ—ï¼Œæ¯å±‚åŒ…å«ï¼š
1. Masked Multi-Head Self-Attention (é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯)
2. Add & Norm
3. Multi-Head Cross-Attention (è¿æ¥ Encoder)
4. Add & Norm
5. Feed Forward Network
6. Add & Norm

```python
class TransformerDecoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout=dropout)
        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout=dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        
        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)  
        self.norm3 = LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        # 1. Masked Self-Attention
        self_attn_output, _ = self.self_attention(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(self_attn_output))
        
        # 2. Cross-Attention with Encoder
        cross_attn_output, attention_weights = self.cross_attention(
            x, encoder_output, encoder_output, src_mask
        )
        x = self.norm2(x + self.dropout(cross_attn_output))
        
        # 3. Feed Forward
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x, attention_weights
```

## ğŸ­ æ©ç æœºåˆ¶è¯¦è§£

### 1. Padding Mask

é˜²æ­¢æ³¨æ„åŠ›å…³æ³¨åˆ°å¡«å……ä½ç½®ï¼š

```python
def create_padding_mask(seq, pad_idx=0):
    # seq: [batch_size, seq_len]
    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)
    # è¿”å›: [batch_size, 1, 1, seq_len]
```

### 2. Look-Ahead Mask (å› æœæ©ç )

åœ¨ Decoder ä¸­é˜²æ­¢å…³æ³¨æœªæ¥ä½ç½®ï¼š

```python
def create_look_ahead_mask(size):
    mask = torch.triu(torch.ones(size, size), diagonal=1)
    return mask == 0  # ä¸‹ä¸‰è§’ä¸º Trueï¼Œä¸Šä¸‰è§’ä¸º False
```

### 3. ç»„åˆæ©ç 

```python
def create_masks(src, tgt, src_pad_idx=0, tgt_pad_idx=0):
    src_mask = create_padding_mask(src, src_pad_idx)
    
    tgt_padding_mask = create_padding_mask(tgt, tgt_pad_idx)
    tgt_look_ahead_mask = create_look_ahead_mask(tgt.size(1))
    tgt_mask = tgt_padding_mask & tgt_look_ahead_mask
    
    return src_mask, tgt_mask
```

## âš¡ è®­ç»ƒä¸æ¨ç†

### 1. è®­ç»ƒè¿‡ç¨‹ï¼ˆTeacher Forcingï¼‰

```python
def train_step(model, src, tgt, criterion, optimizer):
    model.train()
    
    # åˆ›å»ºæ©ç 
    src_mask, tgt_mask = create_masks(src, tgt[:, :-1])
    
    # å‰å‘ä¼ æ’­
    output = model(src, tgt[:, :-1], src_mask, tgt_mask)
    
    # è®¡ç®—æŸå¤±ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰
    loss = criterion(
        output.reshape(-1, output.size(-1)), 
        tgt[:, 1:].reshape(-1)
    )
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    
    return loss.item()
```

### 2. æ¨ç†è¿‡ç¨‹ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰

```python
def greedy_decode(model, src, src_mask, max_len, start_symbol, end_symbol):
    model.eval()
    
    # ç¼–ç æºåºåˆ—
    encoder_output = model.encoder(src, src_mask)
    
    # åˆå§‹åŒ–ç›®æ ‡åºåˆ—
    tgt = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)
    
    for i in range(max_len - 1):
        tgt_mask = create_look_ahead_mask(tgt.size(1))
        
        # è§£ç 
        output = model.decoder(tgt, encoder_output, src_mask, tgt_mask)
        
        # é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
        next_word = output[:, -1].argmax(dim=-1).unsqueeze(0)
        tgt = torch.cat([tgt, next_word], dim=1)
        
        if next_word == end_symbol:
            break
    
    return tgt
```

## ğŸ“Š Transformer ä¼˜åŠ¿åˆ†æ

### 1. ç›¸æ¯” RNN çš„ä¼˜åŠ¿

| ç‰¹æ€§ | RNN/LSTM | Transformer |
|------|----------|-------------|
| **å¹¶è¡ŒåŒ–** | åºåˆ—å¤„ç†ï¼Œæ— æ³•å¹¶è¡Œ | å®Œå…¨å¹¶è¡ŒåŒ– |
| **é•¿è·ç¦»ä¾èµ–** | æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ | ç›´æ¥è¿æ¥ï¼ŒO(1)å¤æ‚åº¦ |
| **è®­ç»ƒé€Ÿåº¦** | æ…¢ | å¿« |
| **å†…å­˜æ•ˆç‡** | å¥½ | éœ€è¦å¤§é‡å†…å­˜ |
| **å¯è§£é‡Šæ€§** | è¾ƒå·® | æ³¨æ„åŠ›æƒé‡å¯è§†åŒ– |

### 2. è®¡ç®—å¤æ‚åº¦åˆ†æ

| æ“ä½œ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ |
|------|-----------|------------|
| **Self-Attention** | O(nÂ²Â·d) | O(nÂ²) |
| **RNN** | O(nÂ·dÂ²) | O(d) |
| **CNN** | O(kÂ·nÂ·dÂ²) | O(kÂ·d) |

å…¶ä¸­ï¼šn=åºåˆ—é•¿åº¦ï¼Œd=ç‰¹å¾ç»´åº¦ï¼Œk=å·ç§¯æ ¸å¤§å°

## ğŸ”§ å®è·µæŠ€å·§ä¸ä¼˜åŒ–

### 1. å­¦ä¹ ç‡è°ƒåº¦

**Warmup + Cosine Decayï¼š**
```python
def get_lr_scheduler(optimizer, d_model, warmup_steps=4000):
    def lr_lambda(step):
        if step == 0:
            step = 1
        return min(step ** -0.5, step * warmup_steps ** -1.5) * (d_model ** -0.5)
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

### 2. æ ‡ç­¾å¹³æ»‘ (Label Smoothing)

```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, size, padding_idx=0, smoothing=0.1):
        super().__init__()
        self.criterion = nn.KLDivLoss(reduction='sum')
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
    
    def forward(self, x, target):
        # åˆ›å»ºå¹³æ»‘æ ‡ç­¾åˆ†å¸ƒ
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        
        return self.criterion(x, true_dist)
```

### 3. æ¨¡å‹å¹¶è¡ŒåŒ–

```python
# æ•°æ®å¹¶è¡Œ
model = nn.DataParallel(transformer)

# æ¨¡å‹å¹¶è¡Œï¼ˆå¤§æ¨¡å‹ï¼‰
class DistributedTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        # å°†ä¸åŒå±‚æ”¾åœ¨ä¸åŒGPUä¸Š
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(config).to(f'cuda:{i//2}') 
            for i in range(config.num_layers)
        ])
```

## ğŸ“ˆ Transformer å˜ä½“

### 1. Encoder-Only æ¨¡å‹
- **BERT**: åŒå‘ç¼–ç å™¨ï¼Œç”¨äºç†è§£ä»»åŠ¡
- **åº”ç”¨**: æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«

### 2. Decoder-Only æ¨¡å‹  
- **GPT**: è‡ªå›å½’ç”Ÿæˆæ¨¡å‹
- **åº”ç”¨**: æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿ

### 3. Encoder-Decoder æ¨¡å‹
- **T5**: Text-to-Text Transfer Transformer
- **åº”ç”¨**: æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦

## ğŸš€ å®é™…åº”ç”¨åœºæ™¯

### 1. æœºå™¨ç¿»è¯‘
```python
# è‹±æ³•ç¿»è¯‘ç¤ºä¾‹
transformer = Transformer(
    src_vocab_size=30000,
    tgt_vocab_size=30000, 
    d_model=512,
    n_heads=8,
    num_encoder_layers=6,
    num_decoder_layers=6
)
```

### 2. æ–‡æœ¬æ‘˜è¦
```python
# æŠ½è±¡æ‘˜è¦ä»»åŠ¡
summarizer = Transformer(
    src_vocab_size=50000,  # è¾“å…¥æ–‡æ¡£è¯æ±‡è¡¨
    tgt_vocab_size=50000,  # æ‘˜è¦è¯æ±‡è¡¨
    d_model=768,
    n_heads=12
)
```

### 3. ä»£ç ç”Ÿæˆ
```python  
# è‡ªç„¶è¯­è¨€åˆ°ä»£ç 
code_generator = Transformer(
    src_vocab_size=10000,  # è‡ªç„¶è¯­è¨€
    tgt_vocab_size=5000,   # ä»£ç tokens
    d_model=512,
    n_heads=8
)
```

## ğŸ¯ è®­ç»ƒå»ºè®®

### 1. è¶…å‚æ•°è®¾ç½®
```python
# æ¨èé…ç½®
config = {
    'd_model': 512,
    'n_heads': 8,
    'd_ff': 2048,
    'num_layers': 6,
    'dropout': 0.1,
    'warmup_steps': 4000,
    'label_smoothing': 0.1
}
```

### 2. è®­ç»ƒç­–ç•¥
- **æ¢¯åº¦ç´¯ç§¯**: æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒ
- **æ··åˆç²¾åº¦**: ä½¿ç”¨ FP16 åŠ é€Ÿè®­ç»ƒ
- **æ£€æŸ¥ç‚¹ä¿å­˜**: å®šæœŸä¿å­˜æœ€ä½³æ¨¡å‹
- **æ—©åœ**: é˜²æ­¢è¿‡æ‹Ÿåˆ

### 3. æ•°æ®é¢„å¤„ç†
```python
# æ•°æ®å¢å¼ºæŠ€å·§
def augment_data(src, tgt):
    # 1. éšæœºåˆ é™¤è¯æ±‡
    # 2. åŒä¹‰è¯æ›¿æ¢  
    # 3. å›è¯‘å¢å¼º
    # 4. å™ªå£°æ³¨å…¥
    return augmented_src, augmented_tgt
```

## ğŸ” è°ƒè¯•ä¸åˆ†æ

### 1. æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
```python
def visualize_attention(model, src, tgt, layer=0, head=0):
    model.eval()
    with torch.no_grad():
        _, attention_weights = model(src, tgt, return_attention=True)
        
    # æå–ç‰¹å®šå±‚å’Œå¤´çš„æ³¨æ„åŠ›æƒé‡
    attn = attention_weights[layer][0, head].cpu().numpy()
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    plt.figure(figsize=(10, 8))
    sns.heatmap(attn, cmap='Blues', cbar=True)
    plt.title(f'Attention Weights - Layer {layer}, Head {head}')
    plt.show()
```

### 2. æ¢¯åº¦åˆ†æ
```python
def analyze_gradients(model):
    total_norm = 0
    param_count = 0
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
            param_count += 1
            print(f'{name}: {param_norm:.4f}')
    
    total_norm = total_norm ** (1. / 2)
    print(f'Total gradient norm: {total_norm:.4f}')
```

## ğŸ“š è¿›ä¸€æ­¥å­¦ä¹ 

### æ¨èè®ºæ–‡é˜…è¯»é¡ºåº
1. **Attention Is All You Need** (Vaswani et al., 2017) - åŸå§‹è®ºæ–‡
2. **BERT** (Devlin et al., 2018) - Encoder-only æ¶æ„
3. **GPT** (Radford et al., 2018) - Decoder-only æ¶æ„  
4. **T5** (Raffel et al., 2019) - ç»Ÿä¸€ Text-to-Text æ¡†æ¶
5. **Vision Transformer** (Dosovitskiy et al., 2020) - è§†è§‰é¢†åŸŸåº”ç”¨

### ä»£ç å®ç°å‚è€ƒ
- **Annotated Transformer**: Harvard NLP è¯¦ç»†æ³¨é‡Šç‰ˆæœ¬
- **Transformers Library**: Hugging Face å·¥ä¸šçº§å®ç°
- **FairSeq**: Facebook ç ”ç©¶ç‰ˆæœ¬
- **Tensor2Tensor**: Google å®˜æ–¹å®ç°

## ğŸ‰ å°ç»“

Transformer æ¶æ„çš„æå‡ºæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ï¼š

### æ ¸å¿ƒåˆ›æ–°ç‚¹
1. **å®Œå…¨åŸºäºæ³¨æ„åŠ›**: æŠ›å¼ƒäº†RNNå’ŒCNN
2. **å¹¶è¡ŒåŒ–è®­ç»ƒ**: å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡
3. **é•¿è·ç¦»å»ºæ¨¡**: æœ‰æ•ˆå¤„ç†é•¿åºåˆ—ä¾èµ–
4. **å¯æ‰©å±•æ€§**: æ”¯æŒå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ

### å½±å“å’Œæ„ä¹‰
- å‚¬ç”Ÿäº†BERTã€GPTç­‰é¢„è®­ç»ƒæ¨¡å‹æµªæ½®
- æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ ‡å‡†æ¶æ„
- æ¨åŠ¨äº†å¤šæ¨¡æ€ã€è·¨é¢†åŸŸåº”ç”¨å‘å±•
- ä¸ºç°ä»£AIç³»ç»Ÿå¥ å®šäº†æ¶æ„åŸºç¡€

### ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘
- å®ç°å®Œæ•´çš„Transformeræ¨¡å‹
- å°è¯•ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶å˜ä½“
- æ¢ç´¢ä½ç½®ç¼–ç çš„æ”¹è¿›æ–¹æ³•
- å­¦ä¹ æ¨¡å‹å‹ç¼©å’ŒåŠ é€ŸæŠ€æœ¯

---

**ä¸‹ä¸€é˜¶æ®µ**: [Stage 5 - GPTæ¶æ„ä¸é¢„è®­ç»ƒ](stage5_gpt.md)

é€šè¿‡ç³»ç»Ÿå­¦ä¹ Transformerï¼Œæ‚¨å°†æŒæ¡ç°ä»£æ·±åº¦å­¦ä¹ æœ€é‡è¦çš„æ¶æ„ä¹‹ä¸€ï¼Œä¸ºç†è§£å’Œå¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹æ‰“ä¸‹åšå®åŸºç¡€ï¼