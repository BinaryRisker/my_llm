# Stage 5: GPT Architecture - è‡ªå›å½’è¯­è¨€æ¨¡å‹è¯¦è§£ ğŸš€

[![Theory](https://img.shields.io/badge/Level-Advanced-red.svg)](https://github.com/your-username/my_llm)
[![Architecture](https://img.shields.io/badge/Architecture-GPT-purple.svg)](https://openai.com/research/gpt)
[![Applications](https://img.shields.io/badge/Applications-Generation-orange.svg)](https://github.com/your-username/my_llm)

## ğŸ“– æ¦‚è¿°

**GPT (Generative Pre-trained Transformer)** æ˜¯ç”±OpenAIæå‡ºçš„åŸºäºTransformer Decoderçš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„Encoder-Decoderæ¶æ„ä¸åŒï¼ŒGPTé‡‡ç”¨çº¯Decoderæ¶æ„ï¼Œé€šè¿‡å› æœæ©ç å®ç°è‡ªå›å½’ç”Ÿæˆï¼Œæˆä¸ºäº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦èŒƒå¼ã€‚

### ğŸ¯ æœ¬é˜¶æ®µå­¦ä¹ ç›®æ ‡

- ç†è§£è‡ªå›å½’è¯­è¨€å»ºæ¨¡çš„æ ¸å¿ƒæ€æƒ³
- æŒæ¡å› æœæ©ç ï¼ˆCausal Maskï¼‰çš„å®ç°åŸç†
- å­¦ä¹ GPTçš„é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼
- å®ç°ç®€åŒ–ç‰ˆGPTæ¨¡å‹ï¼ˆGPT-Miniï¼‰
- ç†è§£ä½ç½®ç¼–ç åœ¨ç”Ÿæˆå¼ä»»åŠ¡ä¸­çš„é‡è¦æ€§
- æŒæ¡æ–‡æœ¬ç”Ÿæˆçš„å„ç§ç­–ç•¥å’ŒæŠ€æœ¯

## ğŸ—ï¸ GPTæ¶æ„è¯¦è§£

### æ•´ä½“æ¶æ„æ¦‚è§ˆ

```
è¾“å…¥æ–‡æœ¬ â†’ Token Embedding + Position Embedding
    â†“
Transformer Decoder Block 1
â”œâ”€â”€ Masked Multi-Head Self-Attention (å› æœæ©ç )
â”œâ”€â”€ Add & Norm
â”œâ”€â”€ Feed Forward Network
â””â”€â”€ Add & Norm
    â†“
Transformer Decoder Block 2
â”œâ”€â”€ Masked Multi-Head Self-Attention
â”œâ”€â”€ Add & Norm  
â”œâ”€â”€ Feed Forward Network
â””â”€â”€ Add & Norm
    â†“
... (é‡å¤ N å±‚)
    â†“
Layer Normalization
    â†“
Linear Head (Vocabulary Projection)
    â†“
Softmax â†’ ä¸‹ä¸€ä¸ªTokençš„æ¦‚ç‡åˆ†å¸ƒ
```

### æ ¸å¿ƒè®¾è®¡ç‰¹ç‚¹

1. **çº¯Decoderæ¶æ„**: ä¸ä½¿ç”¨Encoderï¼Œç›´æ¥åœ¨è¾“å…¥åºåˆ—ä¸Šè‡ªå›å½’å»ºæ¨¡
2. **å› æœæ©ç **: ç¡®ä¿é¢„æµ‹æ—¶åªèƒ½è®¿é—®å½“å‰ä½ç½®ä¹‹å‰çš„ä¿¡æ¯
3. **è‡ªç›‘ç£å­¦ä¹ **: é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªtokenè¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒ
4. **å¯æ‰©å±•æ€§**: é€šè¿‡å¢åŠ å±‚æ•°å’Œå‚æ•°å®ç°æ€§èƒ½æå‡

## ğŸ­ è‡ªå›å½’è¯­è¨€å»ºæ¨¡

### 1. åŸºæœ¬åŸç†

è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ˜¯å­¦ä¹ è¯­è¨€çš„æ¦‚ç‡åˆ†å¸ƒï¼š

```python
P(xâ‚, xâ‚‚, ..., xâ‚™) = âˆáµ¢â‚Œâ‚â¿ P(xáµ¢ | xâ‚, xâ‚‚, ..., xáµ¢â‚‹â‚)
```

**å…³é”®æ€æƒ³:**
- æ¯ä¸ªtokençš„é¢„æµ‹åªä¾èµ–äºå®ƒå‰é¢çš„tokens
- é€šè¿‡æœ€å¤§åŒ–æ¡ä»¶æ¦‚ç‡æ¥è®­ç»ƒæ¨¡å‹
- ç”Ÿæˆæ—¶é€ä¸ªé‡‡æ ·ä¸‹ä¸€ä¸ªtoken

### 2. è®­ç»ƒç›®æ ‡å‡½æ•°

**è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (Negative Log-Likelihood Loss):**

```python
L = -âˆ‘áµ¢â‚Œâ‚â¿ log P(xáµ¢ | xâ‚, ..., xáµ¢â‚‹â‚; Î¸)

å…¶ä¸­:
- Î¸: æ¨¡å‹å‚æ•°
- xáµ¢: ç¬¬iä¸ªtoken
- P(xáµ¢ | context): ç»™å®šä¸Šä¸‹æ–‡çš„æ¡ä»¶æ¦‚ç‡
```

**ä»£ç å®ç°:**
```python
def autoregressive_loss(logits, targets, ignore_index=-100):
    """
    è®¡ç®—è‡ªå›å½’è¯­è¨€æ¨¡å‹æŸå¤±
    
    Args:
        logits: æ¨¡å‹è¾“å‡º [batch_size, seq_len, vocab_size]
        targets: ç›®æ ‡tokens [batch_size, seq_len]
        ignore_index: å¿½ç•¥çš„tokenç´¢å¼• (å¦‚padding)
    """
    # ç§»ä½ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªtoken
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = targets[..., 1:].contiguous()
    
    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    loss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)
    loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), 
                   shift_labels.view(-1))
    
    return loss
```

## ğŸ”’ å› æœæ©ç æœºåˆ¶

### 1. ç†è®ºåŸºç¡€

å› æœæ©ç ç¡®ä¿æ¨¡å‹åœ¨é¢„æµ‹ä½ç½®içš„tokenæ—¶ï¼Œåªèƒ½çœ‹åˆ°ä½ç½®1åˆ°i-1çš„ä¿¡æ¯ï¼š

```python
# å› æœæ©ç çŸ©é˜µ (ä¸‹ä¸‰è§’çŸ©é˜µ)
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) == 0

# ç¤ºä¾‹: seq_len = 4
[[True,  False, False, False],   # ä½ç½®0åªçœ‹è‡ªå·±
 [True,  True,  False, False],   # ä½ç½®1çœ‹0,1
 [True,  True,  True,  False],   # ä½ç½®2çœ‹0,1,2  
 [True,  True,  True,  True ]]   # ä½ç½®3çœ‹0,1,2,3
```

### 2. å®ç°æ–¹å¼

**æ–¹æ³•1: æ©ç çŸ©é˜µ**
```python
def create_causal_mask(seq_len, device):
    """åˆ›å»ºå› æœæ©ç """
    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)
    return mask == 0  # ä¸‹ä¸‰è§’ä¸ºTrue

def apply_causal_mask(attention_scores, mask):
    """åº”ç”¨å› æœæ©ç åˆ°æ³¨æ„åŠ›åˆ†æ•°"""
    attention_scores = attention_scores.masked_fill(~mask, -torch.inf)
    return attention_scores
```

**æ–¹æ³•2: æ³¨æ„åŠ›åç½®**
```python
class CausalSelfAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.qkv = nn.Linear(d_model, 3 * d_model)
        self.proj = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
        
        # æ³¨å†Œå› æœæ©ç ä¸ºbuffer (ä¸å‚ä¸è®­ç»ƒ)
        self.register_buffer("causal_mask", 
                           torch.tril(torch.ones(1024, 1024)).view(1, 1, 1024, 1024))
    
    def forward(self, x):
        B, T, C = x.size()
        
        # è®¡ç®—QKV
        qkv = self.qkv(x).view(B, T, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # æ³¨æ„åŠ›åˆ†æ•°
        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # åº”ç”¨å› æœæ©ç 
        att = att.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float('-inf'))
        
        # Softmaxå’Œdropout
        att = F.softmax(att, dim=-1)
        att = self.dropout(att)
        
        # åº”ç”¨åˆ°values
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        
        return self.proj(y)
```

## ğŸ§  GPTæ¨¡å‹æ¶æ„è¯¦è§£

### 1. GPT Blockç»“æ„

æ¯ä¸ªGPT BlockåŒ…å«ï¼š

```python
class GPTBlock(nn.Module):
    """GPT Transformer Block"""
    
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # Layer Normalization (Pre-norm)
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
        # Causal Self-Attention
        self.attention = CausalSelfAttention(d_model, n_heads, dropout)
        
        # Feed Forward Network
        self.mlp = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),  # GPTä½¿ç”¨GELUæ¿€æ´»å‡½æ•°
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        # Pre-normalization + residual connection
        x = x + self.attention(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x
```

### 2. å®Œæ•´GPTæ¨¡å‹

```python
class GPT(nn.Module):
    """ç®€åŒ–ç‰ˆGPTæ¨¡å‹"""
    
    def __init__(self, vocab_size, d_model, n_heads, n_layers, 
                 max_seq_len=1024, dropout=0.1):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # Tokenå’Œä½ç½®åµŒå…¥
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Transformer blocks
        self.blocks = nn.ModuleList([
            GPTBlock(d_model, n_heads, 4*d_model, dropout) 
            for _ in range(n_layers)
        ])
        
        # æœ€ç»ˆlayer norm
        self.ln_f = nn.LayerNorm(d_model)
        
        # è¾“å‡ºhead
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
        # æƒé‡å…±äº« (å¯é€‰)
        self.lm_head.weight = self.token_embedding.weight
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(self, input_ids, position_ids=None):
        B, T = input_ids.size()
        assert T <= self.max_seq_len, f"åºåˆ—é•¿åº¦ {T} è¶…è¿‡æœ€å¤§é•¿åº¦ {self.max_seq_len}"
        
        # ä½ç½®ç¼–ç 
        if position_ids is None:
            position_ids = torch.arange(0, T, dtype=torch.long, device=input_ids.device)
        
        # åµŒå…¥
        tok_emb = self.token_embedding(input_ids)
        pos_emb = self.position_embedding(position_ids)
        x = tok_emb + pos_emb
        
        # é€šè¿‡transformer blocks
        for block in self.blocks:
            x = block(x)
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        x = self.ln_f(x)
        
        # è¾“å‡ºlogits
        logits = self.lm_head(x)
        
        return logits
```

## ğŸ¯ é¢„è®­ç»ƒç­–ç•¥

### 1. æ•°æ®å‡†å¤‡

**æ–‡æœ¬é¢„å¤„ç†:**
```python
def prepare_training_data(texts, tokenizer, max_length=1024):
    """
    å‡†å¤‡GPTé¢„è®­ç»ƒæ•°æ®
    
    Args:
        texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
    """
    
    # åˆ†è¯
    all_tokens = []
    for text in texts:
        tokens = tokenizer.encode(text)
        all_tokens.extend(tokens)
    
    # åˆ†å‰²æˆåºåˆ—
    sequences = []
    for i in range(0, len(all_tokens) - max_length, max_length):
        sequence = all_tokens[i:i + max_length]
        sequences.append(sequence)
    
    return sequences

class TextDataset(Dataset):
    """GPTè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, sequences):
        self.sequences = sequences
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        sequence = torch.tensor(self.sequences[idx], dtype=torch.long)
        return sequence
```

### 2. è®­ç»ƒå¾ªç¯

```python
def train_gpt(model, dataloader, optimizer, scheduler, num_epochs):
    """GPTè®­ç»ƒå¾ªç¯"""
    
    model.train()
    
    for epoch in range(num_epochs):
        total_loss = 0
        
        for batch_idx, batch in enumerate(dataloader):
            input_ids = batch.to(device)
            
            # å‰å‘ä¼ æ’­
            logits = model(input_ids)
            
            # è®¡ç®—æŸå¤± (é¢„æµ‹ä¸‹ä¸€ä¸ªtoken)
            loss = autoregressive_loss(logits, input_ids)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        avg_loss = total_loss / len(dataloader)
        perplexity = math.exp(avg_loss)
        print(f'Epoch {epoch} completed, Avg Loss: {avg_loss:.4f}, PPL: {perplexity:.2f}')
```

## ğŸ¨ æ–‡æœ¬ç”Ÿæˆç­–ç•¥

### 1. è´ªå¿ƒè§£ç  (Greedy Decoding)

```python
@torch.no_grad()
def generate_greedy(model, tokenizer, prompt, max_length=100):
    """è´ªå¿ƒè§£ç ç”Ÿæˆæ–‡æœ¬"""
    model.eval()
    
    # ç¼–ç prompt
    input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)
    
    for _ in range(max_length):
        # å‰å‘ä¼ æ’­
        logits = model(input_ids)
        
        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token
        next_token = logits[0, -1, :].argmax(dim=-1).unsqueeze(0).unsqueeze(0)
        
        # æ·»åŠ åˆ°åºåˆ—
        input_ids = torch.cat([input_ids, next_token], dim=1)
        
        # æ£€æŸ¥ç»“æŸæ¡ä»¶
        if next_token.item() == tokenizer.eos_token_id:
            break
    
    return tokenizer.decode(input_ids[0].tolist())
```

### 2. éšæœºé‡‡æ · (Random Sampling)

```python
@torch.no_grad()
def generate_sample(model, tokenizer, prompt, max_length=100, 
                   temperature=1.0, top_k=None, top_p=None):
    """éšæœºé‡‡æ ·ç”Ÿæˆæ–‡æœ¬"""
    model.eval()
    
    input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)
    
    for _ in range(max_length):
        logits = model(input_ids)
        next_token_logits = logits[0, -1, :] / temperature
        
        # Top-kè¿‡æ»¤
        if top_k is not None:
            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
            next_token_logits[next_token_logits < top_k_logits[-1]] = -float('inf')
        
        # Top-p (nucleus) è¿‡æ»¤
        if top_p is not None:
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
            sorted_indices_to_remove[0] = 0
            
            indices_to_remove = sorted_indices[sorted_indices_to_remove]
            next_token_logits[indices_to_remove] = -float('inf')
        
        # é‡‡æ ·
        probs = F.softmax(next_token_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)
        
        if next_token.item() == tokenizer.eos_token_id:
            break
    
    return tokenizer.decode(input_ids[0].tolist())
```

### 3. æŸæœç´¢ (Beam Search)

```python
@torch.no_grad()
def generate_beam_search(model, tokenizer, prompt, max_length=100, 
                        beam_size=5, length_penalty=1.0):
    """æŸæœç´¢ç”Ÿæˆæ–‡æœ¬"""
    model.eval()
    
    input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)
    batch_size, seq_len = input_ids.size()
    
    # åˆå§‹åŒ–beams
    beams = [(input_ids, 0.0)]  # (sequence, score)
    
    for _ in range(max_length):
        new_beams = []
        
        for seq, score in beams:
            if seq[0, -1].item() == tokenizer.eos_token_id:
                new_beams.append((seq, score))
                continue
            
            # å‰å‘ä¼ æ’­
            logits = model(seq)
            next_token_logits = logits[0, -1, :]
            log_probs = F.log_softmax(next_token_logits, dim=-1)
            
            # è·å–top-k candidates
            top_log_probs, top_indices = torch.topk(log_probs, beam_size)
            
            for log_prob, token_id in zip(top_log_probs, top_indices):
                new_seq = torch.cat([seq, token_id.unsqueeze(0).unsqueeze(0)], dim=1)
                new_score = score + log_prob.item()
                
                # é•¿åº¦æƒ©ç½š
                if length_penalty != 1.0:
                    new_score = new_score / (new_seq.size(1) ** length_penalty)
                
                new_beams.append((new_seq, new_score))
        
        # ä¿ç•™æœ€å¥½çš„beams
        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]
    
    # è¿”å›æœ€ä½³åºåˆ—
    best_seq = beams[0][0]
    return tokenizer.decode(best_seq[0].tolist())
```

## ğŸ”§ å¾®è°ƒæŠ€æœ¯

### 1. ä»»åŠ¡ç‰¹å®šå¾®è°ƒ

```python
class GPTForClassification(nn.Module):
    """ç”¨äºåˆ†ç±»ä»»åŠ¡çš„GPTå¾®è°ƒ"""
    
    def __init__(self, gpt_model, num_classes):
        super().__init__()
        self.gpt = gpt_model
        self.classifier = nn.Linear(gpt_model.d_model, num_classes)
        
        # å†»ç»“éƒ¨åˆ†å±‚ (å¯é€‰)
        for param in self.gpt.blocks[:-2].parameters():
            param.requires_grad = False
    
    def forward(self, input_ids):
        # è·å–GPTçš„éšè—çŠ¶æ€
        hidden_states = self.gpt.blocks[0](self.gpt.token_embedding(input_ids) + 
                                          self.gpt.position_embedding(torch.arange(input_ids.size(1))))
        
        for block in self.gpt.blocks[1:]:
            hidden_states = block(hidden_states)
        
        hidden_states = self.gpt.ln_f(hidden_states)
        
        # ä½¿ç”¨CLS tokenæˆ–æœ€åä¸€ä¸ªtokenè¿›è¡Œåˆ†ç±»
        pooled_output = hidden_states[:, -1, :]  # æœ€åä¸€ä¸ªtoken
        logits = self.classifier(pooled_output)
        
        return logits
```

### 2. LoRA (Low-Rank Adaptation)

```python
class LoRALinear(nn.Module):
    """LoRAé€‚é…çš„çº¿æ€§å±‚"""
    
    def __init__(self, in_features, out_features, rank=16, alpha=16):
        super().__init__()
        
        # å†»ç»“åŸå§‹æƒé‡
        self.linear = nn.Linear(in_features, out_features)
        self.linear.weight.requires_grad = False
        
        # LoRAå‚æ•°
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        self.scaling = alpha / rank
        
    def forward(self, x):
        # åŸå§‹è¾“å‡º + LoRAé€‚é…
        return self.linear(x) + (x @ self.lora_A.T @ self.lora_B.T) * self.scaling

def apply_lora_to_gpt(model, rank=16, alpha=16):
    """å°†LoRAåº”ç”¨åˆ°GPTæ¨¡å‹"""
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and 'lm_head' not in name:
            # æ›¿æ¢çº¿æ€§å±‚ä¸ºLoRAç‰ˆæœ¬
            lora_layer = LoRALinear(
                module.in_features, 
                module.out_features, 
                rank=rank, 
                alpha=alpha
            )
            lora_layer.linear.weight.data = module.weight.data
            if module.bias is not None:
                lora_layer.linear.bias.data = module.bias.data
            
            # æ›¿æ¢æ¨¡å—
            parent = model
            for attr in name.split('.')[:-1]:
                parent = getattr(parent, attr)
            setattr(parent, name.split('.')[-1], lora_layer)
```

## ğŸ“Š æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

### 1. å›°æƒ‘åº¦ (Perplexity)

```python
def compute_perplexity(model, dataloader):
    """è®¡ç®—æ¨¡å‹å›°æƒ‘åº¦"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch.to(device)
            logits = model(input_ids)
            
            # è®¡ç®—æŸå¤±
            loss = autoregressive_loss(logits, input_ids)
            
            # ç´¯è®¡ç»Ÿè®¡
            total_loss += loss.item() * input_ids.numel()
            total_tokens += input_ids.numel()
    
    avg_loss = total_loss / total_tokens
    perplexity = math.exp(avg_loss)
    
    return perplexity
```

### 2. ç”Ÿæˆè´¨é‡è¯„ä¼°

```python
def evaluate_generation_quality(model, tokenizer, test_prompts):
    """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
    
    results = {
        'diversity': [],
        'coherence': [],
        'fluency': []
    }
    
    for prompt in test_prompts:
        # ç”Ÿæˆå¤šä¸ªæ ·æœ¬
        samples = []
        for _ in range(5):
            generated = generate_sample(model, tokenizer, prompt, temperature=0.8)
            samples.append(generated)
        
        # å¤šæ ·æ€§: ç‹¬ç‰¹n-gramçš„æ¯”ä¾‹
        all_bigrams = set()
        unique_bigrams = set()
        
        for sample in samples:
            tokens = tokenizer.encode(sample)
            bigrams = list(zip(tokens[:-1], tokens[1:]))
            all_bigrams.update(bigrams)
            unique_bigrams.update(set(bigrams))
        
        diversity = len(unique_bigrams) / len(all_bigrams) if all_bigrams else 0
        results['diversity'].append(diversity)
    
    return {key: np.mean(values) for key, values in results.items()}
```

## ğŸš€ æ‰©å±•å’Œä¼˜åŒ–

### 1. æ¨¡å‹æ¶æ„æ”¹è¿›

**RoPEä½ç½®ç¼–ç :**
```python
class RotaryPositionalEmbedding(nn.Module):
    """æ—‹è½¬ä½ç½®ç¼–ç """
    
    def __init__(self, dim, max_seq_len=2048, base=10000):
        super().__init__()
        
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
        
    def forward(self, x, seq_len):
        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
        freqs = torch.einsum('i,j->ij', t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        
        cos_emb = emb.cos()
        sin_emb = emb.sin()
        
        return cos_emb, sin_emb

def apply_rotary_pos_emb(q, k, cos, sin):
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç """
    def rotate_half(x):
        x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
        return torch.cat((-x2, x1), dim=-1)
    
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    
    return q_embed, k_embed
```

### 2. è®­ç»ƒä¼˜åŒ–

**æ¢¯åº¦æ£€æŸ¥ç‚¹:**
```python
import torch.utils.checkpoint as checkpoint

class CheckpointGPTBlock(nn.Module):
    """ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹çš„GPT Block"""
    
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        self.attention = CausalSelfAttention(d_model, n_heads, dropout)
        self.mlp = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        # ä½¿ç”¨æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜ä½¿ç”¨
        x = x + checkpoint.checkpoint(self.attention, self.ln1(x))
        x = x + checkpoint.checkpoint(self.mlp, self.ln2(x))
        return x
```

## ğŸ“š å®é™…åº”ç”¨åœºæ™¯

### 1. å¯¹è¯ç³»ç»Ÿ

```python
class ChatGPT(nn.Module):
    """å¯¹è¯ç‰ˆGPT"""
    
    def __init__(self, gpt_model, tokenizer):
        super().__init__()
        self.gpt = gpt_model
        self.tokenizer = tokenizer
    
    def chat(self, message, history=None, max_response_length=100):
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        if history is None:
            history = []
        
        context = ""
        for human, assistant in history:
            context += f"Human: {human}\nAssistant: {assistant}\n"
        context += f"Human: {message}\nAssistant: "
        
        # ç”Ÿæˆå›å¤
        response = generate_sample(
            self.gpt, self.tokenizer, context,
            max_length=max_response_length,
            temperature=0.7,
            top_p=0.9
        )
        
        # æå–å›å¤éƒ¨åˆ†
        assistant_response = response.split("Assistant: ")[-1].split("Human:")[0].strip()
        
        return assistant_response
```

### 2. ä»£ç ç”Ÿæˆ

```python
def generate_code(model, tokenizer, description, language="python"):
    """åŸºäºæè¿°ç”Ÿæˆä»£ç """
    
    prompt = f"""
# Language: {language}
# Description: {description}
# Code:
"""
    
    generated = generate_sample(
        model, tokenizer, prompt,
        max_length=200,
        temperature=0.2,  # è¾ƒä½æ¸©åº¦ä¿è¯ä»£ç è´¨é‡
        top_p=0.95
    )
    
    # æå–ä»£ç éƒ¨åˆ†
    code_start = generated.find("# Code:\n") + len("# Code:\n")
    code = generated[code_start:].strip()
    
    return code
```

### 3. åˆ›æ„å†™ä½œ

```python
def creative_writing(model, tokenizer, genre, theme, length=500):
    """åˆ›æ„å†™ä½œåŠ©æ‰‹"""
    
    prompts = {
        "fantasy": f"In a world where magic exists, a story about {theme}:\n",
        "sci-fi": f"In the year 2150, a tale involving {theme}:\n", 
        "mystery": f"A mysterious case involving {theme}:\n"
    }
    
    prompt = prompts.get(genre, f"A story about {theme}:\n")
    
    story = generate_sample(
        model, tokenizer, prompt,
        max_length=length,
        temperature=0.8,  # è¾ƒé«˜æ¸©åº¦å¢åŠ åˆ›é€ æ€§
        top_p=0.9
    )
    
    return story
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. è®­ç»ƒæŠ€å·§

- **æ•°æ®é¢„å¤„ç†**: æ¸…æ´—æ–‡æœ¬ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼Œç»´æŠ¤è¯æ±‡è¡¨
- **æ‰¹æ¬¡æ‰“åŒ…**: å°†ç›¸ä¼¼é•¿åº¦çš„åºåˆ—æ‰“åŒ…ä»¥æé«˜æ•ˆç‡
- **å­¦ä¹ ç‡è°ƒåº¦**: ä½¿ç”¨warmup + cosine decay
- **æ­£åˆ™åŒ–**: åº”ç”¨dropoutï¼Œæƒé‡è¡°å‡ï¼Œæ ‡ç­¾å¹³æ»‘

### 2. æ¨ç†ä¼˜åŒ–

- **KV-Cache**: ç¼“å­˜attentionçš„key-valueä»¥åŠ é€Ÿç”Ÿæˆ
- **æ¨¡å‹é‡åŒ–**: ä½¿ç”¨INT8æˆ–FP16å‡å°‘å†…å­˜ä½¿ç”¨
- **æ‰¹é‡æ¨ç†**: å¹¶è¡Œå¤„ç†å¤šä¸ªè¯·æ±‚

### 3. æ¨¡å‹éƒ¨ç½²

- **æ¨¡å‹è’¸é¦**: è®­ç»ƒå°æ¨¡å‹æ¨¡ä»¿å¤§æ¨¡å‹
- **åŠ¨æ€æ‰¹å¤„ç†**: æ ¹æ®è¯·æ±‚åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
- **æ¨¡å‹å¹¶è¡Œ**: åœ¨å¤šGPUä¸Šåˆ†å¸ƒæ¨¡å‹å±‚

## ğŸ”® æœªæ¥å‘å±•æ–¹å‘

1. **æ›´å¤§è§„æ¨¡**: å‚æ•°é‡æŒç»­å¢é•¿ï¼Œä»GPT-1çš„117Måˆ°GPT-3çš„175B
2. **å¤šæ¨¡æ€èåˆ**: ç»“åˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€
3. **æ•ˆç‡æå‡**: MoEã€ç¨€ç–æ³¨æ„åŠ›ã€çº¿æ€§æ³¨æ„åŠ›ç­‰æŠ€æœ¯
4. **å¯¹é½æŠ€æœ¯**: RLHFã€Constitutional AIç­‰äººç±»ä»·å€¼å¯¹é½
5. **ä¸“ä¸šé¢†åŸŸ**: åŒ»ç–—ã€æ³•å¾‹ã€ç§‘å­¦ç­‰å‚ç›´é¢†åŸŸçš„ä¸“ç”¨æ¨¡å‹

## ğŸ‰ å°ç»“

GPTçš„æå‡ºæ ‡å¿—ç€ç”Ÿæˆå¼AIçš„é‡è¦çªç ´ï¼š

### æ ¸å¿ƒè´¡çŒ®
1. **ç®€åŒ–æ¶æ„**: çº¯Decoderè®¾è®¡ï¼Œå»é™¤Encoderå¤æ‚æ€§
2. **è‡ªå›å½’å»ºæ¨¡**: é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªtokenå­¦ä¹ è¯­è¨€æ¨¡å¼
3. **å¯æ‰©å±•æ€§**: è¯æ˜äº†"è§„æ¨¡æ³•åˆ™"åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§
4. **é¢„è®­ç»ƒèŒƒå¼**: å»ºç«‹äº†é¢„è®­ç»ƒ+å¾®è°ƒçš„æ ‡å‡†æµç¨‹

### æŠ€æœ¯åˆ›æ–°
- å› æœæ©ç ç¡®ä¿è‡ªå›å½’ç‰¹æ€§
- ä½ç½®ç¼–ç å¤„ç†åºåˆ—ä¿¡æ¯
- å¤šå±‚Transformeræ•è·å¤æ‚è¯­è¨€æ¨¡å¼
- å¤šæ ·åŒ–ç”Ÿæˆç­–ç•¥æ»¡è¶³ä¸åŒéœ€æ±‚

### å½±å“æ„ä¹‰
- æ¨åŠ¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•æµªæ½®
- ä¸ºChatGPTç­‰åº”ç”¨å¥ å®šäº†æŠ€æœ¯åŸºç¡€
- å±•ç¤ºäº†æ— ç›‘ç£å­¦ä¹ çš„å¼ºå¤§æ½œåŠ›
- å¯å‘äº†ä¼—å¤šä¸‹æ¸¸åº”ç”¨å’Œç ”ç©¶æ–¹å‘

---

**ä¸‹ä¸€é˜¶æ®µ**: è¿›å…¥GPTæ¨¡å‹çš„å®é™…å®ç°å’Œå®éªŒé˜¶æ®µ

é€šè¿‡æ·±å…¥å­¦ä¹ GPTï¼Œæ‚¨å°†æŒæ¡ç°ä»£ç”Ÿæˆå¼AIçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä¸ºç†è§£å’Œå¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹æ‰“ä¸‹åšå®åŸºç¡€ï¼