"""
åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒæ¨¡å—
================

æä¾›å®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®å¹¶è¡Œè®­ç»ƒï¼ˆDDPï¼‰
- æ¨¡å‹å¹¶è¡Œè®­ç»ƒ
- ç®¡é“å¹¶è¡Œè®­ç»ƒ
- æ··åˆç²¾åº¦è®­ç»ƒ
- åˆ†å¸ƒå¼æ•°æ®åŠ è½½
- æ¢¯åº¦åŒæ­¥å’Œé€šä¿¡

ç»„ä»¶è¯´æ˜:
- distributed_training: æ ¸å¿ƒåˆ†å¸ƒå¼è®­ç»ƒå®ç°
- training_utils: è®­ç»ƒå·¥å…·å’Œè¾…åŠ©å‡½æ•°
- optimization: ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦

ä½¿ç”¨æ–¹æ³•:
    from training import DistributedTrainer, DistributedConfig
    
    # é…ç½®åˆ†å¸ƒå¼è®­ç»ƒ
    config = DistributedConfig(world_size=4, batch_size=32)
    
    # åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒå™¨
    trainer = DistributedTrainer(model, config, strategy='ddp')
    trainer.setup()
    
    # è®­ç»ƒ
    trainer.train_step(batch, optimizer, criterion)

ç‰ˆæœ¬: 1.0.0
"""

# åˆ†å¸ƒå¼è®­ç»ƒå¯¼å…¥
try:
    from .distributed_training import (
        DistributedTrainer,
        DistributedConfig,
        DistributedDataLoader,
        BaseDistributedStrategy,
        DataParallelStrategy,
        ModelParallelStrategy,
        PipelineParallelStrategy,
        demo as distributed_demo
    )
    DISTRIBUTED_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å—: {e}")
    DistributedTrainer = DistributedConfig = DistributedDataLoader = None
    BaseDistributedStrategy = DataParallelStrategy = None
    ModelParallelStrategy = PipelineParallelStrategy = None
    distributed_demo = None
    DISTRIBUTED_AVAILABLE = False

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "1.0.0"
__author__ = "LLM Implementation Team"
__description__ = "Distributed training support for large language models"

# å¯¼å‡ºçš„å…¬å…±æ¥å£
__all__ = [
    # åˆ†å¸ƒå¼è®­ç»ƒ
    'DistributedTrainer',
    'DistributedConfig',
    'DistributedDataLoader',
    
    # åˆ†å¸ƒå¼ç­–ç•¥
    'BaseDistributedStrategy',
    'DataParallelStrategy',
    'ModelParallelStrategy',
    'PipelineParallelStrategy',
    
    # æ¼”ç¤ºå‡½æ•°
    'distributed_demo',
    'demo',
    
    # å¯ç”¨æ€§æ ‡å¿—
    'DISTRIBUTED_AVAILABLE',
]


def get_training_info():
    """è·å–è®­ç»ƒç³»ç»Ÿä¿¡æ¯"""
    return {
        "name": "Distributed Training System",
        "version": __version__,
        "description": __description__,
        "author": __author__,
        "components": {
            "distributed_training": DISTRIBUTED_AVAILABLE,
        },
        "supported_strategies": [
            "ddp", "data_parallel", "model_parallel", "pipeline_parallel"
        ],
        "supported_backends": [
            "nccl", "gloo", "mpi"
        ],
        "features": [
            "Multi-GPU training", "Multi-node training",
            "Mixed precision", "Gradient accumulation",
            "Model checkpointing", "Distributed data loading"
        ]
    }


def list_available_strategies():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ†å¸ƒå¼ç­–ç•¥"""
    strategies = []
    
    if DISTRIBUTED_AVAILABLE:
        strategies.extend([
            {
                "name": "ddp",
                "description": "åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆæ¨èï¼‰",
                "type": "data_parallel",
                "gpu_requirement": "å¤šGPU",
                "memory_efficiency": "é«˜",
                "communication_overhead": "ä½",
                "available": True
            },
            {
                "name": "data_parallel",
                "description": "æ•°æ®å¹¶è¡Œè®­ç»ƒ",
                "type": "data_parallel", 
                "gpu_requirement": "å¤šGPU",
                "memory_efficiency": "é«˜",
                "communication_overhead": "ä½",
                "available": True
            },
            {
                "name": "model_parallel",
                "description": "æ¨¡å‹å¹¶è¡Œè®­ç»ƒ",
                "type": "model_parallel",
                "gpu_requirement": "å¤šGPU",
                "memory_efficiency": "ä¸­ç­‰",
                "communication_overhead": "é«˜",
                "available": True
            },
            {
                "name": "pipeline_parallel",
                "description": "ç®¡é“å¹¶è¡Œè®­ç»ƒ",
                "type": "pipeline_parallel",
                "gpu_requirement": "å¤šGPU/å¤šèŠ‚ç‚¹",
                "memory_efficiency": "é«˜",
                "communication_overhead": "ä¸­ç­‰",
                "available": True
            }
        ])
    
    return strategies


def create_distributed_trainer(
    model,
    strategy: str = "ddp",
    world_size: int = 1,
    rank: int = 0,
    batch_size: int = 32,
    **kwargs
):
    """
    åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒå™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model: PyTorchæ¨¡å‹
        strategy: åˆ†å¸ƒå¼ç­–ç•¥ ("ddp", "model_parallel", "pipeline_parallel")
        world_size: ä¸–ç•Œå¤§å°ï¼ˆè¿›ç¨‹æ€»æ•°ï¼‰
        rank: å½“å‰è¿›ç¨‹çš„rank
        batch_size: æ‰¹æ¬¡å¤§å°
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„åˆ†å¸ƒå¼è®­ç»ƒå™¨
        
    Example:
        >>> trainer = create_distributed_trainer(
        ...     model, strategy="ddp", world_size=4, batch_size=32
        ... )
        >>> trainer.setup()
        >>> trainer.train_step(batch, optimizer, criterion)
    """
    if not DISTRIBUTED_AVAILABLE:
        raise ImportError("åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å—ä¸å¯ç”¨")
    
    # åˆ›å»ºé…ç½®
    config = DistributedConfig(
        world_size=world_size,
        rank=rank,
        batch_size=batch_size,
        **kwargs
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DistributedTrainer(model, config, strategy=strategy)
    
    return trainer


def create_distributed_dataloader(
    dataset,
    batch_size: int,
    world_size: int = 1,
    rank: int = 0,
    **kwargs
):
    """
    åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        dataset: æ•°æ®é›†
        batch_size: æ‰¹æ¬¡å¤§å°
        world_size: ä¸–ç•Œå¤§å°
        rank: å½“å‰è¿›ç¨‹rank
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        åˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨
        
    Example:
        >>> dataloader = create_distributed_dataloader(
        ...     dataset, batch_size=32, world_size=4
        ... )
        >>> loader = dataloader.setup()
    """
    if not DISTRIBUTED_AVAILABLE:
        raise ImportError("åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å—ä¸å¯ç”¨")
    
    config = DistributedConfig(world_size=world_size, rank=rank)
    
    return DistributedDataLoader(
        dataset=dataset,
        batch_size=batch_size,
        config=config,
        **kwargs
    )


def demo():
    """è¿è¡Œå®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒæ¼”ç¤º"""
    print("ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿå®Œæ•´æ¼”ç¤º")
    print("=" * 60)
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    info = get_training_info()
    print("ğŸ“‹ ç³»ç»Ÿä¿¡æ¯:")
    print(f"  åç§°: {info['name']}")
    print(f"  ç‰ˆæœ¬: {info['version']}")
    print(f"  æè¿°: {info['description']}")
    
    print(f"\nğŸ§© ç»„ä»¶çŠ¶æ€:")
    for component, available in info['components'].items():
        status = "âœ… å¯ç”¨" if available else "âŒ ä¸å¯ç”¨"
        print(f"  {component}: {status}")
    
    # æ˜¾ç¤ºæ”¯æŒçš„ç­–ç•¥
    print(f"\nğŸ”§ æ”¯æŒçš„åˆ†å¸ƒå¼ç­–ç•¥:")
    strategies = list_available_strategies()
    for strategy in strategies:
        print(f"  â€¢ {strategy['name']}: {strategy['description']}")
        print(f"    ç±»å‹: {strategy['type']}, GPUè¦æ±‚: {strategy['gpu_requirement']}")
        print(f"    å†…å­˜æ•ˆç‡: {strategy['memory_efficiency']}, é€šä¿¡å¼€é”€: {strategy['communication_overhead']}")
    
    # æ˜¾ç¤ºæ”¯æŒçš„ç‰¹æ€§
    print(f"\nğŸ¯ æ”¯æŒçš„ç‰¹æ€§:")
    for feature in info['features']:
        print(f"  â€¢ {feature}")
    
    # è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒæ¼”ç¤º
    if DISTRIBUTED_AVAILABLE and distributed_demo:
        print(f"\n" + "="*60)
        print("ğŸ”§ åˆ†å¸ƒå¼è®­ç»ƒæ¼”ç¤º")
        print("="*60)
        try:
            distributed_demo()
        except Exception as e:
            print(f"âŒ åˆ†å¸ƒå¼è®­ç»ƒæ¼”ç¤ºå¤±è´¥: {e}")
    
    print(f"\n" + "="*60)
    print("ğŸ‰ åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿæ¼”ç¤ºå®Œæˆ!")
    print("="*60)


if __name__ == "__main__":
    demo()