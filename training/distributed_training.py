"""
åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒæ¨¡å—
================

å®ç°åˆ†å¸ƒå¼è®­ç»ƒçš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼‰
- æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰
- åˆ†å¸ƒå¼æ•°æ®åŠ è½½
- å¤šGPU/å¤šèŠ‚ç‚¹è®­ç»ƒæ”¯æŒ
- æ¢¯åº¦åŒæ­¥å’Œé€šä¿¡

æ”¯æŒçš„åˆ†å¸ƒå¼ç­–ç•¥ï¼š
- PyTorch DDP (DistributedDataParallel)
- æ¨¡å‹åˆ†ç‰‡ (Model Sharding)
- ç®¡é“å¹¶è¡Œ (Pipeline Parallelism)
- æ··åˆç²¾åº¦è®­ç»ƒ

ä½¿ç”¨æ–¹æ³•:
    from training.distributed_training import DistributedTrainer
    
    trainer = DistributedTrainer(model, strategy='ddp')
    trainer.setup_distributed()
    trainer.train(train_loader)
"""

import os
import logging
import multiprocessing as mp
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod

try:
    import torch
    import torch.nn as nn
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.utils.data import DataLoader, DistributedSampler
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = nn = dist = DDP = DataLoader = DistributedSampler = None
    
    # åˆ›å»ºæ¨¡æ‹Ÿç±»ç”¨äºç±»å‹æ£€æŸ¥
    class MockModule:
        def __init__(self, *args, **kwargs):
            pass
        def __call__(self, *args, **kwargs):
            return self
        def __getattr__(self, name):
            return MockModule()
    
    class MockTorch:
        class nn:
            Module = MockModule
        class optim:
            Optimizer = MockModule
    
    if torch is None:
        torch = MockTorch()
        nn = torch.nn

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class DistributedConfig:
    """åˆ†å¸ƒå¼è®­ç»ƒé…ç½®"""
    backend: str = "nccl"  # é€šä¿¡åç«¯ï¼šnccl, gloo, mpi
    world_size: int = 1    # æ€»è¿›ç¨‹æ•°
    rank: int = 0          # å½“å‰è¿›ç¨‹çš„rank
    local_rank: int = 0    # æœ¬åœ°GPUçš„rank
    master_addr: str = "localhost"
    master_port: str = "12355"
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 32
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0
    
    # ä¼˜åŒ–é…ç½®
    find_unused_parameters: bool = False
    bucket_cap_mb: int = 25
    
    # è°ƒè¯•é…ç½®
    debug: bool = False
    profile: bool = False


class BaseDistributedStrategy(ABC):
    """åŸºç¡€åˆ†å¸ƒå¼ç­–ç•¥ç±»"""
    
    def __init__(self, config: DistributedConfig):
        self.config = config
        self.is_initialized = False
    
    @abstractmethod
    def setup(self) -> None:
        """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
        pass
    
    @abstractmethod
    def wrap_model(self, model: nn.Module) -> nn.Module:
        """åŒ…è£…æ¨¡å‹ä»¥æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ"""
        pass
    
    @abstractmethod
    def cleanup(self) -> None:
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
        pass
    
    @abstractmethod
    def is_main_process(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
        pass
    
    @abstractmethod
    def synchronize(self) -> None:
        """åŒæ­¥æ‰€æœ‰è¿›ç¨‹"""
        pass


class DataParallelStrategy(BaseDistributedStrategy):
    """æ•°æ®å¹¶è¡Œç­–ç•¥ï¼ˆDDPï¼‰"""
    
    def __init__(self, config: DistributedConfig):
        super().__init__(config)
        self.name = "data_parallel"
    
    def setup(self) -> None:
        """åˆå§‹åŒ–DDPç¯å¢ƒ"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['MASTER_ADDR'] = self.config.master_addr
        os.environ['MASTER_PORT'] = self.config.master_port
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        if not dist.is_initialized():
            dist.init_process_group(
                backend=self.config.backend,
                world_size=self.config.world_size,
                rank=self.config.rank
            )
        
        # è®¾ç½®å½“å‰è®¾å¤‡
        if torch.cuda.is_available():
            torch.cuda.set_device(self.config.local_rank)
        
        self.is_initialized = True
        logger.info(f"DDPåˆå§‹åŒ–å®Œæˆ - Rank: {self.config.rank}, World Size: {self.config.world_size}")
    
    def wrap_model(self, model: nn.Module) -> nn.Module:
        """ç”¨DDPåŒ…è£…æ¨¡å‹"""
        if not self.is_initialized:
            raise RuntimeError("åˆ†å¸ƒå¼ç¯å¢ƒæœªåˆå§‹åŒ–")
        
        # å°†æ¨¡å‹ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
        if torch.cuda.is_available():
            device = torch.device(f'cuda:{self.config.local_rank}')
            model = model.to(device)
        
        # åŒ…è£…ä¸ºDDP
        ddp_model = DDP(
            model,
            device_ids=[self.config.local_rank] if torch.cuda.is_available() else None,
            output_device=self.config.local_rank if torch.cuda.is_available() else None,
            find_unused_parameters=self.config.find_unused_parameters,
            bucket_cap_mb=self.config.bucket_cap_mb
        )
        
        logger.info(f"æ¨¡å‹å·²åŒ…è£…ä¸ºDDP - Device: {device if torch.cuda.is_available() else 'cpu'}")
        return ddp_model
    
    def cleanup(self) -> None:
        """æ¸…ç†DDPç¯å¢ƒ"""
        if dist.is_initialized():
            dist.destroy_process_group()
        logger.info("DDPç¯å¢ƒå·²æ¸…ç†")
    
    def is_main_process(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰"""
        return self.config.rank == 0
    
    def synchronize(self) -> None:
        """åŒæ­¥æ‰€æœ‰è¿›ç¨‹"""
        if dist.is_initialized():
            dist.barrier()


class ModelParallelStrategy(BaseDistributedStrategy):
    """æ¨¡å‹å¹¶è¡Œç­–ç•¥"""
    
    def __init__(self, config: DistributedConfig):
        super().__init__(config)
        self.name = "model_parallel"
        self.device_map = {}
    
    def setup(self) -> None:
        """åˆå§‹åŒ–æ¨¡å‹å¹¶è¡Œç¯å¢ƒ"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
        
        # æ£€æŸ¥å¯ç”¨GPUæ•°é‡
        if not torch.cuda.is_available():
            raise RuntimeError("æ¨¡å‹å¹¶è¡Œéœ€è¦CUDAæ”¯æŒ")
        
        gpu_count = torch.cuda.device_count()
        if gpu_count < 2:
            logger.warning("æ¨¡å‹å¹¶è¡Œå»ºè®®ä½¿ç”¨å¤šä¸ªGPU")
        
        self.is_initialized = True
        logger.info(f"æ¨¡å‹å¹¶è¡Œåˆå§‹åŒ–å®Œæˆ - å¯ç”¨GPUæ•°: {gpu_count}")
    
    def wrap_model(self, model: nn.Module) -> nn.Module:
        """ä¸ºæ¨¡å‹å¹¶è¡Œåˆ†å‰²æ¨¡å‹"""
        if not self.is_initialized:
            raise RuntimeError("åˆ†å¸ƒå¼ç¯å¢ƒæœªåˆå§‹åŒ–")
        
        # ç®€åŒ–çš„æ¨¡å‹å¹¶è¡Œå®ç°
        # å®é™…å®ç°ä¸­éœ€è¦æ ¹æ®æ¨¡å‹ç»“æ„è¿›è¡Œæ›´å¤æ‚çš„åˆ†å‰²
        gpu_count = torch.cuda.device_count()
        
        if hasattr(model, 'layers') and hasattr(model.layers, '__len__'):
            # å‡è®¾æ¨¡å‹æœ‰layerså±æ€§
            layers_per_gpu = len(model.layers) // gpu_count
            
            for i, layer in enumerate(model.layers):
                device_id = min(i // layers_per_gpu, gpu_count - 1)
                layer.to(f'cuda:{device_id}')
                self.device_map[f'layer_{i}'] = device_id
            
            logger.info(f"æ¨¡å‹å·²åˆ†å‰²åˆ°{gpu_count}ä¸ªGPUä¸Š")
        else:
            # ç®€å•åœ°å°†æ•´ä¸ªæ¨¡å‹æ”¾åˆ°ç¬¬ä¸€ä¸ªGPU
            model.to('cuda:0')
            logger.info("æ¨¡å‹æ”¾ç½®åœ¨å•ä¸ªGPUä¸Šï¼ˆæœªè¿›è¡Œå±‚çº§åˆ†å‰²ï¼‰")
        
        return model
    
    def cleanup(self) -> None:
        """æ¸…ç†æ¨¡å‹å¹¶è¡Œç¯å¢ƒ"""
        self.device_map.clear()
        logger.info("æ¨¡å‹å¹¶è¡Œç¯å¢ƒå·²æ¸…ç†")
    
    def is_main_process(self) -> bool:
        """æ¨¡å‹å¹¶è¡Œä¸­æ€»æ˜¯è¿”å›True"""
        return True
    
    def synchronize(self) -> None:
        """æ¨¡å‹å¹¶è¡Œä¸éœ€è¦è¿›ç¨‹åŒæ­¥"""
        pass


class PipelineParallelStrategy(BaseDistributedStrategy):
    """ç®¡é“å¹¶è¡Œç­–ç•¥"""
    
    def __init__(self, config: DistributedConfig, num_stages: int = 2):
        super().__init__(config)
        self.name = "pipeline_parallel"
        self.num_stages = num_stages
        self.stage_id = config.rank % num_stages
    
    def setup(self) -> None:
        """åˆå§‹åŒ–ç®¡é“å¹¶è¡Œç¯å¢ƒ"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['MASTER_ADDR'] = self.config.master_addr
        os.environ['MASTER_PORT'] = self.config.master_port
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        if not dist.is_initialized():
            dist.init_process_group(
                backend=self.config.backend,
                world_size=self.config.world_size,
                rank=self.config.rank
            )
        
        self.is_initialized = True
        logger.info(f"ç®¡é“å¹¶è¡Œåˆå§‹åŒ–å®Œæˆ - Stage: {self.stage_id}/{self.num_stages}")
    
    def wrap_model(self, model: nn.Module) -> nn.Module:
        """ä¸ºç®¡é“å¹¶è¡Œåˆ†å‰²æ¨¡å‹"""
        if not self.is_initialized:
            raise RuntimeError("åˆ†å¸ƒå¼ç¯å¢ƒæœªåˆå§‹åŒ–")
        
        # ç®€åŒ–çš„ç®¡é“å¹¶è¡Œå®ç°
        # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„æ¨¡å‹åˆ†å‰²å’Œä¸­é—´æ¿€æ´»ä¼ é€’
        
        if torch.cuda.is_available():
            device = torch.device(f'cuda:{self.config.local_rank}')
            model = model.to(device)
        
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„ç®¡é“åˆ†å‰²é€»è¾‘
        # ç›®å‰åªæ˜¯ç®€å•åœ°å°†æ•´ä¸ªæ¨¡å‹æ”¾åœ¨æŒ‡å®šè®¾å¤‡ä¸Š
        
        logger.info(f"ç®¡é“é˜¶æ®µ{self.stage_id}çš„æ¨¡å‹å·²å‡†å¤‡")
        return model
    
    def cleanup(self) -> None:
        """æ¸…ç†ç®¡é“å¹¶è¡Œç¯å¢ƒ"""
        if dist.is_initialized():
            dist.destroy_process_group()
        logger.info("ç®¡é“å¹¶è¡Œç¯å¢ƒå·²æ¸…ç†")
    
    def is_main_process(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
        return self.config.rank == 0
    
    def synchronize(self) -> None:
        """åŒæ­¥æ‰€æœ‰è¿›ç¨‹"""
        if dist.is_initialized():
            dist.barrier()


class DistributedDataLoader:
    """åˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, 
                 dataset,
                 batch_size: int,
                 config: DistributedConfig,
                 shuffle: bool = True,
                 num_workers: int = 0,
                 pin_memory: bool = True):
        self.dataset = dataset
        self.batch_size = batch_size
        self.config = config
        self.shuffle = shuffle
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        
        self.sampler = None
        self.dataloader = None
    
    def setup(self) -> DataLoader:
        """åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchä¸å¯ç”¨")
        
        # åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨
        if self.config.world_size > 1:
            self.sampler = DistributedSampler(
                self.dataset,
                num_replicas=self.config.world_size,
                rank=self.config.rank,
                shuffle=self.shuffle
            )
            shuffle = False  # ä½¿ç”¨é‡‡æ ·å™¨æ—¶ä¸èƒ½åŒæ—¶shuffle
        else:
            self.sampler = None
            shuffle = self.shuffle
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.dataloader = DataLoader(
            self.dataset,
            batch_size=self.batch_size,
            sampler=self.sampler,
            shuffle=shuffle,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory and torch.cuda.is_available(),
            drop_last=True  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹çš„batchæ•°é‡ç›¸åŒ
        )
        
        logger.info(f"åˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨å·²åˆ›å»º - Batch Size: {self.batch_size}, Workers: {self.num_workers}")
        return self.dataloader
    
    def set_epoch(self, epoch: int) -> None:
        """è®¾ç½®epochï¼ˆç”¨äºåˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼‰"""
        if self.sampler is not None:
            self.sampler.set_epoch(epoch)


class DistributedTrainer:
    """åˆ†å¸ƒå¼è®­ç»ƒå™¨"""
    
    def __init__(self, 
                 model: nn.Module,
                 config: DistributedConfig,
                 strategy: str = "ddp"):
        self.model = model
        self.config = config
        self.strategy_name = strategy
        
        # åˆ›å»ºåˆ†å¸ƒå¼ç­–ç•¥
        self.strategy = self._create_strategy(strategy, config)
        self.wrapped_model = None
        
        # è®­ç»ƒçŠ¶æ€
        self.is_setup = False
    
    def _create_strategy(self, strategy: str, config: DistributedConfig) -> BaseDistributedStrategy:
        """åˆ›å»ºåˆ†å¸ƒå¼ç­–ç•¥"""
        strategies = {
            "ddp": DataParallelStrategy,
            "data_parallel": DataParallelStrategy,
            "model_parallel": ModelParallelStrategy,
            "pipeline_parallel": PipelineParallelStrategy,
        }
        
        if strategy not in strategies:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å¸ƒå¼ç­–ç•¥: {strategy}. å¯ç”¨ç­–ç•¥: {list(strategies.keys())}")
        
        return strategies[strategy](config)
    
    def setup(self) -> None:
        """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç­–ç•¥
        self.strategy.setup()
        
        # åŒ…è£…æ¨¡å‹
        self.wrapped_model = self.strategy.wrap_model(self.model)
        
        self.is_setup = True
        logger.info(f"åˆ†å¸ƒå¼è®­ç»ƒå™¨å·²è®¾ç½® - ç­–ç•¥: {self.strategy.name}")
    
    def train_step(self, 
                   batch: Dict[str, Any],
                   optimizer: torch.optim.Optimizer,
                   criterion: nn.Module,
                   scaler: Optional[Any] = None) -> Dict[str, float]:
        """å•æ­¥è®­ç»ƒ"""
        if not self.is_setup:
            raise RuntimeError("åˆ†å¸ƒå¼è®­ç»ƒå™¨æœªè®¾ç½®")
        
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchä¸å¯ç”¨")
        
        self.wrapped_model.train()
        
        # å‰å‘ä¼ æ’­
        if scaler is not None:
            # æ··åˆç²¾åº¦è®­ç»ƒ
            with torch.cuda.amp.autocast():
                outputs = self.wrapped_model(**batch)
                if isinstance(outputs, dict) and 'loss' in outputs:
                    loss = outputs['loss']
                else:
                    # å‡è®¾ç¬¬ä¸€ä¸ªè¾“å‡ºæ˜¯logits
                    logits = outputs[0] if isinstance(outputs, tuple) else outputs
                    labels = batch.get('labels')
                    loss = criterion(logits, labels) if labels is not None else logits.mean()
            
            # ç¼©æ”¾æŸå¤±
            scaled_loss = scaler.scale(loss)
            scaled_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.max_grad_norm > 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(self.wrapped_model.parameters(), self.config.max_grad_norm)
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            scaler.step(optimizer)
            scaler.update()
        else:
            # å¸¸è§„è®­ç»ƒ
            outputs = self.wrapped_model(**batch)
            if isinstance(outputs, dict) and 'loss' in outputs:
                loss = outputs['loss']
            else:
                logits = outputs[0] if isinstance(outputs, tuple) else outputs
                labels = batch.get('labels')
                loss = criterion(logits, labels) if labels is not None else logits.mean()
            
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(self.wrapped_model.parameters(), self.config.max_grad_norm)
            
            optimizer.step()
        
        optimizer.zero_grad()
        
        return {"loss": loss.item()}
    
    def evaluate_step(self, batch: Dict[str, Any]) -> Dict[str, float]:
        """å•æ­¥è¯„ä¼°"""
        if not self.is_setup:
            raise RuntimeError("åˆ†å¸ƒå¼è®­ç»ƒå™¨æœªè®¾ç½®")
        
        self.wrapped_model.eval()
        
        with torch.no_grad():
            outputs = self.wrapped_model(**batch)
            if isinstance(outputs, dict) and 'loss' in outputs:
                loss = outputs['loss']
            else:
                loss = outputs[0] if isinstance(outputs, tuple) else outputs
                loss = loss.mean()  # ç®€åŒ–å¤„ç†
        
        return {"loss": loss.item()}
    
    def save_checkpoint(self, filepath: str, optimizer: torch.optim.Optimizer, epoch: int) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if not self.strategy.is_main_process():
            return  # åªæœ‰ä¸»è¿›ç¨‹ä¿å­˜
        
        if not TORCH_AVAILABLE:
            return
        
        # è·å–åŸå§‹æ¨¡å‹çŠ¶æ€ï¼ˆå»é™¤DDPåŒ…è£…ï¼‰
        model_state = self.wrapped_model.module.state_dict() \
                     if hasattr(self.wrapped_model, 'module') \
                     else self.wrapped_model.state_dict()
        
        checkpoint = {
            'model_state_dict': model_state,
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': epoch,
            'config': self.config
        }
        
        torch.save(checkpoint, filepath)
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
    
    def load_checkpoint(self, filepath: str, optimizer: torch.optim.Optimizer) -> int:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not TORCH_AVAILABLE:
            return 0
        
        checkpoint = torch.load(filepath, map_location='cpu')
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        if hasattr(self.wrapped_model, 'module'):
            self.wrapped_model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.wrapped_model.load_state_dict(checkpoint['model_state_dict'])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        epoch = checkpoint.get('epoch', 0)
        logger.info(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}, Epoch: {epoch}")
        
        return epoch
    
    def cleanup(self) -> None:
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
        if self.strategy:
            self.strategy.cleanup()
        logger.info("åˆ†å¸ƒå¼è®­ç»ƒå™¨å·²æ¸…ç†")


def demo():
    """åˆ†å¸ƒå¼è®­ç»ƒæ¼”ç¤º"""
    print("ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    if not TORCH_AVAILABLE:
        print("âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œæ¼”ç¤º")
        return
    
    # åˆ›å»ºé…ç½®
    config = DistributedConfig(
        world_size=1,  # å•è¿›ç¨‹æ¼”ç¤º
        rank=0,
        batch_size=16,
        gradient_accumulation_steps=2
    )
    
    print("ğŸ“‹ åˆ†å¸ƒå¼é…ç½®:")
    print(f"  åç«¯: {config.backend}")
    print(f"  ä¸–ç•Œå¤§å°: {config.world_size}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.gradient_accumulation_steps}")
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(100, 64),
        nn.ReLU(),
        nn.Linear(64, 10)
    )
    
    print(f"\nğŸ§® æ¨¡å‹ä¿¡æ¯:")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
    
    # æ¼”ç¤ºä¸åŒçš„åˆ†å¸ƒå¼ç­–ç•¥
    strategies = ["ddp", "model_parallel", "pipeline_parallel"]
    
    for strategy_name in strategies:
        print(f"\nğŸ”§ {strategy_name.upper()} ç­–ç•¥æ¼”ç¤º:")
        print("-" * 30)
        
        try:
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = DistributedTrainer(model, config, strategy=strategy_name)
            
            # ç”±äºæ˜¯å•è¿›ç¨‹æ¼”ç¤ºï¼Œè·³è¿‡å®é™…çš„åˆ†å¸ƒå¼è®¾ç½®
            print(f"  âœ… {strategy_name} è®­ç»ƒå™¨å·²åˆ›å»º")
            print(f"  ç­–ç•¥åç§°: {trainer.strategy.name}")
            print(f"  æ˜¯å¦ä¸ºä¸»è¿›ç¨‹: {trainer.strategy.is_main_process()}")
            
            # æ¸…ç†
            trainer.cleanup()
            
        except Exception as e:
            print(f"  âš ï¸ {strategy_name} æ¼”ç¤ºå¤±è´¥: {e}")
    
    # æ¼”ç¤ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨
    print(f"\nğŸ“Š åˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨æ¼”ç¤º:")
    print("-" * 30)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
    class MockDataset:
        def __init__(self, size=1000):
            self.size = size
        
        def __len__(self):
            return self.size
        
        def __getitem__(self, idx):
            return {
                'input_ids': torch.randn(50),
                'labels': torch.randint(0, 10, (1,)).item()
            }
    
    dataset = MockDataset(1000)
    
    try:
        dist_loader = DistributedDataLoader(
            dataset=dataset,
            batch_size=config.batch_size,
            config=config,
            num_workers=0  # é¿å…Windowsä¸Šçš„å¤šè¿›ç¨‹é—®é¢˜
        )
        
        print(f"  âœ… åˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨å·²åˆ›å»º")
        print(f"  æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"  æ‰¹æ¬¡å¤§å°: {config.batch_size}")
        print(f"  ä¸–ç•Œå¤§å°: {config.world_size}")
        
        # å®é™…åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆåœ¨å•è¿›ç¨‹ç¯å¢ƒä¸‹ï¼‰
        dataloader = dist_loader.setup()
        print(f"  æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
        
    except Exception as e:
        print(f"  âš ï¸ æ•°æ®åŠ è½½å™¨æ¼”ç¤ºå¤±è´¥: {e}")
    
    print(f"\nğŸ¯ åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŠ¿:")
    print("  â€¢ æ•°æ®å¹¶è¡Œï¼šæé«˜è®­ç»ƒååé‡")
    print("  â€¢ æ¨¡å‹å¹¶è¡Œï¼šæ”¯æŒè¶…å¤§æ¨¡å‹è®­ç»ƒ")
    print("  â€¢ ç®¡é“å¹¶è¡Œï¼šå‡å°‘å†…å­˜å ç”¨")
    print("  â€¢ æ··åˆç²¾åº¦ï¼šåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹")
    print("  â€¢ æ¢¯åº¦åŒæ­¥ï¼šä¿è¯è®­ç»ƒä¸€è‡´æ€§")
    
    print(f"\nğŸ‰ åˆ†å¸ƒå¼è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    demo()