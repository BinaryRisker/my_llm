"""
æ•°æ®é›†ä¸‹è½½è„šæœ¬
================

ä¸‹è½½å„ç§ç”¨äºè¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ ‡å‡†æ•°æ®é›†ï¼š
- WMTç¿»è¯‘æ•°æ®é›†ï¼ˆè‹±æ³•ã€è‹±å¾·ç­‰ï¼‰
- OpenWebTexté¢„è®­ç»ƒæ•°æ®
- WikiTextè¯­è¨€å»ºæ¨¡æ•°æ®
- IMDBæƒ…æ„Ÿåˆ†ææ•°æ®
- å…¶ä»–å¸¸ç”¨NLPæ•°æ®é›†

ä½¿ç”¨æ–¹æ³•:
    python scripts/download_datasets.py --dataset wmt_en_fr --output_dir ./data
    python scripts/download_datasets.py --all --output_dir ./data
"""

import os
import sys
import argparse
import requests
import tarfile
import zipfile
import gzip
import json
from pathlib import Path
from typing import List, Dict, Optional
import hashlib
from tqdm import tqdm
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


class DatasetDownloader:
    """æ•°æ®é›†ä¸‹è½½å™¨"""
    
    def __init__(self, base_dir: str = "./data"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True, parents=True)
        
        # æ•°æ®é›†é…ç½®
        self.datasets = {
            'wmt_en_fr': {
                'name': 'WMT English-French Translation',
                'url': 'http://www.statmt.org/wmt14/training-parallel-europarl-v7.tgz',
                'description': 'WMT 2014 English-French parallel corpus',
                'size_mb': 500,
                'files': ['europarl-v7.fr-en.en', 'europarl-v7.fr-en.fr'],
                'md5': 'c1ba6e207f1e3b15bf9b7b70f24db672'
            },
            
            'wmt_en_de': {
                'name': 'WMT English-German Translation',
                'url': 'http://www.statmt.org/wmt14/training-parallel-europarl-v7.tgz',
                'description': 'WMT 2014 English-German parallel corpus',
                'size_mb': 500,
                'files': ['europarl-v7.de-en.en', 'europarl-v7.de-en.de'],
                'md5': 'c1ba6e207f1e3b15bf9b7b70f24db672'
            },
            
            'openwebtext': {
                'name': 'OpenWebText',
                'url': 'https://skylion007.github.io/OpenWebTextCorpus/',
                'description': 'Open source recreation of WebText corpus',
                'size_mb': 40000,  # ~40GB
                'note': 'Large dataset - requires significant disk space',
                'manual': True  # éœ€è¦æ‰‹åŠ¨ä¸‹è½½
            },
            
            'wikitext_103': {
                'name': 'WikiText-103',
                'url': 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip',
                'description': 'WikiText-103 language modeling dataset',
                'size_mb': 500,
                'files': ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens'],
                'md5': '0ca3512bd7a238be4a63ce7b434f8935'
            },
            
            'wikitext_2': {
                'name': 'WikiText-2',
                'url': 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip',
                'description': 'WikiText-2 language modeling dataset (smaller)',
                'size_mb': 10,
                'files': ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens'],
                'md5': '3c914d17d80b1459be871a5039ac23e0'
            },
            
            'imdb_reviews': {
                'name': 'IMDB Movie Reviews',
                'url': 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',
                'description': 'IMDB sentiment classification dataset',
                'size_mb': 80,
                'files': ['train/pos/', 'train/neg/', 'test/pos/', 'test/neg/'],
                'md5': '7c2ac02c03563afcf9b574c7e56c153a'
            },
            
            'penn_treebank': {
                'name': 'Penn Treebank',
                'url': 'https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/penn/',
                'description': 'Penn Treebank language modeling dataset',
                'size_mb': 5,
                'files': ['train.txt', 'valid.txt', 'test.txt'],
                'base_urls': [
                    'https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/penn/train.txt',
                    'https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/penn/valid.txt', 
                    'https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/penn/test.txt'
                ]
            },
            
            'multi30k': {
                'name': 'Multi30K Translation',
                'url': 'https://github.com/multi30k/dataset/raw/master/data/task1/tok/',
                'description': 'Multi30K multimodal translation dataset',
                'size_mb': 10,
                'files': ['train.en', 'train.de', 'val.en', 'val.de', 'test_2016_flickr.en', 'test_2016_flickr.de'],
                'base_urls': [
                    'https://github.com/multi30k/dataset/raw/master/data/task1/tok/train.en',
                    'https://github.com/multi30k/dataset/raw/master/data/task1/tok/train.de',
                    'https://github.com/multi30k/dataset/raw/master/data/task1/tok/val.en',
                    'https://github.com/multi30k/dataset/raw/master/data/task1/tok/val.de',
                    'https://github.com/multi30k/dataset/raw/master/data/task1/tok/test_2016_flickr.en',
                    'https://github.com/multi30k/dataset/raw/master/data/task1/tok/test_2016_flickr.de'
                ]
            }
        }
    
    def download_file(self, url: str, output_path: Path, expected_md5: str = None) -> bool:
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(output_path, 'wb') as f, tqdm(
                desc=f"ä¸‹è½½ {output_path.name}",
                total=total_size,
                unit='iB',
                unit_scale=True,
                unit_divisor=1024,
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    size = f.write(chunk)
                    pbar.update(size)
            
            # éªŒè¯MD5
            if expected_md5:
                actual_md5 = self.compute_md5(output_path)
                if actual_md5 != expected_md5:
                    logger.warning(f"MD5æ ¡éªŒå¤±è´¥: æœŸæœ› {expected_md5}, å®é™… {actual_md5}")
                    return False
                else:
                    logger.info(f"MD5æ ¡éªŒé€šè¿‡: {actual_md5}")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            return False
    
    def compute_md5(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶MD5"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def extract_archive(self, archive_path: Path, extract_to: Path) -> bool:
        """è§£å‹ç¼©æ–‡ä»¶"""
        try:
            if archive_path.suffix == '.zip':
                with zipfile.ZipFile(archive_path, 'r') as zip_file:
                    zip_file.extractall(extract_to)
            elif archive_path.suffix in ['.tar', '.tgz'] or '.tar.' in archive_path.name:
                with tarfile.open(archive_path, 'r:*') as tar_file:
                    tar_file.extractall(extract_to)
            elif archive_path.suffix == '.gz' and not '.tar.' in archive_path.name:
                with gzip.open(archive_path, 'rb') as gz_file:
                    with open(extract_to / archive_path.stem, 'wb') as out_file:
                        out_file.write(gz_file.read())
            else:
                logger.warning(f"ä¸æ”¯æŒçš„å‹ç¼©æ ¼å¼: {archive_path}")
                return False
            
            logger.info(f"è§£å‹å®Œæˆ: {archive_path} -> {extract_to}")
            return True
            
        except Exception as e:
            logger.error(f"è§£å‹å¤±è´¥ {archive_path}: {e}")
            return False
    
    def download_dataset(self, dataset_name: str, force: bool = False) -> bool:
        """ä¸‹è½½æŒ‡å®šæ•°æ®é›†"""
        if dataset_name not in self.datasets:
            logger.error(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
            return False
        
        config = self.datasets[dataset_name]
        dataset_dir = self.base_dir / dataset_name
        dataset_dir.mkdir(exist_ok=True, parents=True)
        
        logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®é›†: {config['name']}")
        logger.info(f"æè¿°: {config['description']}")
        logger.info(f"å¤§å°: {config['size_mb']} MB")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰‹åŠ¨ä¸‹è½½
        if config.get('manual'):
            logger.warning(f"æ•°æ®é›† {dataset_name} éœ€è¦æ‰‹åŠ¨ä¸‹è½½")
            logger.info(f"è¯·è®¿é—®: {config['url']}")
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if not force and self.is_dataset_downloaded(dataset_name):
            logger.info(f"æ•°æ®é›† {dataset_name} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            return True
        
        # ä¸‹è½½æ•°æ®é›†
        success = False
        
        if 'base_urls' in config:
            # å¤šä¸ªæ–‡ä»¶åˆ†åˆ«ä¸‹è½½
            success = True
            for i, url in enumerate(config['base_urls']):
                filename = config['files'][i]
                output_path = dataset_dir / filename
                output_path.parent.mkdir(exist_ok=True, parents=True)
                
                if not self.download_file(url, output_path):
                    success = False
                    break
        else:
            # å•ä¸ªå‹ç¼©æ–‡ä»¶ä¸‹è½½
            url = config['url']
            filename = Path(url).name
            archive_path = dataset_dir / filename
            
            # ä¸‹è½½
            expected_md5 = config.get('md5')
            if self.download_file(url, archive_path, expected_md5):
                # è§£å‹
                if self.extract_archive(archive_path, dataset_dir):
                    # åˆ é™¤å‹ç¼©æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
                    # archive_path.unlink()
                    success = True
        
        if success:
            # åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
            info_file = dataset_dir / 'dataset_info.json'
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'name': config['name'],
                    'description': config['description'],
                    'downloaded_at': str(Path(__file__).stat().st_mtime),
                    'files': config.get('files', []),
                    'size_mb': config['size_mb']
                }, f, indent=2, ensure_ascii=False)
            
            logger.info(f"æ•°æ®é›† {dataset_name} ä¸‹è½½å®Œæˆ!")
        else:
            logger.error(f"æ•°æ®é›† {dataset_name} ä¸‹è½½å¤±è´¥!")
        
        return success
    
    def is_dataset_downloaded(self, dataset_name: str) -> bool:
        """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ä¸‹è½½"""
        dataset_dir = self.base_dir / dataset_name
        if not dataset_dir.exists():
            return False
        
        info_file = dataset_dir / 'dataset_info.json'
        if not info_file.exists():
            return False
        
        # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        config = self.datasets[dataset_name]
        if 'files' in config:
            for filename in config['files']:
                file_path = dataset_dir / filename
                # æ£€æŸ¥æ–‡ä»¶æˆ–ç›®å½•
                if not (file_path.exists() or any(dataset_dir.rglob(filename))):
                    return False
        
        return True
    
    def list_available_datasets(self):
        """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"""
        print("\nğŸ“š å¯ç”¨æ•°æ®é›†:")
        print("=" * 80)
        
        for name, config in self.datasets.items():
            status = "âœ… å·²ä¸‹è½½" if self.is_dataset_downloaded(name) else "ğŸ“¥ æœªä¸‹è½½"
            manual = " ğŸ”— éœ€æ‰‹åŠ¨ä¸‹è½½" if config.get('manual') else ""
            
            print(f"\nğŸ”¹ {name}")
            print(f"   åç§°: {config['name']}")
            print(f"   æè¿°: {config['description']}")
            print(f"   å¤§å°: {config['size_mb']} MB")
            print(f"   çŠ¶æ€: {status}{manual}")
            
            if config.get('note'):
                print(f"   æ³¨æ„: {config['note']}")
    
    def download_all(self, force: bool = False, skip_manual: bool = True):
        """ä¸‹è½½æ‰€æœ‰æ•°æ®é›†"""
        logger.info("å¼€å§‹ä¸‹è½½æ‰€æœ‰æ•°æ®é›†...")
        
        success_count = 0
        total_count = 0
        
        for dataset_name, config in self.datasets.items():
            if skip_manual and config.get('manual'):
                logger.info(f"è·³è¿‡éœ€è¦æ‰‹åŠ¨ä¸‹è½½çš„æ•°æ®é›†: {dataset_name}")
                continue
            
            total_count += 1
            if self.download_dataset(dataset_name, force):
                success_count += 1
        
        logger.info(f"ä¸‹è½½å®Œæˆ: {success_count}/{total_count} ä¸ªæ•°æ®é›†ä¸‹è½½æˆåŠŸ")
    
    def create_sample_configs(self):
        """ä¸ºå„é˜¶æ®µåˆ›å»ºç¤ºä¾‹æ•°æ®é›†é…ç½®"""
        configs = {
            'stage2_rnn_lstm': {
                'datasets': ['penn_treebank', 'wikitext_2'],
                'task': 'language_modeling',
                'description': 'RNN/LSTMè¯­è¨€å»ºæ¨¡æ•°æ®'
            },
            'stage3_attention': {
                'datasets': ['multi30k', 'wmt_en_fr'],
                'task': 'translation', 
                'description': 'æ³¨æ„åŠ›æœºåˆ¶ç¿»è¯‘æ•°æ®'
            },
            'stage4_transformer': {
                'datasets': ['wmt_en_fr', 'wmt_en_de'],
                'task': 'translation',
                'description': 'Transformerç¿»è¯‘æ•°æ®'
            },
            'stage5_gpt': {
                'datasets': ['wikitext_103', 'openwebtext'],
                'task': 'language_modeling',
                'description': 'GPTé¢„è®­ç»ƒæ•°æ®'
            }
        }
        
        configs_dir = self.base_dir / 'configs'
        configs_dir.mkdir(exist_ok=True)
        
        for stage, config in configs.items():
            config_file = configs_dir / f'{stage}_datasets.json'
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ•°æ®é›†é…ç½®æ–‡ä»¶å·²åˆ›å»ºåœ¨: {configs_dir}")


def main():
    parser = argparse.ArgumentParser(description='ä¸‹è½½è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®é›†')
    
    parser.add_argument('--dataset', type=str, 
                       help='è¦ä¸‹è½½çš„æ•°æ®é›†åç§°')
    parser.add_argument('--all', action='store_true',
                       help='ä¸‹è½½æ‰€æœ‰æ•°æ®é›†')
    parser.add_argument('--list', action='store_true', 
                       help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†')
    parser.add_argument('--output_dir', type=str, default='./data',
                       help='æ•°æ®ä¸‹è½½ç›®å½• (é»˜è®¤: ./data)')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²å­˜åœ¨çš„æ•°æ®é›†')
    parser.add_argument('--skip_manual', action='store_true', default=True,
                       help='è·³è¿‡éœ€è¦æ‰‹åŠ¨ä¸‹è½½çš„æ•°æ®é›†')
    
    args = parser.parse_args()
    
    downloader = DatasetDownloader(args.output_dir)
    
    if args.list:
        downloader.list_available_datasets()
        return
    
    if args.all:
        downloader.download_all(args.force, args.skip_manual)
        downloader.create_sample_configs()
        return
    
    if args.dataset:
        success = downloader.download_dataset(args.dataset, args.force)
        if success:
            print(f"\nâœ… æ•°æ®é›† {args.dataset} ä¸‹è½½æˆåŠŸ!")
        else:
            print(f"\nâŒ æ•°æ®é›† {args.dataset} ä¸‹è½½å¤±è´¥!")
        return
    
    # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©
    parser.print_help()


if __name__ == "__main__":
    main()