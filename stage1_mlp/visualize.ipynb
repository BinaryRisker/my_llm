{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# é˜¶æ®µ1ï¼šMLPæ–‡æœ¬åˆ†ç±»å¯è§†åŒ–åˆ†æ\\n\",\n    \"\\n\",\n    \"è¿™ä¸ªnotebookæä¾›äº†å¯¹MLPæ–‡æœ¬åˆ†ç±»æ¨¡å‹çš„æ·±å…¥å¯è§†åŒ–åˆ†æï¼ŒåŒ…æ‹¬ï¼š\\n\",\n    \"- è®­ç»ƒè¿‡ç¨‹åˆ†æ\\n\",\n    \"- æ¨¡å‹æ€§èƒ½è¯„ä¼°\\n\",\n    \"- é”™è¯¯åˆ†æ\\n\",\n    \"- è¯åµŒå…¥å¯è§†åŒ–\\n\",\n    \"- æ³¨æ„åŠ›æƒé‡åˆ†æ\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"import os\\n\",\n    \"import sys\\n\",\n    \"import json\\n\",\n    \"import numpy as np\\n\",\n    \"import pandas as pd\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import seaborn as sns\\n\",\n    \"from sklearn.metrics import classification_report, confusion_matrix\\n\",\n    \"from sklearn.decomposition import PCA\\n\",\n    \"from sklearn.manifold import TSNE\\n\",\n    \"import torch\\n\",\n    \"import torch.nn.functional as F\\n\",\n    \"\\n\",\n    \"# è®¾ç½®é¡¹ç›®è·¯å¾„\\n\",\n    \"sys.path.append('..')\\n\",\n    \"\\n\",\n    \"from models.mlp import SimpleMLP, MLPWithBagOfWords\\n\",\n    \"from utils.data_utils import TextVocabulary, load_ag_news_sample\\n\",\n    \"\\n\",\n    \"# è®¾ç½®å¯è§†åŒ–æ ·å¼\\n\",\n    \"plt.style.use('default')\\n\",\n    \"sns.set_palette(\\\"husl\\\")\\n\",\n    \"\\n\",\n    \"%matplotlib inline\\n\",\n    \"\\n\",\n    \"print(\\\"ğŸ“Š å¯¼å…¥å®Œæˆï¼\\\")\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 1. åŠ è½½è®­ç»ƒæ•°æ®å’Œæ¨¡å‹\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# æ¨¡å‹ä¿å­˜ç›®å½•\\n\",\n    \"MODEL_DIR = './checkpoints'\\n\",\n    \"\\n\",\n    \"# æ£€æŸ¥æ˜¯å¦å­˜åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹\\n\",\n    \"if not os.path.exists(MODEL_DIR):\\n\",\n    \"    print(\\\"âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œ train.py\\\")\\n\",\n    \"    print(\\\"ğŸ’¡ è¿è¡Œ: python train.py --epochs 10\\\")\\n\",\n    \"else:\\n\",\n    \"    print(f\\\"âœ… æ‰¾åˆ°æ¨¡å‹ç›®å½•: {MODEL_DIR}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# åŠ è½½é…ç½®æ–‡ä»¶\\n\",\n    \"try:\\n\",\n    \"    with open(os.path.join(MODEL_DIR, 'config.json'), 'r') as f:\\n\",\n    \"        config = json.load(f)\\n\",\n    \"    \\n\",\n    \"    with open(os.path.join(MODEL_DIR, 'training_summary.json'), 'r') as f:\\n\",\n    \"        training_summary = json.load(f)\\n\",\n    \"    \\n\",\n    \"    print(\\\"æ¨¡å‹é…ç½®:\\\")\\n\",\n    \"    for key, value in config.items():\\n\",\n    \"        print(f\\\"  {key}: {value}\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\næœ€ç»ˆè®­ç»ƒæ€§èƒ½:\\\")\\n\",\n    \"    print(f\\\"  è®­ç»ƒå‡†ç¡®ç‡: {training_summary['final_train_acc']:.4f}\\\")\\n\",\n    \"    print(f\\\"  éªŒè¯å‡†ç¡®ç‡: {training_summary['final_val_acc']:.4f}\\\")\\n\",\n    \"    print(f\\\"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {training_summary['best_val_acc']:.4f}\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}\\\")\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 2. è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# ç»˜åˆ¶è®­ç»ƒæ›²çº¿\\n\",\n    \"def plot_training_curves(training_summary):\\n\",\n    \"    history = training_summary['train_history']\\n\",\n    \"    \\n\",\n    \"    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n\",\n    \"    \\n\",\n    \"    # æŸå¤±æ›²çº¿\\n\",\n    \"    axes[0, 0].plot(history['train_losses'], label='è®­ç»ƒæŸå¤±', marker='o', markersize=4)\\n\",\n    \"    axes[0, 0].plot(history['val_losses'], label='éªŒè¯æŸå¤±', marker='s', markersize=4)\\n\",\n    \"    axes[0, 0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')\\n\",\n    \"    axes[0, 0].set_xlabel('Epoch')\\n\",\n    \"    axes[0, 0].set_ylabel('Loss')\\n\",\n    \"    axes[0, 0].legend()\\n\",\n    \"    axes[0, 0].grid(True, alpha=0.3)\\n\",\n    \"    \\n\",\n    \"    # å‡†ç¡®ç‡æ›²çº¿\\n\",\n    \"    axes[0, 1].plot(history['train_accuracies'], label='è®­ç»ƒå‡†ç¡®ç‡', marker='o', markersize=4)\\n\",\n    \"    axes[0, 1].plot(history['val_accuracies'], label='éªŒè¯å‡†ç¡®ç‡', marker='s', markersize=4)\\n\",\n    \"    axes[0, 1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')\\n\",\n    \"    axes[0, 1].set_xlabel('Epoch')\\n\",\n    \"    axes[0, 1].set_ylabel('Accuracy')\\n\",\n    \"    axes[0, 1].legend()\\n\",\n    \"    axes[0, 1].grid(True, alpha=0.3)\\n\",\n    \"    \\n\",\n    \"    # è¿‡æ‹Ÿåˆåˆ†æ\\n\",\n    \"    train_val_diff = np.array(history['train_accuracies']) - np.array(history['val_accuracies'])\\n\",\n    \"    axes[1, 0].plot(train_val_diff, marker='d', color='red', markersize=4)\\n\",\n    \"    axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\\n\",\n    \"    axes[1, 0].set_title('è®­ç»ƒ-éªŒè¯å‡†ç¡®ç‡å·®å€¼ï¼ˆè¿‡æ‹ŸåˆæŒ‡æ ‡ï¼‰')\\n\",\n    \"    axes[1, 0].set_xlabel('Epoch')\\n\",\n    \"    axes[1, 0].set_ylabel('Train Acc - Val Acc')\\n\",\n    \"    axes[1, 0].grid(True, alpha=0.3)\\n\",\n    \"    \\n\",\n    \"    # å­¦ä¹ é€Ÿåº¦åˆ†æ\\n\",\n    \"    train_loss_diff = np.diff(history['train_losses'])\\n\",\n    \"    axes[1, 1].plot(train_loss_diff, marker='v', color='green', markersize=4)\\n\",\n    \"    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\\n\",\n    \"    axes[1, 1].set_title('è®­ç»ƒæŸå¤±å˜åŒ–ç‡ï¼ˆå­¦ä¹ é€Ÿåº¦ï¼‰')\\n\",\n    \"    axes[1, 1].set_xlabel('Epoch')\\n\",\n    \"    axes[1, 1].set_ylabel('Loss Change')\\n\",\n    \"    axes[1, 1].grid(True, alpha=0.3)\\n\",\n    \"    \\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"\\n\",\n    \"plot_training_curves(training_summary)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 3. æ¨¡å‹æ€§èƒ½åˆ†æ\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# åŠ è½½æ¨¡å‹å’Œæ•°æ®è¿›è¡Œè¯¦ç»†è¯„ä¼°\\n\",\n    \"def load_model_and_data():\\n\",\n    \"    # åŠ è½½è¯æ±‡è¡¨\\n\",\n    \"    vocab = TextVocabulary()\\n\",\n    \"    vocab.load(os.path.join(MODEL_DIR, 'vocabulary.pkl'))\\n\",\n    \"    \\n\",\n    \"    # åŠ è½½æ•°æ®\\n\",\n    \"    texts, labels, class_names = load_ag_news_sample()\\n\",\n    \"    \\n\",\n    \"    # åˆ†å‰²æ•°æ®ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰\\n\",\n    \"    split_idx = int(0.8 * len(texts))\\n\",\n    \"    val_texts = texts[split_idx:]\\n\",\n    \"    val_labels = labels[split_idx:]\\n\",\n    \"    \\n\",\n    \"    # åŠ è½½æ¨¡å‹\\n\",\n    \"    if config['model_type'] == 'embedding':\\n\",\n    \"        model = SimpleMLP(\\n\",\n    \"            vocab_size=config['vocab_size'],\\n\",\n    \"            embedding_dim=config['embedding_dim'],\\n\",\n    \"            hidden_dims=config['hidden_dims'],\\n\",\n    \"            num_classes=config['num_classes'],\\n\",\n    \"            dropout_rate=config['dropout']\\n\",\n    \"        )\\n\",\n    \"        checkpoint_path = os.path.join(MODEL_DIR, 'best_embedding_mlp.pt')\\n\",\n    \"    else:\\n\",\n    \"        model = MLPWithBagOfWords(\\n\",\n    \"            vocab_size=config['vocab_size'],\\n\",\n    \"            hidden_dims=config['hidden_dims'],\\n\",\n    \"            num_classes=config['num_classes'],\\n\",\n    \"            dropout_rate=config['dropout']\\n\",\n    \"        )\\n\",\n    \"        checkpoint_path = os.path.join(MODEL_DIR, 'best_bow_mlp.pt')\\n\",\n    \"    \\n\",\n    \"    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡\\n\",\n    \"    checkpoint = torch.load(checkpoint_path, map_location='cpu')\\n\",\n    \"    model.load_state_dict(checkpoint['model_state_dict'])\\n\",\n    \"    model.eval()\\n\",\n    \"    \\n\",\n    \"    return model, vocab, val_texts, val_labels, class_names\\n\",\n    \"\\n\",\n    \"model, vocab, val_texts, val_labels, class_names = load_model_and_data()\\n\",\n    \"print(f\\\"âœ… åŠ è½½å®Œæˆ: {len(val_texts)} ä¸ªéªŒè¯æ ·æœ¬\\\")\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# ç”Ÿæˆé¢„æµ‹ç»“æœ\\n\",\n    \"def generate_predictions(model, vocab, texts, config):\\n\",\n    \"    predictions = []\\n\",\n    \"    probabilities = []\\n\",\n    \"    \\n\",\n    \"    model.eval()\\n\",\n    \"    with torch.no_grad():\\n\",\n    \"        for text in texts:\\n\",\n    \"            if config['model_type'] == 'embedding':\\n\",\n    \"                # é¢„å¤„ç†\\n\",\n    \"                indices = vocab.text_to_indices(text, config['max_length'])\\n\",\n    \"                mask = [1 if i != vocab.token2idx[vocab.PAD_TOKEN] else 0 for i in indices]\\n\",\n    \"                \\n\",\n    \"                # è½¬æ¢ä¸ºå¼ é‡\\n\",\n    \"                input_tensor = torch.tensor([indices], dtype=torch.long)\\n\",\n    \"                mask_tensor = torch.tensor([mask], dtype=torch.long)\\n\",\n    \"                \\n\",\n    \"                # é¢„æµ‹\\n\",\n    \"                logits = model(input_tensor, mask_tensor)\\n\",\n    \"            else:\\n\",\n    \"                # Bag-of-wordså¤„ç†\\n\",\n    \"                tokens = vocab.tokenize(text)\\n\",\n    \"                bow_vector = torch.zeros(len(vocab), dtype=torch.float)\\n\",\n    \"                for token in tokens:\\n\",\n    \"                    idx = vocab.token2idx.get(token, vocab.token2idx[vocab.UNK_TOKEN])\\n\",\n    \"                    bow_vector[idx] += 1.0\\n\",\n    \"                \\n\",\n    \"                input_tensor = bow_vector.unsqueeze(0)\\n\",\n    \"                logits = model(input_tensor)\\n\",\n    \"            \\n\",\n    \"            probs = F.softmax(logits, dim=-1)\\n\",\n    \"            pred = torch.argmax(probs, dim=-1).item()\\n\",\n    \"            \\n\",\n    \"            predictions.append(pred)\\n\",\n    \"            probabilities.append(probs.numpy()[0])\\n\",\n    \"    \\n\",\n    \"    return predictions, probabilities\\n\",\n    \"\\n\",\n    \"val_predictions, val_probabilities = generate_predictions(model, vocab, val_texts, config)\\n\",\n    \"print(f\\\"âœ… ç”Ÿæˆäº† {len(val_predictions)} ä¸ªé¢„æµ‹ç»“æœ\\\")\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\\n\",\n    \"def plot_confusion_matrix(y_true, y_pred, class_names):\\n\",\n    \"    cm = confusion_matrix(y_true, y_pred)\\n\",\n    \"    \\n\",\n    \"    plt.figure(figsize=(10, 8))\\n\",\n    \"    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\\n\",\n    \"                xticklabels=class_names, yticklabels=class_names)\\n\",\n    \"    plt.title('æ··æ·†çŸ©é˜µ', fontsize=16)\\n\",\n    \"    plt.xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)\\n\",\n    \"    plt.ylabel('çœŸå®ç±»åˆ«', fontsize=12)\\n\",\n    \"    \\n\",\n    \"    # æ·»åŠ å‡†ç¡®ç‡æ ‡æ³¨\\n\",\n    \"    accuracy = np.diag(cm) / np.sum(cm, axis=1)\\n\",\n    \"    for i, acc in enumerate(accuracy):\\n\",\n    \"        plt.text(len(class_names) + 0.1, i + 0.5, f'å‡†ç¡®ç‡: {acc:.3f}', \\n\",\n    \"                verticalalignment='center')\\n\",\n    \"    \\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"    \\n\",\n    \"    return cm\\n\",\n    \"\\n\",\n    \"cm = plot_confusion_matrix(val_labels, val_predictions, class_names)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\\n\",\n    \"print(\\\"ğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\\\")\\n\",\n    \"print(classification_report(val_labels, val_predictions, target_names=class_names))\\n\",\n    \"\\n\",\n    \"# è®¡ç®—å„ç±»åˆ«çš„å‡†ç¡®ç‡\\n\",\n    \"class_accuracy = np.diag(cm) / np.sum(cm, axis=1)\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nğŸ“ˆ å„ç±»åˆ«å‡†ç¡®ç‡:\\\")\\n\",\n    \"for i, (name, acc) in enumerate(zip(class_names, class_accuracy)):\\n\",\n    \"    print(f\\\"  {name}: {acc:.4f}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 4. é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# åˆ†æé¢„æµ‹ç½®ä¿¡åº¦\\n\",\n    \"def analyze_prediction_confidence(probabilities, predictions, true_labels, class_names):\\n\",\n    \"    # è®¡ç®—æœ€å¤§æ¦‚ç‡ï¼ˆç½®ä¿¡åº¦ï¼‰\\n\",\n    \"    confidences = [prob[pred] for prob, pred in zip(probabilities, predictions)]\\n\",\n    \"    \\n\",\n    \"    # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®\\n\",\n    \"    correct = [pred == true for pred, true in zip(predictions, true_labels)]\\n\",\n    \"    \\n\",\n    \"    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\\n\",\n    \"    \\n\",\n    \"    # 1. ç½®ä¿¡åº¦åˆ†å¸ƒ\\n\",\n    \"    axes[0].hist([conf for conf, corr in zip(confidences, correct) if corr], \\n\",\n    \"                alpha=0.7, bins=20, label='æ­£ç¡®é¢„æµ‹', color='green')\\n\",\n    \"    axes[0].hist([conf for conf, corr in zip(confidences, correct) if not corr], \\n\",\n    \"                alpha=0.7, bins=20, label='é”™è¯¯é¢„æµ‹', color='red')\\n\",\n    \"    axes[0].set_title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')\\n\",\n    \"    axes[0].set_xlabel('ç½®ä¿¡åº¦')\\n\",\n    \"    axes[0].set_ylabel('é¢‘æ¬¡')\\n\",\n    \"    axes[0].legend()\\n\",\n    \"    \\n\",\n    \"    # 2. ç½®ä¿¡åº¦vså‡†ç¡®ç‡\\n\",\n    \"    confidence_bins = np.linspace(0.25, 1.0, 10)\\n\",\n    \"    bin_accuracy = []\\n\",\n    \"    bin_counts = []\\n\",\n    \"    \\n\",\n    \"    for i in range(len(confidence_bins)-1):\\n\",\n    \"        mask = (np.array(confidences) >= confidence_bins[i]) & (np.array(confidences) < confidence_bins[i+1])\\n\",\n    \"        if mask.sum() > 0:\\n\",\n    \"            acc = np.array(correct)[mask].mean()\\n\",\n    \"            count = mask.sum()\\n\",\n    \"        else:\\n\",\n    \"            acc = 0\\n\",\n    \"            count = 0\\n\",\n    \"        bin_accuracy.append(acc)\\n\",\n    \"        bin_counts.append(count)\\n\",\n    \"    \\n\",\n    \"    bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\\n\",\n    \"    axes[1].bar(bin_centers, bin_accuracy, width=0.08, alpha=0.7)\\n\",\n    \"    axes[1].plot([0.25, 1.0], [0.25, 1.0], 'r--', alpha=0.5, label='ç†æƒ³æ ¡å‡†')\\n\",\n    \"    axes[1].set_title('ç½®ä¿¡åº¦æ ¡å‡†æ›²çº¿')\\n\",\n    \"    axes[1].set_xlabel('é¢„æµ‹ç½®ä¿¡åº¦')\\n\",\n    \"    axes[1].set_ylabel('å®é™…å‡†ç¡®ç‡')\\n\",\n    \"    axes[1].legend()\\n\",\n    \"    \\n\",\n    \"    # 3. å„ç±»åˆ«å¹³å‡ç½®ä¿¡åº¦\\n\",\n    \"    class_confidences = []\\n\",\n    \"    for class_idx in range(len(class_names)):\\n\",\n    \"        class_mask = np.array(predictions) == class_idx\\n\",\n    \"        if class_mask.sum() > 0:\\n\",\n    \"            avg_conf = np.array(confidences)[class_mask].mean()\\n\",\n    \"        else:\\n\",\n    \"            avg_conf = 0\\n\",\n    \"        class_confidences.append(avg_conf)\\n\",\n    \"    \\n\",\n    \"    axes[2].bar(range(len(class_names)), class_confidences, alpha=0.7)\\n\",\n    \"    axes[2].set_title('å„ç±»åˆ«å¹³å‡ç½®ä¿¡åº¦')\\n\",\n    \"    axes[2].set_xlabel('ç±»åˆ«')\\n\",\n    \"    axes[2].set_ylabel('å¹³å‡ç½®ä¿¡åº¦')\\n\",\n    \"    axes[2].set_xticks(range(len(class_names)))\\n\",\n    \"    axes[2].set_xticklabels(class_names, rotation=45)\\n\",\n    \"    \\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"    \\n\",\n    \"    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯\\n\",\n    \"    print(f\\\"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences):.4f}\\\")\\n\",\n    \"    print(f\\\"æ­£ç¡®é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {np.mean([c for c, corr in zip(confidences, correct) if corr]):.4f}\\\")\\n\",\n    \"    print(f\\\"é”™è¯¯é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {np.mean([c for c, corr in zip(confidences, correct) if not corr]):.4f}\\\")\\n\",\n    \"\\n\",\n    \"analyze_prediction_confidence(val_probabilities, val_predictions, val_labels, class_names)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 5. é”™è¯¯æ¡ˆä¾‹åˆ†æ\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# åˆ†æé”™è¯¯é¢„æµ‹æ¡ˆä¾‹\\n\",\n    \"def analyze_errors(texts, true_labels, predictions, probabilities, class_names):\\n\",\n    \"    errors = []\\n\",\n    \"    \\n\",\n    \"    for i, (text, true_label, pred_label, prob) in enumerate(zip(texts, true_labels, predictions, probabilities)):\\n\",\n    \"        if true_label != pred_label:\\n\",\n    \"            confidence = prob[pred_label]\\n\",\n    \"            true_prob = prob[true_label]\\n\",\n    \"            errors.append({\\n\",\n    \"                'index': i,\\n\",\n    \"                'text': text,\\n\",\n    \"                'true_label': true_label,\\n\",\n    \"                'true_class': class_names[true_label],\\n\",\n    \"                'pred_label': pred_label,\\n\",\n    \"                'pred_class': class_names[pred_label],\\n\",\n    \"                'confidence': confidence,\\n\",\n    \"                'true_prob': true_prob,\\n\",\n    \"                'prob_diff': confidence - true_prob\\n\",\n    \"            })\\n\",\n    \"    \\n\",\n    \"    # æŒ‰ç½®ä¿¡åº¦æ’åº\\n\",\n    \"    errors.sort(key=lambda x: x['confidence'], reverse=True)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"ğŸ” å‘ç° {len(errors)} ä¸ªé”™è¯¯é¢„æµ‹æ¡ˆä¾‹\\\\n\\\")\\n\",\n    \"    \\n\",\n    \"    # æ˜¾ç¤ºé«˜ç½®ä¿¡åº¦é”™è¯¯æ¡ˆä¾‹\\n\",\n    \"    print(\\\"âŒ é«˜ç½®ä¿¡åº¦é”™è¯¯é¢„æµ‹ï¼ˆæ¨¡å‹å¾ˆç¡®ä¿¡ä½†æ˜¯é”™è¯¯çš„ï¼‰:\\\")\\n\",\n    \"    for i, error in enumerate(errors[:3]):\\n\",\n    \"        print(f\\\"\\\\n{i+1}. æ–‡æœ¬: {error['text']}\\\")\\n\",\n    \"        print(f\\\"   çœŸå®ç±»åˆ«: {error['true_class']} (æ¦‚ç‡: {error['true_prob']:.4f})\\\")\\n\",\n    \"        print(f\\\"   é¢„æµ‹ç±»åˆ«: {error['pred_class']} (ç½®ä¿¡åº¦: {error['confidence']:.4f})\\\")\\n\",\n    \"    \\n\",\n    \"    # æ˜¾ç¤ºä½ç½®ä¿¡åº¦é”™è¯¯æ¡ˆä¾‹\\n\",\n    \"    print(\\\"\\\\nğŸ¤” ä½ç½®ä¿¡åº¦é”™è¯¯é¢„æµ‹ï¼ˆæ¨¡å‹ä¸ç¡®å®šä¸”é”™è¯¯ï¼‰:\\\")\\n\",\n    \"    for i, error in enumerate(errors[-3:]):\\n\",\n    \"        print(f\\\"\\\\n{i+1}. æ–‡æœ¬: {error['text']}\\\")\\n\",\n    \"        print(f\\\"   çœŸå®ç±»åˆ«: {error['true_class']} (æ¦‚ç‡: {error['true_prob']:.4f})\\\")\\n\",\n    \"        print(f\\\"   é¢„æµ‹ç±»åˆ«: {error['pred_class']} (ç½®ä¿¡åº¦: {error['confidence']:.4f})\\\")\\n\",\n    \"    \\n\",\n    \"    return errors\\n\",\n    \"\\n\",\n    \"errors = analyze_errors(val_texts, val_labels, val_predictions, val_probabilities, class_names)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 6. è¯åµŒå…¥å¯è§†åŒ–ï¼ˆä»…é€‚ç”¨äºEmbeddingæ¨¡å‹ï¼‰\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# è¯åµŒå…¥å¯è§†åŒ–\\n\",\n    \"def visualize_embeddings(model, vocab, config, num_words=50):\\n\",\n    \"    if config['model_type'] != 'embedding':\\n\",\n    \"        print(\\\"âš ï¸ å½“å‰æ¨¡å‹ç±»å‹ä¸æ”¯æŒè¯åµŒå…¥å¯è§†åŒ–\\\")\\n\",\n    \"        return\\n\",\n    \"    \\n\",\n    \"    # æå–åµŒå…¥å±‚æƒé‡\\n\",\n    \"    embeddings = model.embedding.weight.data.numpy()\\n\",\n    \"    \\n\",\n    \"    # é€‰æ‹©æœ€é¢‘ç¹çš„è¯æ±‡è¿›è¡Œå¯è§†åŒ–\\n\",\n    \"    word_indices = list(range(4, min(len(vocab), num_words + 4)))  # è·³è¿‡ç‰¹æ®Štoken\\n\",\n    \"    word_embeddings = embeddings[word_indices]\\n\",\n    \"    words = [vocab.idx2token[idx] for idx in word_indices]\\n\",\n    \"    \\n\",\n    \"    # ä½¿ç”¨t-SNEè¿›è¡Œé™ç»´\\n\",\n    \"    print(\\\"ğŸ”„ æ­£åœ¨è¿›è¡Œt-SNEé™ç»´...\\\")\\n\",\n    \"    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(words)-1))\\n\",\n    \"    embeddings_2d = tsne.fit_transform(word_embeddings)\\n\",\n    \"    \\n\",\n    \"    # ç»˜åˆ¶æ•£ç‚¹å›¾\\n\",\n    \"    plt.figure(figsize=(12, 8))\\n\",\n    \"    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, s=60)\\n\",\n    \"    \\n\",\n    \"    # æ·»åŠ è¯æ±‡æ ‡ç­¾\\n\",\n    \"    for i, word in enumerate(words):\\n\",\n    \"        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \\n\",\n    \"                    xytext=(5, 5), textcoords='offset points', \\n\",\n    \"                    fontsize=8, alpha=0.8)\\n\",\n    \"    \\n\",\n    \"    plt.title('è¯åµŒå…¥t-SNEå¯è§†åŒ–', fontsize=16)\\n\",\n    \"    plt.xlabel('t-SNE ç»´åº¦ 1')\\n\",\n    \"    plt.ylabel('t-SNE ç»´åº¦ 2')\\n\",\n    \"    plt.grid(True, alpha=0.3)\\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"    \\n\",\n    \"    # PCAé™ç»´å¯¹æ¯”\\n\",\n    \"    pca = PCA(n_components=2)\\n\",\n    \"    embeddings_pca = pca.fit_transform(word_embeddings)\\n\",\n    \"    \\n\",\n    \"    plt.figure(figsize=(12, 8))\\n\",\n    \"    plt.scatter(embeddings_pca[:, 0], embeddings_pca[:, 1], alpha=0.7, s=60)\\n\",\n    \"    \\n\",\n    \"    for i, word in enumerate(words):\\n\",\n    \"        plt.annotate(word, (embeddings_pca[i, 0], embeddings_pca[i, 1]), \\n\",\n    \"                    xytext=(5, 5), textcoords='offset points', \\n\",\n    \"                    fontsize=8, alpha=0.8)\\n\",\n    \"    \\n\",\n    \"    plt.title('è¯åµŒå…¥PCAå¯è§†åŒ–', fontsize=16)\\n\",\n    \"    plt.xlabel(f'PC1 (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[0]:.3f})')\\n\",\n    \"    plt.ylabel(f'PC2 (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[1]:.3f})')\\n\",\n    \"    plt.grid(True, alpha=0.3)\\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"\\n\",\n    \"visualize_embeddings(model, vocab, config)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 7. æ¨¡å‹æ€§èƒ½æ€»ç»“\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# ç”Ÿæˆå®Œæ•´çš„æ€§èƒ½æŠ¥å‘Š\\n\",\n    \"def generate_performance_report(config, training_summary, val_labels, val_predictions, errors):\\n\",\n    \"    print(\\\"ğŸ“‹ æ¨¡å‹æ€§èƒ½æ€»ç»“æŠ¥å‘Š\\\")\\n\",\n    \"    print(\\\"=\\\" * 50)\\n\",\n    \"    \\n\",\n    \"    # æ¨¡å‹åŸºæœ¬ä¿¡æ¯\\n\",\n    \"    print(f\\\"ğŸ—ï¸ æ¨¡å‹ç±»å‹: {config['model_type']}\\\")\\n\",\n    \"    if config['model_type'] == 'embedding':\\n\",\n    \"        print(f\\\"ğŸ“Š åµŒå…¥ç»´åº¦: {config['embedding_dim']}\\\")\\n\",\n    \"    print(f\\\"ğŸ§  éšè—å±‚: {config['hidden_dims']}\\\")\\n\",\n    \"    print(f\\\"ğŸ“š è¯æ±‡è¡¨å¤§å°: {config['vocab_size']}\\\")\\n\",\n    \"    print(f\\\"ğŸ¯ ç±»åˆ«æ•°: {config['num_classes']}\\\")\\n\",\n    \"    \\n\",\n    \"    # è®­ç»ƒæ€§èƒ½\\n\",\n    \"    print(f\\\"\\\\nğŸƒ è®­ç»ƒæ€§èƒ½:\\\")\\n\",\n    \"    print(f\\\"   æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {training_summary['final_train_acc']:.4f}\\\")\\n\",\n    \"    print(f\\\"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {training_summary['final_val_acc']:.4f}\\\")\\n\",\n    \"    print(f\\\"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {training_summary['best_val_acc']:.4f}\\\")\\n\",\n    \"    \\n\",\n    \"    # éªŒè¯é›†è¯¦ç»†æ€§èƒ½\\n\",\n    \"    val_accuracy = sum(pred == true for pred, true in zip(val_predictions, val_labels)) / len(val_labels)\\n\",\n    \"    print(f\\\"\\\\nâœ… éªŒè¯é›†æ€§èƒ½:\\\")\\n\",\n    \"    print(f\\\"   å‡†ç¡®ç‡: {val_accuracy:.4f}\\\")\\n\",\n    \"    print(f\\\"   é”™è¯¯é¢„æµ‹æ•°: {len(errors)} / {len(val_labels)}\\\")\\n\",\n    \"    \\n\",\n    \"    # è¿‡æ‹Ÿåˆåˆ†æ\\n\",\n    \"    overfitting = training_summary['final_train_acc'] - training_summary['final_val_acc']\\n\",\n    \"    print(f\\\"\\\\nğŸ” è¿‡æ‹Ÿåˆåˆ†æ:\\\")\\n\",\n    \"    print(f\\\"   è®­ç»ƒ-éªŒè¯å‡†ç¡®ç‡å·®å€¼: {overfitting:.4f}\\\")\\n\",\n    \"    if overfitting > 0.1:\\n\",\n    \"        print(f\\\"   âš ï¸ å­˜åœ¨è¿‡æ‹Ÿåˆç°è±¡ï¼Œå»ºè®®å¢åŠ æ­£åˆ™åŒ–æˆ–å‡å°‘æ¨¡å‹å¤æ‚åº¦\\\")\\n\",\n    \"    elif overfitting < 0:\\n\",\n    \"        print(f\\\"   âš ï¸ éªŒè¯å‡†ç¡®ç‡é«˜äºè®­ç»ƒå‡†ç¡®ç‡ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²æˆ–æ ·æœ¬ä¸è¶³\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(f\\\"   âœ… æ¨¡å‹æ³›åŒ–è‰¯å¥½\\\")\\n\",\n    \"    \\n\",\n    \"    # æ”¹è¿›å»ºè®®\\n\",\n    \"    print(f\\\"\\\\nğŸ’¡ æ”¹è¿›å»ºè®®:\\\")\\n\",\n    \"    if val_accuracy < 0.7:\\n\",\n    \"        print(f\\\"   - å‡†ç¡®ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æ¨¡å‹å¤æ‚åº¦æˆ–æ›´å¤šè®­ç»ƒæ•°æ®\\\")\\n\",\n    \"    if len(errors) > len(val_labels) * 0.3:\\n\",\n    \"        print(f\\\"   - é”™è¯¯ç‡è¾ƒé«˜ï¼Œæ£€æŸ¥æ•°æ®è´¨é‡å’Œé¢„å¤„ç†æµç¨‹\\\")\\n\",\n    \"    if overfitting > 0.1:\\n\",\n    \"        print(f\\\"   - å¢åŠ Dropoutç‡æˆ–ä½¿ç”¨å…¶ä»–æ­£åˆ™åŒ–æŠ€æœ¯\\\")\\n\",\n    \"        print(f\\\"   - æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®\\\")\\n\",\n    \"    if config['model_type'] == 'bow' and val_accuracy < 0.8:\\n\",\n    \"        print(f\\\"   - è€ƒè™‘ä½¿ç”¨embeddingæ¨¡å‹è·å¾—æ›´å¥½çš„æ€§èƒ½\\\")\\n\",\n    \"    \\n\",\n    \"generate_performance_report(config, training_summary, val_labels, val_predictions, errors)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 8. äº¤äº’å¼é¢„æµ‹æµ‹è¯•\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# äº¤äº’å¼æµ‹è¯•æ–°æ–‡æœ¬\\n\",\n    \"def predict_new_text(text, model, vocab, config, class_names):\\n\",\n    \"    model.eval()\\n\",\n    \"    with torch.no_grad():\\n\",\n    \"        if config['model_type'] == 'embedding':\\n\",\n    \"            indices = vocab.text_to_indices(text, config['max_length'])\\n\",\n    \"            mask = [1 if i != vocab.token2idx[vocab.PAD_TOKEN] else 0 for i in indices]\\n\",\n    \"            \\n\",\n    \"            input_tensor = torch.tensor([indices], dtype=torch.long)\\n\",\n    \"            mask_tensor = torch.tensor([mask], dtype=torch.long)\\n\",\n    \"            \\n\",\n    \"            logits = model(input_tensor, mask_tensor)\\n\",\n    \"        else:\\n\",\n    \"            tokens = vocab.tokenize(text)\\n\",\n    \"            bow_vector = torch.zeros(len(vocab), dtype=torch.float)\\n\",\n    \"            for token in tokens:\\n\",\n    \"                idx = vocab.token2idx.get(token, vocab.token2idx[vocab.UNK_TOKEN])\\n\",\n    \"                bow_vector[idx] += 1.0\\n\",\n    \"            \\n\",\n    \"            input_tensor = bow_vector.unsqueeze(0)\\n\",\n    \"            logits = model(input_tensor)\\n\",\n    \"        \\n\",\n    \"        probabilities = F.softmax(logits, dim=-1).numpy()[0]\\n\",\n    \"        predicted_class = np.argmax(probabilities)\\n\",\n    \"        confidence = probabilities[predicted_class]\\n\",\n    \"    \\n\",\n    \"    print(f\\\"ğŸ“ è¾“å…¥æ–‡æœ¬: {text}\\\")\\n\",\n    \"    print(f\\\"ğŸ¯ é¢„æµ‹ç±»åˆ«: {class_names[predicted_class]} (ç½®ä¿¡åº¦: {confidence:.4f})\\\")\\n\",\n    \"    print(f\\\"ğŸ“Š æ‰€æœ‰ç±»åˆ«æ¦‚ç‡:\\\")\\n\",\n    \"    for i, (prob, name) in enumerate(zip(probabilities, class_names)):\\n\",\n    \"        print(f\\\"   {name}: {prob:.4f}\\\")\\n\",\n    \"    \\n\",\n    \"    return predicted_class, confidence, probabilities\\n\",\n    \"\\n\",\n    \"# æµ‹è¯•ä¸€äº›ç¤ºä¾‹\\n\",\n    \"test_texts = [\\n\",\n    \"    \\\"Apple releases new iPhone with amazing camera\\\",\\n\",\n    \"    \\\"Stock market crashes due to economic uncertainty\\\",\\n\",\n    \"    \\\"Scientists discover new planet in distant galaxy\\\",\\n\",\n    \"    \\\"Basketball team wins championship after thrilling game\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(\\\"ğŸ§ª æµ‹è¯•æ–°æ–‡æœ¬é¢„æµ‹:\\\")\\n\",\n    \"print(\\\"=\\\" * 60)\\n\",\n    \"for i, text in enumerate(test_texts, 1):\\n\",\n    \"    print(f\\\"\\\\næµ‹è¯• {i}:\\\")\\n\",\n    \"    predict_new_text(text, model, vocab, config, class_names)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## ç»“è®º\\n\",\n    \"\\n\",\n    \"é€šè¿‡è¿™ä¸ªè¯¦ç»†çš„åˆ†æï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š\\n\",\n    \"\\n\",\n    \"1. **ç†è§£æ¨¡å‹æ€§èƒ½**: é€šè¿‡è®­ç»ƒæ›²çº¿å’Œæ··æ·†çŸ©é˜µäº†è§£æ¨¡å‹çš„ä¼˜ç¼ºç‚¹\\n\",\n    \"2. **è¯†åˆ«é—®é¢˜**: é€šè¿‡é”™è¯¯æ¡ˆä¾‹åˆ†ææ‰¾åˆ°æ¨¡å‹çš„è–„å¼±ç¯èŠ‚\\n\",\n    \"3. **å¯è§†åŒ–ç†è§£**: é€šè¿‡è¯åµŒå…¥å¯è§†åŒ–ç†è§£æ¨¡å‹å­¦åˆ°çš„è¯­ä¹‰è¡¨ç¤º\\n\",\n    \"4. **æ”¹è¿›æ–¹å‘**: åŸºäºåˆ†æç»“æœç¡®å®šä¸‹ä¸€æ­¥æ”¹è¿›ç­–ç•¥\\n\",\n    \"\\n\",\n    \"### ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®\\n\",\n    \"\\n\",\n    \"- å°è¯•ä¸åŒçš„è¶…å‚æ•°è®¾ç½®\\n\",\n    \"- ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†\\n\",\n    \"- è¿›å…¥é˜¶æ®µ2å­¦ä¹ RNN/LSTMå¤„ç†åºåˆ—æ•°æ®\\n\",\n    \"- æ¢ç´¢æ›´é«˜çº§çš„æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•\\n\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.5\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}