"""
BPE (Byte Pair Encoding) åˆ†è¯å™¨å®ç°
==================================

ä»é›¶å®ç°BPEåˆ†è¯å™¨ï¼Œæ”¯æŒï¼š
- åŸºäºè¯­æ–™åº“è®­ç»ƒBPEæ¨¡å‹
- ç¼–ç å’Œè§£ç æ–‡æœ¬
- è¯æ±‡è¡¨ç®¡ç†
- ç‰¹æ®Štokenå¤„ç†

ä½¿ç”¨æ–¹æ³•:
    from utils.tokenizers.bpe_tokenizer import BPETokenizer
    
    tokenizer = BPETokenizer()
    tokenizer.train(['Hello world', 'Hello BPE'], vocab_size=1000)
    tokens = tokenizer.encode('Hello world!')
    text = tokenizer.decode(tokens)
"""

import re
import json
from collections import defaultdict, Counter
from typing import List, Dict, Set, Tuple, Optional, Union
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BPETokenizer:
    """BPEåˆ†è¯å™¨å®ç°"""
    
    def __init__(self, 
                 vocab_size: int = 10000,
                 min_frequency: int = 2,
                 special_tokens: Optional[List[str]] = None,
                 lowercase: bool = False,
                 dropout: float = 0.0):
        """
        åˆå§‹åŒ–BPEåˆ†è¯å™¨
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            min_frequency: æœ€å°è¯é¢‘é˜ˆå€¼
            special_tokens: ç‰¹æ®Štokenåˆ—è¡¨
            lowercase: æ˜¯å¦è½¬å°å†™
            dropout: BPE dropoutç‡
        """
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.lowercase = lowercase
        self.dropout = dropout
        
        # ç‰¹æ®Štoken
        self.special_tokens = special_tokens or [
            '<pad>', '<unk>', '<bos>', '<eos>'
        ]
        
        # åˆå§‹åŒ–è¯æ±‡è¡¨å’Œç¼–ç è¡¨
        self.word_to_id = {}
        self.id_to_word = {}
        self.bpe_ranks = {}
        self.word_freqs = {}
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.word_pattern = re.compile(r'\b\w+\b|[^\w\s]')
        
        # æ˜¯å¦å·²è®­ç»ƒ
        self.trained = False
    
    def _get_word_tokens(self, word: str) -> List[str]:
        """å°†å•è¯åˆ†è§£ä¸ºå­—ç¬¦åˆ—è¡¨ï¼Œæœ€åä¸€ä¸ªå­—ç¬¦åŠ ä¸Š</w>æ ‡è®°"""
        if not word:
            return []
        chars = list(word)
        chars[-1] += '</w>'
        return chars
    
    def _get_pairs(self, word_tokens: List[str]) -> Set[Tuple[str, str]]:
        """è·å–ç›¸é‚»å­—ç¬¦å¯¹"""
        pairs = set()
        prev_char = word_tokens[0]
        for char in word_tokens[1:]:
            pairs.add((prev_char, char))
            prev_char = char
        return pairs
    
    def _get_stats(self, vocab: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, str], int]:
        """ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡"""
        pairs = defaultdict(int)
        for word, freq in vocab.items():
            symbols = list(word)
            for i in range(len(symbols) - 1):
                pairs[(symbols[i], symbols[i + 1])] += freq
        return pairs
    
    def _merge_symbols(self, pair: Tuple[str, str], vocab: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], int]:
        """åˆå¹¶å­—ç¬¦å¯¹"""
        new_vocab = {}
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        
        for word in vocab:
            new_word = p.sub(''.join(pair), ' '.join(word))
            new_vocab[tuple(new_word.split())] = vocab[word]
        return new_vocab
    
    def train(self, 
              texts: List[str], 
              progress_callback: Optional[callable] = None) -> None:
        """
        è®­ç»ƒBPEæ¨¡å‹
        
        Args:
            texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        """
        logger.info(f"å¼€å§‹è®­ç»ƒBPEæ¨¡å‹ï¼Œç›®æ ‡è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        
        # 1. é¢„å¤„ç†æ–‡æœ¬å’Œç»Ÿè®¡è¯é¢‘
        word_freqs = Counter()
        
        for text in texts:
            if self.lowercase:
                text = text.lower()
            
            words = self.word_pattern.findall(text)
            for word in words:
                word_freqs[word] += 1
        
        # è¿‡æ»¤ä½é¢‘è¯
        word_freqs = {word: freq for word, freq in word_freqs.items() 
                     if freq >= self.min_frequency}
        
        logger.info(f"ç»Ÿè®¡åˆ° {len(word_freqs)} ä¸ªä¸é‡å¤å•è¯")
        
        # 2. åˆå§‹åŒ–è¯æ±‡è¡¨
        vocab = {}
        for word, freq in word_freqs.items():
            word_tokens = self._get_word_tokens(word)
            vocab[tuple(word_tokens)] = freq
        
        # æ·»åŠ ç‰¹æ®Štoken
        for special_token in self.special_tokens:
            self.word_to_id[special_token] = len(self.word_to_id)
            self.id_to_word[len(self.id_to_word)] = special_token
        
        # 3. è¿›è¡ŒBPEè®­ç»ƒ
        num_merges = self.vocab_size - len(self.special_tokens) - len(set(''.join(vocab.keys())))
        
        for i in range(num_merges):
            # ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡
            pairs = self._get_stats(vocab)
            
            if not pairs:
                break
                
            # é€‰æ‹©é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹
            best_pair = max(pairs, key=pairs.get)
            
            # è®°å½•åˆå¹¶æ“ä½œ
            self.bpe_ranks[best_pair] = i
            
            # æ‰§è¡Œåˆå¹¶
            vocab = self._merge_symbols(best_pair, vocab)
            
            # è¿›åº¦å›è°ƒ
            if progress_callback and (i + 1) % 100 == 0:
                progress_callback(i + 1, num_merges)
            
            if (i + 1) % 1000 == 0:
                logger.info(f"å®Œæˆ {i + 1}/{num_merges} æ¬¡åˆå¹¶")
        
        # 4. æ„å»ºæœ€ç»ˆè¯æ±‡è¡¨
        for word_tokens in vocab.keys():
            for token in word_tokens:
                if token not in self.word_to_id:
                    self.word_to_id[token] = len(self.word_to_id)
                    self.id_to_word[len(self.id_to_word)] = token
        
        self.word_freqs = word_freqs
        self.trained = True
        
        logger.info(f"BPEè®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(self.word_to_id)}")
    
    def _bpe_encode(self, word: str) -> List[str]:
        """å¯¹å•ä¸ªå•è¯è¿›è¡ŒBPEç¼–ç """
        if not word:
            return []
            
        # æ·»åŠ è¯ç»“å°¾æ ‡è®°
        word_tokens = self._get_word_tokens(word)
        
        if len(word_tokens) == 1:
            return word_tokens
        
        # è¿­ä»£åˆå¹¶
        pairs = self._get_pairs(word_tokens)
        
        if not pairs:
            return word_tokens
        
        while True:
            # æ‰¾åˆ°rankæœ€å°çš„pairï¼ˆæœ€æ—©è¢«åˆå¹¶çš„ï¼‰
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
            
            if bigram not in self.bpe_ranks:
                break
                
            # åˆå¹¶bigram
            first, second = bigram
            new_word = []
            i = 0
            
            while i < len(word_tokens):
                try:
                    j = word_tokens.index(first, i)
                    new_word.extend(word_tokens[i:j])
                    i = j
                except ValueError:
                    new_word.extend(word_tokens[i:])
                    break
                
                if i < len(word_tokens) - 1 and word_tokens[i + 1] == second:
                    new_word.append(first + second)
                    i += 2
                else:
                    new_word.append(word_tokens[i])
                    i += 1
            
            word_tokens = new_word
            
            if len(word_tokens) == 1:
                break
            
            pairs = self._get_pairs(word_tokens)
        
        return word_tokens
    
    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
        """
        ç¼–ç æ–‡æœ¬ä¸ºtoken IDåˆ—è¡¨
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            add_special_tokens: æ˜¯å¦æ·»åŠ ç‰¹æ®Štoken
            
        Returns:
            token IDåˆ—è¡¨
        """
        if not self.trained:
            raise ValueError("BPEæ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        if self.lowercase:
            text = text.lower()
        
        # æå–å•è¯
        words = self.word_pattern.findall(text)
        
        # ç¼–ç æ¯ä¸ªå•è¯
        token_ids = []
        
        if add_special_tokens:
            token_ids.append(self.word_to_id.get('<bos>', 0))
        
        for word in words:
            # BPEç¼–ç 
            bpe_tokens = self._bpe_encode(word)
            
            # è½¬æ¢ä¸ºID
            for token in bpe_tokens:
                token_id = self.word_to_id.get(token, self.word_to_id.get('<unk>', 1))
                token_ids.append(token_id)
        
        if add_special_tokens:
            token_ids.append(self.word_to_id.get('<eos>', 3))
        
        return token_ids
    
    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """
        è§£ç token IDåˆ—è¡¨ä¸ºæ–‡æœ¬
        
        Args:
            token_ids: token IDåˆ—è¡¨
            skip_special_tokens: æ˜¯å¦è·³è¿‡ç‰¹æ®Štoken
            
        Returns:
            è§£ç åçš„æ–‡æœ¬
        """
        if not self.trained:
            raise ValueError("BPEæ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        tokens = []
        for token_id in token_ids:
            token = self.id_to_word.get(token_id, '<unk>')
            
            if skip_special_tokens and token in self.special_tokens:
                continue
                
            tokens.append(token)
        
        # é‡æ„æ–‡æœ¬
        text = ''.join(tokens)
        text = text.replace('</w>', ' ')
        
        return text.strip()
    
    def encode_batch(self, texts: List[str], add_special_tokens: bool = True) -> List[List[int]]:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬"""
        return [self.encode(text, add_special_tokens) for text in texts]
    
    def decode_batch(self, token_ids_list: List[List[int]], skip_special_tokens: bool = True) -> List[str]:
        """æ‰¹é‡è§£ç token ID"""
        return [self.decode(token_ids, skip_special_tokens) for token_ids in token_ids_list]
    
    def get_vocab(self) -> Dict[str, int]:
        """è·å–è¯æ±‡è¡¨"""
        return self.word_to_id.copy()
    
    def get_vocab_size(self) -> int:
        """è·å–è¯æ±‡è¡¨å¤§å°"""
        return len(self.word_to_id)
    
    def save(self, save_path: Union[str, Path]) -> None:
        """ä¿å­˜BPEæ¨¡å‹"""
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config = {
            'vocab_size': self.vocab_size,
            'min_frequency': self.min_frequency,
            'special_tokens': self.special_tokens,
            'lowercase': self.lowercase,
            'dropout': self.dropout,
            'trained': self.trained
        }
        
        with open(save_path / 'config.json', 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯æ±‡è¡¨
        with open(save_path / 'vocab.json', 'w', encoding='utf-8') as f:
            json.dump(self.word_to_id, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜BPEè§„åˆ™
        with open(save_path / 'merges.txt', 'w', encoding='utf-8') as f:
            # æŒ‰rankæ’åº
            sorted_merges = sorted(self.bpe_ranks.items(), key=lambda x: x[1])
            for (first, second), rank in sorted_merges:
                f.write(f"{first} {second}\n")
        
        # ä¿å­˜è¯é¢‘
        with open(save_path / 'word_freqs.json', 'w', encoding='utf-8') as f:
            json.dump(self.word_freqs, f, indent=2, ensure_ascii=False)
        
        logger.info(f"BPEæ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    @classmethod
    def load(cls, load_path: Union[str, Path]) -> 'BPETokenizer':
        """åŠ è½½BPEæ¨¡å‹"""
        load_path = Path(load_path)
        
        # åŠ è½½é…ç½®
        with open(load_path / 'config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # åˆ›å»ºå®ä¾‹
        tokenizer = cls(**{k: v for k, v in config.items() if k != 'trained'})
        
        # åŠ è½½è¯æ±‡è¡¨
        with open(load_path / 'vocab.json', 'r', encoding='utf-8') as f:
            tokenizer.word_to_id = json.load(f)
        
        tokenizer.id_to_word = {v: k for k, v in tokenizer.word_to_id.items()}
        
        # åŠ è½½BPEè§„åˆ™
        tokenizer.bpe_ranks = {}
        with open(load_path / 'merges.txt', 'r', encoding='utf-8') as f:
            for rank, line in enumerate(f):
                parts = line.strip().split()
                if len(parts) == 2:
                    tokenizer.bpe_ranks[(parts[0], parts[1])] = rank
        
        # åŠ è½½è¯é¢‘
        with open(load_path / 'word_freqs.json', 'r', encoding='utf-8') as f:
            tokenizer.word_freqs = json.load(f)
        
        tokenizer.trained = config['trained']
        
        logger.info(f"BPEæ¨¡å‹å·²ä» {load_path} åŠ è½½")
        return tokenizer


def demo():
    """æ¼”ç¤ºBPEåˆ†è¯å™¨çš„ä½¿ç”¨"""
    print("ğŸš€ BPEåˆ†è¯å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºåˆ†è¯å™¨
    tokenizer = BPETokenizer(vocab_size=100, min_frequency=1)
    
    # è®­ç»ƒè¯­æ–™
    texts = [
        "Hello world! This is a test.",
        "Hello BPE tokenizer.",
        "Byte pair encoding is powerful.",
        "The quick brown fox jumps over the lazy dog.",
        "Natural language processing with BPE.",
        "Machine learning and artificial intelligence."
    ]
    
    print("ğŸ“š è®­ç»ƒè¯­æ–™:")
    for text in texts:
        print(f"  - {text}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸ”§ è®­ç»ƒBPEæ¨¡å‹...")
    tokenizer.train(texts)
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
    
    # æµ‹è¯•ç¼–ç å’Œè§£ç 
    test_text = "Hello BPE! This is amazing."
    print(f"\nğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}")
    
    # ç¼–ç 
    token_ids = tokenizer.encode(test_text)
    print(f"ğŸ”¢ ç¼–ç ç»“æœ: {token_ids}")
    
    # è§£ç 
    decoded_text = tokenizer.decode(token_ids)
    print(f"ğŸ“„ è§£ç ç»“æœ: {decoded_text}")
    
    # æ˜¾ç¤ºè¯æ±‡è¡¨
    print(f"\nğŸ“– è¯æ±‡è¡¨ (å‰20ä¸ª):")
    vocab = tokenizer.get_vocab()
    for i, (word, id_) in enumerate(list(vocab.items())[:20]):
        print(f"  {id_:3d}: '{word}'")
    
    # ä¿å­˜æ¨¡å‹
    save_path = "temp_bpe_model"
    tokenizer.save(save_path)
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    # æµ‹è¯•åŠ è½½
    loaded_tokenizer = BPETokenizer.load(save_path)
    test_ids = loaded_tokenizer.encode("Test loading")
    print(f"ğŸ”„ åŠ è½½æµ‹è¯•: {loaded_tokenizer.decode(test_ids)}")


if __name__ == "__main__":
    demo()