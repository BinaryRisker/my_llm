"""
SentencePieceåˆ†è¯å™¨åŒ…è£…å™¨
========================

å¯¹SentencePieceè¿›è¡Œå°è£…ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£å’Œé¢å¤–åŠŸèƒ½ï¼š
- ç»Ÿä¸€çš„è®­ç»ƒå’Œä½¿ç”¨æ¥å£
- æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹ (BPE, Unigram, Word, Char)
- æ‰¹é‡å¤„ç†åŠŸèƒ½
- æ¨¡å‹ç®¡ç†å’ŒæŒä¹…åŒ–

ä½¿ç”¨æ–¹æ³•:
    from utils.tokenizers.sentencepiece_wrapper import SentencePieceWrapper
    
    tokenizer = SentencePieceWrapper()
    tokenizer.train(['Hello world', 'Hello SentencePiece'], vocab_size=1000)
    tokens = tokenizer.encode('Hello world!')
    text = tokenizer.decode(tokens)
"""

import os
import json
import tempfile
from pathlib import Path
from typing import List, Dict, Optional, Union, Any
import logging

try:
    import sentencepiece as spm
    SENTENCEPIECE_AVAILABLE = True
except ImportError:
    SENTENCEPIECE_AVAILABLE = False
    spm = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SentencePieceWrapper:
    """SentencePieceåˆ†è¯å™¨åŒ…è£…å™¨"""
    
    def __init__(self,
                 vocab_size: int = 10000,
                 model_type: str = 'bpe',
                 character_coverage: float = 0.9995,
                 normalization_rule_name: str = 'nmt_nfkc_cf',
                 split_by_unicode_script: bool = True,
                 split_by_number: bool = True,
                 split_by_whitespace: bool = True,
                 treat_whitespace_as_suffix: bool = False,
                 allow_whitespace_only_pieces: bool = False,
                 split_digits: bool = False,
                 control_symbols: Optional[List[str]] = None,
                 user_defined_symbols: Optional[List[str]] = None,
                 byte_fallback: bool = False,
                 vocabulary_output_piece_score: bool = True,
                 train_extremely_large_corpus: bool = False,
                 enable_sampling: bool = False,
                 nbest_size: int = -1,
                 alpha: float = 0.1):
        """
        åˆå§‹åŒ–SentencePieceåŒ…è£…å™¨
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            model_type: æ¨¡å‹ç±»å‹ ('bpe', 'unigram', 'word', 'char')
            character_coverage: å­—ç¬¦è¦†ç›–ç‡
            normalization_rule_name: æ ‡å‡†åŒ–è§„åˆ™
            split_by_unicode_script: æŒ‰Unicodeè„šæœ¬åˆ†å‰²
            split_by_number: æŒ‰æ•°å­—åˆ†å‰²
            split_by_whitespace: æŒ‰ç©ºç™½å­—ç¬¦åˆ†å‰²
            treat_whitespace_as_suffix: å°†ç©ºç™½å­—ç¬¦è§†ä¸ºåç¼€
            allow_whitespace_only_pieces: å…è®¸ä»…ç©ºç™½å­—ç¬¦çš„ç‰‡æ®µ
            split_digits: åˆ†å‰²æ•°å­—
            control_symbols: æ§åˆ¶ç¬¦å·
            user_defined_symbols: ç”¨æˆ·å®šä¹‰ç¬¦å·
            byte_fallback: å­—èŠ‚å›é€€
            vocabulary_output_piece_score: è¾“å‡ºç‰‡æ®µåˆ†æ•°
            train_extremely_large_corpus: è®­ç»ƒæå¤§è¯­æ–™åº“
            enable_sampling: å¯ç”¨é‡‡æ ·
            nbest_size: N-bestå¤§å°
            alpha: é‡‡æ ·å‚æ•°
        """
        if not SENTENCEPIECE_AVAILABLE:
            raise ImportError("SentencePieceæœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install sentencepiece")
        
        self.vocab_size = vocab_size
        self.model_type = model_type.lower()
        
        # éªŒè¯æ¨¡å‹ç±»å‹
        if self.model_type not in ['bpe', 'unigram', 'word', 'char']:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        # è®­ç»ƒå‚æ•°
        self.training_args = {
            'vocab_size': vocab_size,
            'model_type': self.model_type,
            'character_coverage': character_coverage,
            'normalization_rule_name': normalization_rule_name,
            'split_by_unicode_script': split_by_unicode_script,
            'split_by_number': split_by_number,
            'split_by_whitespace': split_by_whitespace,
            'treat_whitespace_as_suffix': treat_whitespace_as_suffix,
            'allow_whitespace_only_pieces': allow_whitespace_only_pieces,
            'split_digits': split_digits,
            'byte_fallback': byte_fallback,
            'vocabulary_output_piece_score': vocabulary_output_piece_score,
            'train_extremely_large_corpus': train_extremely_large_corpus
        }
        
        # æ§åˆ¶ç¬¦å·å’Œç”¨æˆ·å®šä¹‰ç¬¦å·
        if control_symbols:
            self.training_args['control_symbols'] = ','.join(control_symbols)
        
        if user_defined_symbols:
            self.training_args['user_defined_symbols'] = ','.join(user_defined_symbols)
        
        # é‡‡æ ·å‚æ•°
        self.enable_sampling = enable_sampling
        self.nbest_size = nbest_size
        self.alpha = alpha
        
        # SentencePieceå¤„ç†å™¨
        self.sp = None
        self.trained = False
        self.model_path = None
    
    def train(self, 
              texts: List[str],
              model_prefix: str = 'sentencepiece_model',
              temp_dir: Optional[str] = None) -> None:
        """
        è®­ç»ƒSentencePieceæ¨¡å‹
        
        Args:
            texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
            model_prefix: æ¨¡å‹æ–‡ä»¶å‰ç¼€
            temp_dir: ä¸´æ—¶ç›®å½•
        """
        logger.info(f"å¼€å§‹è®­ç»ƒSentencePieceæ¨¡å‹ï¼Œè¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        
        if temp_dir is None:
            temp_dir = tempfile.mkdtemp()
        else:
            os.makedirs(temp_dir, exist_ok=True)
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®æ–‡ä»¶
        train_file = os.path.join(temp_dir, 'train.txt')
        with open(train_file, 'w', encoding='utf-8') as f:
            for text in texts:
                f.write(text.strip() + '\n')
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        train_args = self.training_args.copy()
        train_args['input'] = train_file
        train_args['model_prefix'] = os.path.join(temp_dir, model_prefix)
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        cmd_args = []
        for key, value in train_args.items():
            cmd_args.append(f'--{key}={value}')
        
        logger.info("è®­ç»ƒSentencePieceæ¨¡å‹...")
        
        # è®­ç»ƒæ¨¡å‹
        spm.SentencePieceTrainer.Train(' '.join(cmd_args))
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        self.model_path = f"{train_args['model_prefix']}.model"
        self.sp = spm.SentencePieceProcessor(model_file=self.model_path)
        
        self.trained = True
        
        logger.info(f"SentencePieceè®­ç»ƒå®Œæˆï¼Œå®é™…è¯æ±‡è¡¨å¤§å°: {self.sp.get_piece_size()}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(train_file)
        except OSError:
            pass
    
    def train_from_file(self, 
                       input_file: str,
                       model_prefix: str = 'sentencepiece_model',
                       output_dir: str = '.') -> None:
        """
        ä»æ–‡ä»¶è®­ç»ƒSentencePieceæ¨¡å‹
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            model_prefix: æ¨¡å‹æ–‡ä»¶å‰ç¼€
            output_dir: è¾“å‡ºç›®å½•
        """
        logger.info(f"ä»æ–‡ä»¶è®­ç»ƒSentencePieceæ¨¡å‹: {input_file}")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        train_args = self.training_args.copy()
        train_args['input'] = input_file
        train_args['model_prefix'] = os.path.join(output_dir, model_prefix)
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        cmd_args = []
        for key, value in train_args.items():
            cmd_args.append(f'--{key}={value}')
        
        # è®­ç»ƒæ¨¡å‹
        spm.SentencePieceTrainer.Train(' '.join(cmd_args))
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        self.model_path = f"{train_args['model_prefix']}.model"
        self.sp = spm.SentencePieceProcessor(model_file=self.model_path)
        
        self.trained = True
        
        logger.info(f"SentencePieceè®­ç»ƒå®Œæˆï¼Œå®é™…è¯æ±‡è¡¨å¤§å°: {self.sp.get_piece_size()}")
    
    def encode(self, 
               text: str, 
               out_type: str = 'int',
               add_bos: bool = False,
               add_eos: bool = False,
               enable_sampling: bool = None,
               nbest_size: int = None,
               alpha: float = None) -> Union[List[int], List[str]]:
        """
        ç¼–ç æ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            out_type: è¾“å‡ºç±»å‹ ('int' æˆ– 'str')
            add_bos: æ·»åŠ å¼€å§‹æ ‡è®°
            add_eos: æ·»åŠ ç»“æŸæ ‡è®°
            enable_sampling: å¯ç”¨é‡‡æ ·
            nbest_size: N-bestå¤§å°
            alpha: é‡‡æ ·å‚æ•°
            
        Returns:
            ç¼–ç ç»“æœ (token IDs æˆ– token strings)
        """
        if not self.trained:
            raise ValueError("SentencePieceæ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        if enable_sampling is None:
            enable_sampling = self.enable_sampling
        if nbest_size is None:
            nbest_size = self.nbest_size
        if alpha is None:
            alpha = self.alpha
        
        # ç¼–ç æ–‡æœ¬
        if out_type == 'int':
            tokens = self.sp.encode(text, 
                                   out_type=int,
                                   add_bos=add_bos,
                                   add_eos=add_eos,
                                   enable_sampling=enable_sampling,
                                   nbest_size=nbest_size,
                                   alpha=alpha)
        elif out_type == 'str':
            tokens = self.sp.encode(text, 
                                   out_type=str,
                                   add_bos=add_bos,
                                   add_eos=add_eos,
                                   enable_sampling=enable_sampling,
                                   nbest_size=nbest_size,
                                   alpha=alpha)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºç±»å‹: {out_type}")
        
        return tokens
    
    def decode(self, tokens: Union[List[int], List[str]]) -> str:
        """
        è§£ç tokenåºåˆ—
        
        Args:
            tokens: token IDåˆ—è¡¨æˆ–tokenå­—ç¬¦ä¸²åˆ—è¡¨
            
        Returns:
            è§£ç åçš„æ–‡æœ¬
        """
        if not self.trained:
            raise ValueError("SentencePieceæ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        return self.sp.decode(tokens)
    
    def encode_batch(self, 
                    texts: List[str],
                    out_type: str = 'int',
                    add_bos: bool = False,
                    add_eos: bool = False) -> List[Union[List[int], List[str]]]:\n        \"\"\"æ‰¹é‡ç¼–ç æ–‡æœ¬\"\"\"\n        return [self.encode(text, out_type, add_bos, add_eos) for text in texts]\n    \n    def decode_batch(self, token_lists: List[Union[List[int], List[str]]]) -> List[str]:\n        \"\"\"æ‰¹é‡è§£ç tokenåˆ—è¡¨\"\"\"\n        return [self.decode(tokens) for tokens in token_lists]\n    \n    def get_vocab_size(self) -> int:\n        \"\"\"è·å–è¯æ±‡è¡¨å¤§å°\"\"\"\n        if not self.trained:\n            return self.vocab_size\n        return self.sp.get_piece_size()\n    \n    def get_vocab(self) -> Dict[str, int]:\n        \"\"\"è·å–è¯æ±‡è¡¨å­—å…¸\"\"\"\n        if not self.trained:\n            raise ValueError(\"SentencePieceæ¨¡å‹æœªè®­ç»ƒ\")\n        \n        vocab = {}\n        for i in range(self.sp.get_piece_size()):\n            piece = self.sp.id_to_piece(i)\n            vocab[piece] = i\n        \n        return vocab\n    \n    def piece_to_id(self, piece: str) -> int:\n        \"\"\"å°†pieceè½¬æ¢ä¸ºID\"\"\"\n        if not self.trained:\n            raise ValueError(\"SentencePieceæ¨¡å‹æœªè®­ç»ƒ\")\n        return self.sp.piece_to_id(piece)\n    \n    def id_to_piece(self, id_: int) -> str:\n        \"\"\"å°†IDè½¬æ¢ä¸ºpiece\"\"\"\n        if not self.trained:\n            raise ValueError(\"SentencePieceæ¨¡å‹æœªè®­ç»ƒ\")\n        return self.sp.id_to_piece(id_)\n    \n    def is_unknown(self, id_: int) -> bool:\n        \"\"\"æ£€æŸ¥IDæ˜¯å¦ä¸ºæœªçŸ¥token\"\"\"\n        if not self.trained:\n            raise ValueError(\"SentencePieceæ¨¡å‹æœªè®­ç»ƒ\")\n        return self.sp.is_unknown(id_)\n    \n    def is_control(self, id_: int) -> bool:\n        \"\"\"æ£€æŸ¥IDæ˜¯å¦ä¸ºæ§åˆ¶token\"\"\"\n        if not self.trained:\n            raise ValueError(\"SentencePieceæ¨¡å‹æœªè®­ç»ƒ\")\n        return self.sp.is_control(id_)\n    \n    def get_score(self, id_: int) -> float:\n        \"\"\"è·å–tokençš„åˆ†æ•°\"\"\"\n        if not self.trained:\n            raise ValueError(\"SentencePieceæ¨¡å‹æœªè®­ç»ƒ\")\n        return self.sp.get_score(id_)\n    \n    def save(self, save_dir: Union[str, Path]) -> None:\n        \"\"\"ä¿å­˜æ¨¡å‹\"\"\"\n        if not self.trained:\n            raise ValueError(\"SentencePieceæ¨¡å‹æœªè®­ç»ƒ\")\n        \n        save_dir = Path(save_dir)\n        save_dir.mkdir(parents=True, exist_ok=True)\n        \n        # ä¿å­˜æ¨¡å‹æ–‡ä»¶\n        import shutil\n        model_file = save_dir / 'sentencepiece.model'\n        vocab_file = save_dir / 'sentencepiece.vocab'\n        \n        shutil.copy2(self.model_path, model_file)\n        if os.path.exists(self.model_path.replace('.model', '.vocab')):\n            shutil.copy2(self.model_path.replace('.model', '.vocab'), vocab_file)\n        \n        # ä¿å­˜é…ç½®\n        config = {\n            'vocab_size': self.vocab_size,\n            'model_type': self.model_type,\n            'training_args': self.training_args,\n            'enable_sampling': self.enable_sampling,\n            'nbest_size': self.nbest_size,\n            'alpha': self.alpha,\n            'trained': self.trained\n        }\n        \n        with open(save_dir / 'config.json', 'w', encoding='utf-8') as f:\n            json.dump(config, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"SentencePieceæ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}\")\n    \n    @classmethod\n    def load(cls, load_dir: Union[str, Path]) -> 'SentencePieceWrapper':\n        \"\"\"åŠ è½½æ¨¡å‹\"\"\"\n        load_dir = Path(load_dir)\n        \n        # åŠ è½½é…ç½®\n        config_file = load_dir / 'config.json'\n        with open(config_file, 'r', encoding='utf-8') as f:\n            config = json.load(f)\n        \n        # åˆ›å»ºå®ä¾‹\n        instance = cls(\n            vocab_size=config['vocab_size'],\n            model_type=config['model_type'],\n            enable_sampling=config.get('enable_sampling', False),\n            nbest_size=config.get('nbest_size', -1),\n            alpha=config.get('alpha', 0.1)\n        )\n        \n        # åŠ è½½SentencePieceæ¨¡å‹\n        model_file = load_dir / 'sentencepiece.model'\n        instance.sp = spm.SentencePieceProcessor(model_file=str(model_file))\n        instance.model_path = str(model_file)\n        instance.trained = config['trained']\n        \n        logger.info(f\"SentencePieceæ¨¡å‹å·²ä» {load_dir} åŠ è½½\")\n        return instance\n    \n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"è·å–æ¨¡å‹ä¿¡æ¯\"\"\"\n        if not self.trained:\n            return {\n                'trained': False,\n                'vocab_size': self.vocab_size,\n                'model_type': self.model_type\n            }\n        \n        return {\n            'trained': True,\n            'vocab_size': self.sp.get_piece_size(),\n            'model_type': self.model_type,\n            'bos_id': self.sp.bos_id(),\n            'eos_id': self.sp.eos_id(),\n            'unk_id': self.sp.unk_id(),\n            'pad_id': self.sp.pad_id(),\n            'model_path': self.model_path\n        }\n\n\ndef demo():\n    \"\"\"æ¼”ç¤ºSentencePieceåŒ…è£…å™¨çš„ä½¿ç”¨\"\"\"\n    if not SENTENCEPIECE_AVAILABLE:\n        print(\"âŒ SentencePieceæœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œæ¼”ç¤º\")\n        print(\"è¯·è¿è¡Œ: pip install sentencepiece\")\n        return\n    \n    print(\"ğŸš€ SentencePieceåŒ…è£…å™¨æ¼”ç¤º\")\n    print(\"=\" * 50)\n    \n    # åˆ›å»ºåˆ†è¯å™¨\n    tokenizer = SentencePieceWrapper(\n        vocab_size=100, \n        model_type='bpe',\n        character_coverage=0.995\n    )\n    \n    # è®­ç»ƒè¯­æ–™\n    texts = [\n        \"Hello world! This is a test.\",\n        \"Hello SentencePiece tokenizer.\",\n        \"Byte pair encoding with SentencePiece is powerful.\",\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"Natural language processing with advanced tokenization.\",\n        \"Machine learning and artificial intelligence research.\",\n        \"Deep learning models for text generation.\",\n        \"Transformer architecture revolutionized NLP.\"\n    ]\n    \n    print(\"ğŸ“š è®­ç»ƒè¯­æ–™:\")\n    for i, text in enumerate(texts[:3]):\n        print(f\"  {i+1}. {text}\")\n    print(f\"  ... å…± {len(texts)} æ¡\")\n    \n    # è®­ç»ƒæ¨¡å‹\n    print(\"\\nğŸ”§ è®­ç»ƒSentencePieceæ¨¡å‹...\")\n    try:\n        tokenizer.train(texts)\n        print(f\"âœ… è®­ç»ƒå®Œæˆï¼Œå®é™…è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}\")\n        \n        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯\n        model_info = tokenizer.get_model_info()\n        print(f\"\\nğŸ“Š æ¨¡å‹ä¿¡æ¯:\")\n        for key, value in model_info.items():\n            print(f\"  {key}: {value}\")\n        \n        # æµ‹è¯•ç¼–ç å’Œè§£ç \n        test_text = \"Hello SentencePiece! This is amazing.\"\n        print(f\"\\nğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}\")\n        \n        # ç¼–ç ä¸ºID\n        token_ids = tokenizer.encode(test_text, out_type='int')\n        print(f\"ğŸ”¢ ç¼–ç ç»“æœ (IDs): {token_ids}\")\n        \n        # ç¼–ç ä¸ºå­—ç¬¦ä¸²\n        token_strs = tokenizer.encode(test_text, out_type='str')\n        print(f\"ğŸ“ ç¼–ç ç»“æœ (tokens): {token_strs}\")\n        \n        # è§£ç \n        decoded_text = tokenizer.decode(token_ids)\n        print(f\"ğŸ“„ è§£ç ç»“æœ: {decoded_text}\")\n        \n        # æ˜¾ç¤ºéƒ¨åˆ†è¯æ±‡è¡¨\n        print(f\"\\nğŸ“– è¯æ±‡è¡¨ç¤ºä¾‹ (å‰20ä¸ª):\")\n        for i in range(min(20, tokenizer.get_vocab_size())):\n            piece = tokenizer.id_to_piece(i)\n            score = tokenizer.get_score(i)\n            print(f\"  {i:3d}: '{piece}' (score: {score:.4f})\")\n        \n        # æ‰¹é‡å¤„ç†\n        batch_texts = [\"Hello world\", \"SentencePiece rocks\", \"AI is great\"]\n        batch_encoded = tokenizer.encode_batch(batch_texts)\n        batch_decoded = tokenizer.decode_batch(batch_encoded)\n        \n        print(f\"\\nğŸ”„ æ‰¹é‡å¤„ç†æµ‹è¯•:\")\n        for i, (orig, encoded, decoded) in enumerate(zip(batch_texts, batch_encoded, batch_decoded)):\n            print(f\"  {i+1}. '{orig}' -> {encoded} -> '{decoded}'\")\n        \n        # ä¿å­˜å’ŒåŠ è½½æµ‹è¯•\n        save_path = \"temp_spm_model\"\n        tokenizer.save(save_path)\n        print(f\"\\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}\")\n        \n        # æµ‹è¯•åŠ è½½\n        loaded_tokenizer = SentencePieceWrapper.load(save_path)\n        test_result = loaded_tokenizer.decode(loaded_tokenizer.encode(\"Test loading\"))\n        print(f\"ğŸ”„ åŠ è½½æµ‹è¯•: {test_result}\")\n        \n    except Exception as e:\n        print(f\"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n\n\nif __name__ == \"__main__\":\n    demo()