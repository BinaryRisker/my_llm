"""
æ•°æ®é¢„å¤„ç†ç®¡é“
==============

æä¾›å®Œæ•´çš„æ•°æ®é¢„å¤„ç†ç®¡é“ï¼Œæ”¯æŒï¼š
- å¤šç§æ•°æ®æ ¼å¼çš„ç»Ÿä¸€å¤„ç†
- æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–
- å¤šè¯­è¨€æ•°æ®å¤„ç†
- æ•°æ®éªŒè¯å’Œç»Ÿè®¡åˆ†æ
- ç®¡é“å¼å¤„ç†æµç¨‹

ä½¿ç”¨æ–¹æ³•:
    from utils.data_processing.preprocessing_pipeline import DataPreprocessor
    
    preprocessor = DataPreprocessor()
    preprocessor.add_step('clean_text')
    preprocessor.add_step('tokenize')
    processed_data = preprocessor.process(raw_data)
"""

import re
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Callable, Tuple
from dataclasses import dataclass
from collections import defaultdict, Counter
import unicodedata

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    pd = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class DataStats:
    """æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    total_samples: int = 0
    total_tokens: int = 0
    avg_tokens_per_sample: float = 0.0
    max_tokens: int = 0
    min_tokens: int = 0
    unique_tokens: int = 0
    vocabulary_size: int = 0
    language_distribution: Dict[str, int] = None
    
    def __post_init__(self):
        if self.language_distribution is None:
            self.language_distribution = {}


class TextCleaner:
    """æ–‡æœ¬æ¸…æ´—å·¥å…·"""
    
    def __init__(self):
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.patterns = {
            'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
            'url': re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'),
            'phone': re.compile(r'(\+\d{1,3}\s?)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}'),
            'whitespace': re.compile(r'\s+'),
            'special_chars': re.compile(r'[^\w\s\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]'),
            'numbers': re.compile(r'\d+'),
            'repeated_chars': re.compile(r'(.)\1{2,}'),
            'html_tags': re.compile(r'<[^<]+?>')
        }
    
    def clean_text(self, text: str, 
                   remove_emails: bool = True,
                   remove_urls: bool = True,
                   remove_phones: bool = True,
                   normalize_whitespace: bool = True,
                   remove_html: bool = True,
                   normalize_unicode: bool = True,
                   lowercase: bool = False,
                   remove_special_chars: bool = False,
                   remove_numbers: bool = False,
                   fix_repeated_chars: bool = True) -> str:
        """
        æ¸…æ´—æ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            remove_emails: ç§»é™¤é‚®ç®±åœ°å€
            remove_urls: ç§»é™¤URL
            remove_phones: ç§»é™¤ç”µè¯å·ç 
            normalize_whitespace: æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
            remove_html: ç§»é™¤HTMLæ ‡ç­¾
            normalize_unicode: Unicodeæ ‡å‡†åŒ–
            lowercase: è½¬æ¢ä¸ºå°å†™
            remove_special_chars: ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            remove_numbers: ç§»é™¤æ•°å­—
            fix_repeated_chars: ä¿®å¤é‡å¤å­—ç¬¦
            
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text:
            return text
        
        # Unicodeæ ‡å‡†åŒ–
        if normalize_unicode:
            text = unicodedata.normalize('NFKC', text)
        
        # ç§»é™¤HTMLæ ‡ç­¾
        if remove_html:
            text = self.patterns['html_tags'].sub('', text)
        
        # ç§»é™¤é‚®ç®±åœ°å€
        if remove_emails:
            text = self.patterns['email'].sub('[EMAIL]', text)
        
        # ç§»é™¤URL
        if remove_urls:
            text = self.patterns['url'].sub('[URL]', text)
        
        # ç§»é™¤ç”µè¯å·ç 
        if remove_phones:
            text = self.patterns['phone'].sub('[PHONE]', text)
        
        # ä¿®å¤é‡å¤å­—ç¬¦
        if fix_repeated_chars:
            text = self.patterns['repeated_chars'].sub(r'\1\1', text)
        
        # ç§»é™¤æ•°å­—
        if remove_numbers:
            text = self.patterns['numbers'].sub('[NUM]', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ (ä¿ç•™ä¸­æ—¥éŸ©æ–‡å­—)
        if remove_special_chars:
            text = self.patterns['special_chars'].sub(' ', text)
        
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        if normalize_whitespace:
            text = self.patterns['whitespace'].sub(' ', text)
        
        # è½¬æ¢ä¸ºå°å†™
        if lowercase:
            text = text.lower()
        
        return text.strip()


class LanguageDetector:
    """è¯­è¨€æ£€æµ‹å·¥å…·"""
    
    def __init__(self):
        # ç®€å•çš„åŸºäºUnicodeèŒƒå›´çš„è¯­è¨€æ£€æµ‹
        self.language_patterns = {
            'en': re.compile(r'[a-zA-Z]'),
            'zh': re.compile(r'[\u4e00-\u9fff]'),
            'ja': re.compile(r'[\u3040-\u309f\u30a0-\u30ff]'),
            'ko': re.compile(r'[\uac00-\ud7af]'),
            'ar': re.compile(r'[\u0600-\u06ff]'),
            'ru': re.compile(r'[\u0400-\u04ff]'),
            'de': re.compile(r'[Ã¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]'),
            'fr': re.compile(r'[Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¶Ã¹Ã»Ã¼Ã¿Ã§]'),
            'es': re.compile(r'[Ã¡Ã©Ã­Ã³ÃºÃ±Ã¼Â¡Â¿]'),
            'pt': re.compile(r'[Ã¡Ã Ã¢Ã£Ã©ÃªÃ­Ã³Ã´ÃµÃºÃ§]')
        }
    
    def detect_language(self, text: str, threshold: float = 0.1) -> str:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            threshold: å­—ç¬¦æ¯”ä¾‹é˜ˆå€¼
            
        Returns:
            æ£€æµ‹åˆ°çš„è¯­è¨€ä»£ç 
        """
        if not text:
            return 'unknown'
        
        text_length = len(text)
        language_scores = {}
        
        for lang, pattern in self.language_patterns.items():
            matches = pattern.findall(text)
            score = len(''.join(matches)) / text_length if text_length > 0 else 0
            language_scores[lang] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†çš„è¯­è¨€
        best_lang = max(language_scores.items(), key=lambda x: x[1])
        
        if best_lang[1] >= threshold:
            return best_lang[0]
        else:
            return 'mixed' if len([s for s in language_scores.values() if s > threshold/2]) > 1 else 'unknown'


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    def __init__(self):
        self.validation_rules = []
    
    def add_rule(self, rule: Callable[[str], bool], name: str, description: str):
        """æ·»åŠ éªŒè¯è§„åˆ™"""
        self.validation_rules.append({
            'rule': rule,
            'name': name,
            'description': description
        })
    
    def validate_text(self, text: str) -> Dict[str, Any]:
        """éªŒè¯å•ä¸ªæ–‡æœ¬"""
        results = {
            'is_valid': True,
            'errors': [],
            'warnings': []
        }
        
        if not text or not text.strip():
            results['is_valid'] = False
            results['errors'].append('Empty or whitespace-only text')
            return results
        
        # åº”ç”¨æ‰€æœ‰éªŒè¯è§„åˆ™
        for rule_info in self.validation_rules:
            try:
                if not rule_info['rule'](text):
                    results['warnings'].append(f"Failed rule: {rule_info['name']} - {rule_info['description']}")
            except Exception as e:
                results['errors'].append(f"Error in rule {rule_info['name']}: {str(e)}")
        
        return results
    
    def validate_dataset(self, texts: List[str]) -> Dict[str, Any]:
        """éªŒè¯æ•´ä¸ªæ•°æ®é›†"""
        results = {
            'total_samples': len(texts),
            'valid_samples': 0,
            'invalid_samples': 0,
            'warnings_count': 0,
            'errors_count': 0,
            'sample_results': []
        }
        
        for i, text in enumerate(texts):
            sample_result = self.validate_text(text)
            sample_result['index'] = i
            results['sample_results'].append(sample_result)
            
            if sample_result['is_valid']:
                results['valid_samples'] += 1
            else:
                results['invalid_samples'] += 1
            
            results['warnings_count'] += len(sample_result['warnings'])
            results['errors_count'] += len(sample_result['errors'])
        
        return results


class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.text_cleaner = TextCleaner()
        self.language_detector = LanguageDetector()
        self.data_validator = DataValidator()
        
        # å¤„ç†æ­¥éª¤ç®¡é“
        self.pipeline_steps = []
        
        # å†…ç½®å¤„ç†æ­¥éª¤
        self.builtin_steps = {
            'clean_text': self._clean_text_step,
            'detect_language': self._detect_language_step,
            'validate_data': self._validate_data_step,
            'compute_stats': self._compute_stats_step,
            'filter_by_length': self._filter_by_length_step,
            'filter_by_language': self._filter_by_language_step,
            'deduplicate': self._deduplicate_step
        }
    
    def add_step(self, step_name: str, **kwargs):
        """æ·»åŠ å¤„ç†æ­¥éª¤"""
        if step_name in self.builtin_steps:
            self.pipeline_steps.append((step_name, kwargs))
        else:
            raise ValueError(f"Unknown step: {step_name}")
    
    def add_custom_step(self, step_func: Callable, step_name: str, **kwargs):
        """æ·»åŠ è‡ªå®šä¹‰å¤„ç†æ­¥éª¤"""
        self.pipeline_steps.append((step_func, kwargs))
    
    def clear_steps(self):
        """æ¸…ç©ºå¤„ç†æ­¥éª¤"""
        self.pipeline_steps.clear()
    
    def process(self, data: Union[List[str], List[Dict[str, Any]]], 
                return_stats: bool = True) -> Tuple[List[Any], Optional[Dict[str, Any]]]:
        """
        æ‰§è¡Œæ•°æ®é¢„å¤„ç†ç®¡é“
        
        Args:
            data: è¾“å…¥æ•°æ®
            return_stats: æ˜¯å¦è¿”å›ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            å¤„ç†åçš„æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹æ•°æ®é¢„å¤„ç†ï¼Œè¾“å…¥æ ·æœ¬æ•°: {len(data)}")
        
        processed_data = data.copy()
        processing_stats = {
            'original_count': len(data),
            'final_count': 0,
            'step_results': {}
        }
        
        # æ‰§è¡Œæ¯ä¸ªå¤„ç†æ­¥éª¤
        for step_info in self.pipeline_steps:
            if isinstance(step_info[0], str):
                # å†…ç½®æ­¥éª¤
                step_name, kwargs = step_info
                step_func = self.builtin_steps[step_name]
                logger.info(f"æ‰§è¡Œæ­¥éª¤: {step_name}")
            else:
                # è‡ªå®šä¹‰æ­¥éª¤
                step_func, kwargs = step_info
                step_name = getattr(step_func, '__name__', 'custom_step')
                logger.info(f"æ‰§è¡Œè‡ªå®šä¹‰æ­¥éª¤: {step_name}")
            
            try:
                processed_data, step_stats = step_func(processed_data, **kwargs)
                processing_stats['step_results'][step_name] = step_stats
                logger.info(f"æ­¥éª¤ {step_name} å®Œæˆï¼Œå‰©ä½™æ ·æœ¬æ•°: {len(processed_data)}")
            except Exception as e:
                logger.error(f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥: {e}")
                raise
        
        processing_stats['final_count'] = len(processed_data)
        
        if return_stats:
            return processed_data, processing_stats
        else:
            return processed_data, None
    
    def _clean_text_step(self, data: List[Any], **kwargs) -> Tuple[List[Any], Dict[str, Any]]:
        """æ–‡æœ¬æ¸…æ´—æ­¥éª¤"""
        cleaned_data = []
        clean_stats = {'cleaned_count': 0}
        
        for item in data:
            if isinstance(item, str):
                cleaned_text = self.text_cleaner.clean_text(item, **kwargs)
                cleaned_data.append(cleaned_text)
                if cleaned_text != item:
                    clean_stats['cleaned_count'] += 1
            elif isinstance(item, dict) and 'text' in item:
                cleaned_text = self.text_cleaner.clean_text(item['text'], **kwargs)
                item_copy = item.copy()
                item_copy['text'] = cleaned_text
                cleaned_data.append(item_copy)
                if cleaned_text != item['text']:
                    clean_stats['cleaned_count'] += 1
            else:
                cleaned_data.append(item)
        
        return cleaned_data, clean_stats
    
    def _detect_language_step(self, data: List[Any], **kwargs) -> Tuple[List[Any], Dict[str, Any]]:
        """è¯­è¨€æ£€æµ‹æ­¥éª¤"""
        language_stats = defaultdict(int)
        
        for item in data:
            text = item if isinstance(item, str) else item.get('text', '')
            language = self.language_detector.detect_language(text, **kwargs)
            language_stats[language] += 1
            
            if isinstance(item, dict):
                item['detected_language'] = language
        
        return data, {'language_distribution': dict(language_stats)}
    
    def _validate_data_step(self, data: List[Any], **kwargs) -> Tuple[List[Any], Dict[str, Any]]:
        """æ•°æ®éªŒè¯æ­¥éª¤"""
        texts = [item if isinstance(item, str) else item.get('text', '') for item in data]
        validation_results = self.data_validator.validate_dataset(texts)
        
        return data, validation_results
    
    def _compute_stats_step(self, data: List[Any], **kwargs) -> Tuple[List[Any], Dict[str, Any]]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯æ­¥éª¤"""
        texts = [item if isinstance(item, str) else item.get('text', '') for item in data]
        
        token_counts = [len(text.split()) for text in texts]
        all_tokens = []
        for text in texts:
            all_tokens.extend(text.split())
        
        stats = DataStats(
            total_samples=len(texts),
            total_tokens=len(all_tokens),
            avg_tokens_per_sample=sum(token_counts) / len(token_counts) if token_counts else 0,
            max_tokens=max(token_counts) if token_counts else 0,
            min_tokens=min(token_counts) if token_counts else 0,
            unique_tokens=len(set(all_tokens)),
            vocabulary_size=len(set(all_tokens))
        )
        
        return data, stats.__dict__
    
    def _filter_by_length_step(self, data: List[Any], 
                              min_length: int = 1, 
                              max_length: int = 10000, **kwargs) -> Tuple[List[Any], Dict[str, Any]]:
        """æŒ‰é•¿åº¦è¿‡æ»¤æ­¥éª¤"""
        filtered_data = []
        filter_stats = {'filtered_out': 0, 'too_short': 0, 'too_long': 0}
        
        for item in data:
            text = item if isinstance(item, str) else item.get('text', '')
            text_length = len(text.split())
            
            if text_length < min_length:
                filter_stats['filtered_out'] += 1
                filter_stats['too_short'] += 1
            elif text_length > max_length:
                filter_stats['filtered_out'] += 1
                filter_stats['too_long'] += 1
            else:
                filtered_data.append(item)
        
        return filtered_data, filter_stats
    
    def _filter_by_language_step(self, data: List[Any], 
                                allowed_languages: List[str], **kwargs) -> Tuple[List[Any], Dict[str, Any]]:
        """æŒ‰è¯­è¨€è¿‡æ»¤æ­¥éª¤"""
        filtered_data = []
        filter_stats = {'filtered_out': 0, 'language_distribution': defaultdict(int)}
        
        for item in data:
            if isinstance(item, dict) and 'detected_language' in item:
                language = item['detected_language']
            else:
                text = item if isinstance(item, str) else item.get('text', '')
                language = self.language_detector.detect_language(text)
            
            filter_stats['language_distribution'][language] += 1
            
            if language in allowed_languages:
                filtered_data.append(item)
            else:
                filter_stats['filtered_out'] += 1
        
        filter_stats['language_distribution'] = dict(filter_stats['language_distribution'])
        return filtered_data, filter_stats
    
    def _deduplicate_step(self, data: List[Any], **kwargs) -> Tuple[List[Any], Dict[str, Any]]:
        """å»é‡æ­¥éª¤"""
        seen_texts = set()
        deduplicated_data = []
        dedup_stats = {'duplicates_removed': 0}
        
        for item in data:
            text = item if isinstance(item, str) else item.get('text', '')
            text_hash = hash(text)
            
            if text_hash not in seen_texts:
                seen_texts.add(text_hash)
                deduplicated_data.append(item)
            else:
                dedup_stats['duplicates_removed'] += 1
        
        return deduplicated_data, dedup_stats
    
    def setup_default_pipeline(self, 
                              clean_text: bool = True,
                              detect_language: bool = True,
                              validate_data: bool = True,
                              filter_by_length: bool = True,
                              min_length: int = 5,
                              max_length: int = 512,
                              deduplicate: bool = True,
                              compute_stats: bool = True):
        """è®¾ç½®é»˜è®¤çš„é¢„å¤„ç†ç®¡é“"""
        self.clear_steps()
        
        if clean_text:
            self.add_step('clean_text', 
                         normalize_whitespace=True,
                         remove_html=True,
                         normalize_unicode=True)
        
        if detect_language:
            self.add_step('detect_language')
        
        if validate_data:
            self.add_step('validate_data')
        
        if filter_by_length:
            self.add_step('filter_by_length', 
                         min_length=min_length, 
                         max_length=max_length)
        
        if deduplicate:
            self.add_step('deduplicate')
        
        if compute_stats:
            self.add_step('compute_stats')


def demo():
    """æ¼”ç¤ºæ•°æ®é¢„å¤„ç†ç®¡é“çš„ä½¿ç”¨"""
    print("ğŸš€ æ•°æ®é¢„å¤„ç†ç®¡é“æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    sample_data = [
        "Hello world! This is a great example.",
        "è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡ç¤ºä¾‹æ–‡æœ¬ã€‚åŒ…å«ä¸€äº›å†…å®¹ã€‚",
        "   Extra   whitespace   everywhere   ",
        "Contact us at test@example.com or visit https://example.com",
        "<p>HTML content with <b>bold</b> text</p>",
        "Short",
        "This is a duplicate text for testing.",
        "This is a duplicate text for testing.",  # é‡å¤æ–‡æœ¬
        "æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚",
        "Very very very long repeated characters!!!!!!!",
        "",  # ç©ºæ–‡æœ¬
        "Another normal English sentence with good length."
    ]
    
    print(f"ğŸ“ åŸå§‹æ•°æ® ({len(sample_data)} æ ·æœ¬):")
    for i, text in enumerate(sample_data[:5]):
        print(f"  {i+1}. '{text}'")
    print(f"  ... å…± {len(sample_data)} æ¡")
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = DataPreprocessor()
    
    # è®¾ç½®é»˜è®¤ç®¡é“
    preprocessor.setup_default_pipeline(
        clean_text=True,
        detect_language=True,
        validate_data=True,
        filter_by_length=True,
        min_length=3,
        max_length=100,
        deduplicate=True,
        compute_stats=True
    )
    
    print(f"\nğŸ”§ å¤„ç†ç®¡é“æ­¥éª¤:")
    for i, (step_name, kwargs) in enumerate(preprocessor.pipeline_steps):
        print(f"  {i+1}. {step_name} {kwargs}")
    
    # æ‰§è¡Œé¢„å¤„ç†
    print(f"\nâš™ï¸ æ‰§è¡Œé¢„å¤„ç†...")
    processed_data, stats = preprocessor.process(sample_data, return_stats=True)
    
    print(f"\nâœ… é¢„å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š å¤„ç†ç»“æœ:")
    print(f"  åŸå§‹æ ·æœ¬æ•°: {stats['original_count']}")
    print(f"  æœ€ç»ˆæ ·æœ¬æ•°: {stats['final_count']}")
    print(f"  è¿‡æ»¤ç‡: {(1 - stats['final_count'] / stats['original_count']) * 100:.1f}%")
    
    # æ˜¾ç¤ºå„æ­¥éª¤ç»Ÿè®¡
    print(f"\nğŸ“‹ å„æ­¥éª¤ç»Ÿè®¡:")
    for step_name, step_stats in stats['step_results'].items():
        print(f"  {step_name}:")
        if isinstance(step_stats, dict):
            for key, value in step_stats.items():
                if isinstance(value, dict):
                    print(f"    {key}: {len(value)} é¡¹")
                else:
                    print(f"    {key}: {value}")
        else:
            print(f"    ç»“æœ: {step_stats}")
    
    # æ˜¾ç¤ºå¤„ç†åçš„æ•°æ®ç¤ºä¾‹
    print(f"\nğŸ“„ å¤„ç†åçš„æ•°æ®ç¤ºä¾‹:")
    for i, text in enumerate(processed_data[:5]):
        display_text = text if isinstance(text, str) else text.get('text', str(text))
        print(f"  {i+1}. '{display_text}'")
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    demo()