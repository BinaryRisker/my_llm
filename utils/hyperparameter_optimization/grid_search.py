"""
ç½‘æ ¼æœç´¢è¶…å‚æ•°ä¼˜åŒ–
================

å®ç°ç½‘æ ¼æœç´¢ç®—æ³•è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼š
- ç©·ä¸¾æ‰€æœ‰è¶…å‚æ•°ç»„åˆ
- æ”¯æŒå¤šç§è¯„ä¼°æŒ‡æ ‡
- å¹¶è¡Œæ‰§è¡Œå’Œç»“æœç¼“å­˜
- è¯¦ç»†çš„æœç´¢æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    from utils.hyperparameter_optimization.grid_search import GridSearchOptimizer
    
    optimizer = GridSearchOptimizer()
    best_params = optimizer.search(param_grid, objective_function)
"""

import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Callable, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from itertools import product
import concurrent.futures
from collections import defaultdict

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    pd = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœæ•°æ®ç±»"""
    best_params: Dict[str, Any]
    best_score: float
    best_std: float
    cv_results: List[Dict[str, Any]]
    search_time: float
    total_trials: int
    successful_trials: int
    failed_trials: int
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass 
class TrialResult:
    """å•æ¬¡è¯•éªŒç»“æœ"""
    params: Dict[str, Any]
    score: float
    std: float
    execution_time: float
    status: str  # 'success', 'failed', 'timeout'
    error_message: Optional[str] = None
    additional_metrics: Optional[Dict[str, Any]] = None


class GridSearchOptimizer:
    """ç½‘æ ¼æœç´¢ä¼˜åŒ–å™¨"""
    
    def __init__(self,
                 scoring: str = 'accuracy',
                 cv: int = 3,
                 n_jobs: int = 1,
                 verbose: bool = True,
                 cache_results: bool = True,
                 timeout: Optional[float] = None):
        """
        åˆå§‹åŒ–ç½‘æ ¼æœç´¢ä¼˜åŒ–å™¨
        
        Args:
            scoring: è¯„åˆ†æ–¹å¼
            cv: äº¤å‰éªŒè¯æŠ˜æ•°
            n_jobs: å¹¶è¡Œä»»åŠ¡æ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            cache_results: æ˜¯å¦ç¼“å­˜ç»“æœ
            timeout: å•æ¬¡è¯•éªŒè¶…æ—¶æ—¶é—´
        """
        self.scoring = scoring
        self.cv = cv
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.cache_results = cache_results
        self.timeout = timeout
        
        # ç»“æœç¼“å­˜
        self.results_cache = {}
        self.trial_history = []
    
    def _generate_param_combinations(self, param_grid: Dict[str, List[Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ"""
        if not param_grid:
            return [{}]
        
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        
        combinations = []
        for combination in product(*param_values):
            param_dict = dict(zip(param_names, combination))
            combinations.append(param_dict)
        
        return combinations
    
    def _params_to_key(self, params: Dict[str, Any]) -> str:
        """å°†å‚æ•°å­—å…¸è½¬æ¢ä¸ºç¼“å­˜é”®"""
        sorted_items = sorted(params.items())
        return json.dumps(sorted_items, sort_keys=True, default=str)
    
    def _evaluate_single_trial(self, 
                              params: Dict[str, Any], 
                              objective_function: Callable,
                              trial_id: int) -> TrialResult:
        """è¯„ä¼°å•æ¬¡è¯•éªŒ"""
        start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._params_to_key(params)
        if self.cache_results and cache_key in self.results_cache:
            cached_result = self.results_cache[cache_key]
            if self.verbose:
                logger.info(f"è¯•éªŒ {trial_id}: ä½¿ç”¨ç¼“å­˜ç»“æœ - {cached_result.score:.4f}")
            return cached_result
        
        try:
            if self.verbose:
                logger.info(f"è¯•éªŒ {trial_id}: å‚æ•° {params}")
            
            # æ‰§è¡Œç›®æ ‡å‡½æ•°
            if self.timeout:
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(objective_function, params)
                    result = future.result(timeout=self.timeout)
            else:
                result = objective_function(params)
            
            execution_time = time.time() - start_time
            
            # å¤„ç†è¿”å›ç»“æœ
            if isinstance(result, (int, float)):
                score = float(result)
                std = 0.0
                additional_metrics = None
            elif isinstance(result, dict):
                score = float(result.get('score', result.get('accuracy', 0.0)))
                std = float(result.get('std', 0.0))
                additional_metrics = {k: v for k, v in result.items() 
                                    if k not in ['score', 'accuracy', 'std']}
            elif isinstance(result, (list, tuple)) and len(result) >= 2:
                score = float(result[0])
                std = float(result[1])
                additional_metrics = result[2] if len(result) > 2 else None
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¿”å›ç»“æœæ ¼å¼: {type(result)}")
            
            trial_result = TrialResult(
                params=params,
                score=score,
                std=std,
                execution_time=execution_time,
                status='success',
                additional_metrics=additional_metrics
            )
            
            # ç¼“å­˜ç»“æœ
            if self.cache_results:
                self.results_cache[cache_key] = trial_result
            
            if self.verbose:
                logger.info(f"è¯•éªŒ {trial_id}: å¾—åˆ† {score:.4f} (Â±{std:.4f}), ç”¨æ—¶ {execution_time:.2f}s")
            
            return trial_result
            
        except concurrent.futures.TimeoutError:
            execution_time = time.time() - start_time
            error_msg = f"è¯•éªŒè¶…æ—¶ ({self.timeout}s)"
            logger.warning(f"è¯•éªŒ {trial_id}: {error_msg}")
            
            return TrialResult(
                params=params,
                score=float('-inf'),
                std=0.0,
                execution_time=execution_time,
                status='timeout',
                error_message=error_msg
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"è¯•éªŒå¤±è´¥: {str(e)}"
            logger.error(f"è¯•éªŒ {trial_id}: {error_msg}")
            
            return TrialResult(
                params=params,
                score=float('-inf'),
                std=0.0,
                execution_time=execution_time,
                status='failed',
                error_message=error_msg
            )
    
    def search(self, 
               param_grid: Dict[str, List[Any]],
               objective_function: Callable,
               maximize: bool = True) -> OptimizationResult:
        """
        æ‰§è¡Œç½‘æ ¼æœç´¢
        
        Args:
            param_grid: å‚æ•°ç½‘æ ¼
            objective_function: ç›®æ ‡å‡½æ•°
            maximize: æ˜¯å¦æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logger.info("å¼€å§‹ç½‘æ ¼æœç´¢ä¼˜åŒ–...")
        start_time = time.time()
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_combinations = self._generate_param_combinations(param_grid)
        total_trials = len(param_combinations)
        
        logger.info(f"æ€»å…±éœ€è¦è¯„ä¼° {total_trials} ä¸ªå‚æ•°ç»„åˆ")
        
        # æ‰§è¡Œæœç´¢
        trial_results = []
        
        if self.n_jobs == 1:
            # ä¸²è¡Œæ‰§è¡Œ
            for i, params in enumerate(param_combinations):
                result = self._evaluate_single_trial(params, objective_function, i + 1)
                trial_results.append(result)
                self.trial_history.append(result)
        else:
            # å¹¶è¡Œæ‰§è¡Œ
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.n_jobs) as executor:
                futures = {
                    executor.submit(self._evaluate_single_trial, params, objective_function, i + 1): i
                    for i, params in enumerate(param_combinations)
                }
                
                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    trial_results.append(result)
                    self.trial_history.append(result)
        
        # åˆ†æç»“æœ
        successful_results = [r for r in trial_results if r.status == 'success']
        failed_trials = len([r for r in trial_results if r.status == 'failed'])
        timeout_trials = len([r for r in trial_results if r.status == 'timeout'])
        
        if not successful_results:
            raise ValueError("æ‰€æœ‰è¯•éªŒéƒ½å¤±è´¥äº†ï¼Œæ— æ³•æ‰¾åˆ°æœ€ä½³å‚æ•°")
        
        # æ‰¾åˆ°æœ€ä½³ç»“æœ
        if maximize:
            best_result = max(successful_results, key=lambda x: x.score)
        else:
            best_result = min(successful_results, key=lambda x: x.score)
        
        # å‡†å¤‡CVç»“æœ
        cv_results = []
        for result in trial_results:
            cv_result = {
                'params': result.params,
                'mean_test_score': result.score,
                'std_test_score': result.std,
                'rank_test_score': 0,  # ç¨åå¡«å……
                'execution_time': result.execution_time,
                'status': result.status
            }
            if result.additional_metrics:
                cv_result.update(result.additional_metrics)
            cv_results.append(cv_result)
        
        # è®¡ç®—æ’å
        successful_cv_results = [r for r in cv_results if r['status'] == 'success']
        if maximize:
            successful_cv_results.sort(key=lambda x: x['mean_test_score'], reverse=True)
        else:
            successful_cv_results.sort(key=lambda x: x['mean_test_score'])
        
        # å¡«å……æ’å
        for i, result in enumerate(successful_cv_results):
            result['rank_test_score'] = i + 1
        
        search_time = time.time() - start_time
        
        # åˆ›å»ºä¼˜åŒ–ç»“æœ
        optimization_result = OptimizationResult(
            best_params=best_result.params,
            best_score=best_result.score,
            best_std=best_result.std,
            cv_results=cv_results,
            search_time=search_time,
            total_trials=total_trials,
            successful_trials=len(successful_results),
            failed_trials=failed_trials + timeout_trials
        )
        
        logger.info(f"ç½‘æ ¼æœç´¢å®Œæˆ!")
        logger.info(f"æœ€ä½³å‚æ•°: {best_result.params}")
        logger.info(f"æœ€ä½³å¾—åˆ†: {best_result.score:.4f} (Â±{best_result.std:.4f})")
        logger.info(f"æ€»ç”¨æ—¶: {search_time:.2f}s")
        logger.info(f"æˆåŠŸ/å¤±è´¥/æ€»æ•°: {len(successful_results)}/{failed_trials + timeout_trials}/{total_trials}")
        
        return optimization_result
    
    def get_top_results(self, n: int = 5, maximize: bool = True) -> List[TrialResult]:
        """è·å–å‰Nä¸ªæœ€ä½³ç»“æœ"""
        successful_results = [r for r in self.trial_history if r.status == 'success']
        
        if maximize:
            sorted_results = sorted(successful_results, key=lambda x: x.score, reverse=True)
        else:
            sorted_results = sorted(successful_results, key=lambda x: x.score)
        
        return sorted_results[:n]
    
    def save_results(self, filepath: Union[str, Path], 
                    optimization_result: OptimizationResult) -> None:
        """ä¿å­˜æœç´¢ç»“æœ"""
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(optimization_result.to_dict(), f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    
    def load_results(self, filepath: Union[str, Path]) -> OptimizationResult:
        """åŠ è½½æœç´¢ç»“æœ"""
        filepath = Path(filepath)
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return OptimizationResult(**data)
    
    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self.results_cache.clear()
        self.trial_history.clear()
        logger.info("ç¼“å­˜å·²æ¸…ç©º")
    
    def get_search_summary(self) -> Dict[str, Any]:
        """è·å–æœç´¢æ‘˜è¦ç»Ÿè®¡"""
        if not self.trial_history:
            return {"message": "å°šæœªæ‰§è¡Œæœç´¢"}
        
        successful_results = [r for r in self.trial_history if r.status == 'success']
        failed_results = [r for r in self.trial_history if r.status != 'success']
        
        if successful_results:
            scores = [r.score for r in successful_results]
            execution_times = [r.execution_time for r in successful_results]
            
            summary = {
                'total_trials': len(self.trial_history),
                'successful_trials': len(successful_results),
                'failed_trials': len(failed_results),
                'success_rate': len(successful_results) / len(self.trial_history),
                'best_score': max(scores),
                'worst_score': min(scores),
                'mean_score': sum(scores) / len(scores),
                'score_std': (sum((s - sum(scores) / len(scores)) ** 2 for s in scores) / len(scores)) ** 0.5,
                'total_time': sum(execution_times),
                'mean_time_per_trial': sum(execution_times) / len(execution_times),
                'max_time_per_trial': max(execution_times),
                'min_time_per_trial': min(execution_times)
            }
        else:
            summary = {
                'total_trials': len(self.trial_history),
                'successful_trials': 0,
                'failed_trials': len(failed_results),
                'success_rate': 0.0,
                'message': 'æ‰€æœ‰è¯•éªŒéƒ½å¤±è´¥äº†'
            }
        
        return summary


def demo():
    """æ¼”ç¤ºç½‘æ ¼æœç´¢çš„ä½¿ç”¨"""
    print("ğŸš€ ç½‘æ ¼æœç´¢è¶…å‚æ•°ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    # å®šä¹‰ä¸€ä¸ªæ¨¡æ‹Ÿçš„ç›®æ ‡å‡½æ•°
    def objective_function(params):
        """æ¨¡æ‹Ÿçš„ç›®æ ‡å‡½æ•°"""
        import random
        import time
        
        # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        time.sleep(random.uniform(0.1, 0.5))
        
        # æ¨¡æ‹Ÿä¸€äº›å‚æ•°çš„å½±å“
        lr = params.get('learning_rate', 0.001)
        batch_size = params.get('batch_size', 32)
        hidden_size = params.get('hidden_size', 128)
        
        # äººå·¥è®¾è®¡çš„å‡½æ•°ï¼Œæœ‰ä¸€ä¸ªæœ€ä¼˜ç‚¹
        score = (
            0.9 - abs(lr - 0.001) * 100 +  # æœ€ä½³lræ˜¯0.001
            0.1 - abs(batch_size - 64) * 0.001 +  # æœ€ä½³batch_sizeæ˜¯64
            0.1 - abs(hidden_size - 256) * 0.0001  # æœ€ä½³hidden_sizeæ˜¯256
        )
        
        # æ·»åŠ ä¸€äº›éšæœºå™ªå£°
        noise = random.uniform(-0.05, 0.05)
        score += noise
        
        # æ¨¡æ‹Ÿäº¤å‰éªŒè¯çš„æ ‡å‡†å·®
        std = random.uniform(0.01, 0.03)
        
        # è¿”å›é¢å¤–æŒ‡æ ‡
        additional_metrics = {
            'train_accuracy': score + random.uniform(-0.02, 0.02),
            'val_loss': 1.0 - score + random.uniform(-0.1, 0.1)
        }
        
        return {
            'score': max(0, score),  # ç¡®ä¿åˆ†æ•°éè´Ÿ
            'std': std,
            'train_accuracy': additional_metrics['train_accuracy'],
            'val_loss': additional_metrics['val_loss']
        }
    
    # å®šä¹‰æœç´¢ç©ºé—´
    param_grid = {
        'learning_rate': [0.0001, 0.001, 0.01, 0.1],
        'batch_size': [16, 32, 64, 128],
        'hidden_size': [64, 128, 256, 512]
    }
    
    print("ğŸ“‹ æœç´¢ç©ºé—´:")
    for param, values in param_grid.items():
        print(f"  {param}: {values}")
    
    total_combinations = 1
    for values in param_grid.values():
        total_combinations *= len(values)
    print(f"  æ€»ç»„åˆæ•°: {total_combinations}")
    
    # åˆ›å»ºç½‘æ ¼æœç´¢ä¼˜åŒ–å™¨
    optimizer = GridSearchOptimizer(
        scoring='accuracy',
        cv=3,
        n_jobs=2,  # ä½¿ç”¨2ä¸ªå¹¶è¡Œä»»åŠ¡
        verbose=True,
        cache_results=True,
        timeout=10.0  # 10ç§’è¶…æ—¶
    )
    
    print(f"\nğŸ” å¼€å§‹ç½‘æ ¼æœç´¢...")
    
    # æ‰§è¡Œæœç´¢
    result = optimizer.search(
        param_grid=param_grid,
        objective_function=objective_function,
        maximize=True
    )
    
    print(f"\nğŸ“Š æœç´¢ç»“æœ:")
    print(f"  æœ€ä½³å‚æ•°: {result.best_params}")
    print(f"  æœ€ä½³å¾—åˆ†: {result.best_score:.4f} (Â±{result.best_std:.4f})")
    print(f"  æœç´¢æ—¶é—´: {result.search_time:.2f}ç§’")
    print(f"  æˆåŠŸè¯•éªŒ: {result.successful_trials}/{result.total_trials}")
    
    # æ˜¾ç¤ºå‰5ä¸ªæœ€ä½³ç»“æœ
    print(f"\nğŸ† å‰5ä¸ªæœ€ä½³ç»“æœ:")
    top_results = optimizer.get_top_results(n=5, maximize=True)
    for i, trial in enumerate(top_results):
        print(f"  {i+1}. å¾—åˆ†: {trial.score:.4f}, å‚æ•°: {trial.params}")
    
    # æœç´¢æ‘˜è¦
    print(f"\nğŸ“ˆ æœç´¢æ‘˜è¦:")
    summary = optimizer.get_search_summary()
    for key, value in summary.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
        else:
            print(f"  {key}: {value}")
    
    # ä¿å­˜ç»“æœ
    save_path = "temp_grid_search_results.json"
    optimizer.save_results(save_path, result)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    
    # æµ‹è¯•åŠ è½½ç»“æœ
    loaded_result = optimizer.load_results(save_path)
    print(f"ğŸ”„ åŠ è½½æµ‹è¯• - æœ€ä½³å¾—åˆ†: {loaded_result.best_score:.4f}")
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    demo()