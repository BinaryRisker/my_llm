"""
GPTæ–‡æœ¬ç”Ÿæˆå™¨å®ç°
==================

åŒ…å«å¤šç§ç”Ÿæˆç­–ç•¥ï¼š
- è´ªå¿ƒè§£ç 
- éšæœºé‡‡æ · (Top-K, Top-P)
- æŸæœç´¢
- å¯¹æ¯”æœç´¢
- æ‰¹é‡ç”Ÿæˆ
"""

import torch
import torch.nn.functional as F
from typing import List, Optional, Dict, Union, Tuple
import math
from abc import ABC, abstractmethod
from dataclasses import dataclass


@dataclass
class GenerationConfig:
    """æ–‡æœ¬ç”Ÿæˆé…ç½®"""
    max_new_tokens: int = 50          # æœ€å¤§ç”Ÿæˆtokenæ•°
    temperature: float = 1.0          # é‡‡æ ·æ¸©åº¦
    top_k: Optional[int] = None       # Top-Ké‡‡æ ·
    top_p: Optional[float] = None     # Top-P (nucleus)é‡‡æ ·
    repetition_penalty: float = 1.0   # é‡å¤æƒ©ç½š
    length_penalty: float = 1.0       # é•¿åº¦æƒ©ç½š
    no_repeat_ngram_size: int = 0     # é˜²æ­¢n-gramé‡å¤
    
    # æŸæœç´¢å‚æ•°
    num_beams: int = 1               # æŸæœç´¢å¤§å°
    early_stopping: bool = True      # æ—©åœ
    
    # ç‰¹æ®Štoken
    pad_token_id: Optional[int] = None
    eos_token_id: Optional[int] = None
    bos_token_id: Optional[int] = None
    
    # è¾“å‡ºæ§åˆ¶
    do_sample: bool = True           # æ˜¯å¦é‡‡æ ·
    num_return_sequences: int = 1    # è¿”å›åºåˆ—æ•°é‡
    
    # æ‰¹é‡ç”Ÿæˆ
    batch_size: int = 1             # æ‰¹é‡å¤§å°


class BaseGenerator(ABC):
    """ç”Ÿæˆå™¨åŸºç±»"""
    
    def __init__(self, model, tokenizer=None):
        self.model = model
        self.tokenizer = tokenizer
        
    @abstractmethod
    def generate(self, 
                 input_ids: torch.Tensor, 
                 config: GenerationConfig) -> torch.Tensor:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass
    
    def generate_text(self, 
                     prompt: str, 
                     config: GenerationConfig) -> List[str]:
        """ä»æ–‡æœ¬promptç”Ÿæˆ"""
        if self.tokenizer is None:
            raise ValueError("éœ€è¦æä¾›tokenizeræ¥å¤„ç†æ–‡æœ¬")
        
        # ç¼–ç è¾“å…¥
        input_ids = torch.tensor([self.tokenizer.encode(prompt)], dtype=torch.long)
        
        # ç”Ÿæˆ
        outputs = self.generate(input_ids, config)
        
        # è§£ç è¾“å‡º
        results = []
        for output in outputs:
            text = self.tokenizer.decode(output.tolist())
            results.append(text)
        
        return results


class GreedyGenerator(BaseGenerator):
    """è´ªå¿ƒè§£ç ç”Ÿæˆå™¨"""
    
    @torch.no_grad()
    def generate(self, 
                 input_ids: torch.Tensor, 
                 config: GenerationConfig) -> torch.Tensor:
        """è´ªå¿ƒè§£ç ç”Ÿæˆ"""
        self.model.eval()
        device = next(self.model.parameters()).device
        input_ids = input_ids.to(device)
        
        for _ in range(config.max_new_tokens):
            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
            if hasattr(self.model, 'config'):
                max_len = self.model.config.max_seq_len
                if input_ids.size(1) > max_len:
                    input_ids = input_ids[:, -max_len:]
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(input_ids)
            logits = outputs["logits"][:, -1, :]  # æœ€åä½ç½®çš„logits
            
            # åº”ç”¨é‡å¤æƒ©ç½š
            if config.repetition_penalty != 1.0:
                logits = self._apply_repetition_penalty(logits, input_ids, config.repetition_penalty)
            
            # è´ªå¿ƒé€‰æ‹©
            next_token = torch.argmax(logits, dim=-1, keepdim=True)
            
            # æ·»åŠ æ–°token
            input_ids = torch.cat([input_ids, next_token], dim=1)
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if config.eos_token_id and next_token.item() == config.eos_token_id:
                break
        
        return input_ids
    
    def _apply_repetition_penalty(self, logits: torch.Tensor, 
                                 input_ids: torch.Tensor, 
                                 penalty: float) -> torch.Tensor:
        """åº”ç”¨é‡å¤æƒ©ç½š"""
        if penalty == 1.0:
            return logits
        
        for batch_idx in range(logits.size(0)):
            for token_id in set(input_ids[batch_idx].tolist()):
                if logits[batch_idx, token_id] < 0:
                    logits[batch_idx, token_id] *= penalty
                else:
                    logits[batch_idx, token_id] /= penalty
        
        return logits


class SamplingGenerator(BaseGenerator):
    """éšæœºé‡‡æ ·ç”Ÿæˆå™¨ (æ”¯æŒTop-Kå’ŒTop-P)"""
    
    @torch.no_grad()
    def generate(self, 
                 input_ids: torch.Tensor, 
                 config: GenerationConfig) -> torch.Tensor:
        """éšæœºé‡‡æ ·ç”Ÿæˆ"""
        self.model.eval()
        device = next(self.model.parameters()).device
        input_ids = input_ids.to(device)
        
        for _ in range(config.max_new_tokens):
            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
            if hasattr(self.model, 'config'):
                max_len = self.model.config.max_seq_len
                if input_ids.size(1) > max_len:
                    input_ids = input_ids[:, -max_len:]
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(input_ids)
            logits = outputs["logits"][:, -1, :] / config.temperature
            
            # åº”ç”¨é‡å¤æƒ©ç½š
            if config.repetition_penalty != 1.0:
                logits = self._apply_repetition_penalty(logits, input_ids, config.repetition_penalty)
            
            # Top-Kè¿‡æ»¤
            if config.top_k is not None and config.top_k > 0:
                logits = self._top_k_filtering(logits, config.top_k)
            
            # Top-Pè¿‡æ»¤
            if config.top_p is not None and config.top_p < 1.0:
                logits = self._top_p_filtering(logits, config.top_p)
            
            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # æ·»åŠ æ–°token
            input_ids = torch.cat([input_ids, next_token], dim=1)
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if config.eos_token_id and next_token.item() == config.eos_token_id:
                break
        
        return input_ids
    
    def _apply_repetition_penalty(self, logits: torch.Tensor, 
                                 input_ids: torch.Tensor, 
                                 penalty: float) -> torch.Tensor:
        """åº”ç”¨é‡å¤æƒ©ç½š"""
        if penalty == 1.0:
            return logits
        
        for batch_idx in range(logits.size(0)):
            for token_id in set(input_ids[batch_idx].tolist()):
                if logits[batch_idx, token_id] < 0:
                    logits[batch_idx, token_id] *= penalty
                else:
                    logits[batch_idx, token_id] /= penalty
        
        return logits
    
    def _top_k_filtering(self, logits: torch.Tensor, k: int) -> torch.Tensor:
        """Top-Kè¿‡æ»¤"""
        if k <= 0:
            return logits
        
        top_k_logits, _ = torch.topk(logits, min(k, logits.size(-1)))
        indices_to_remove = logits < top_k_logits[..., [-1]]
        logits = logits.masked_fill(indices_to_remove, -torch.inf)
        
        return logits
    
    def _top_p_filtering(self, logits: torch.Tensor, p: float) -> torch.Tensor:
        """Top-P (nucleus)è¿‡æ»¤"""
        if p >= 1.0:
            return logits
        
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡pçš„tokens
        sorted_indices_to_remove = cumulative_probs > p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        # æ˜ å°„å›åŸå§‹ç´¢å¼•
        indices_to_remove = sorted_indices_to_remove.scatter(
            dim=-1, index=sorted_indices, src=sorted_indices_to_remove
        )
        logits = logits.masked_fill(indices_to_remove, -torch.inf)
        
        return logits


class BeamSearchGenerator(BaseGenerator):
    """æŸæœç´¢ç”Ÿæˆå™¨"""
    
    @torch.no_grad()
    def generate(self, 
                 input_ids: torch.Tensor, 
                 config: GenerationConfig) -> torch.Tensor:
        """æŸæœç´¢ç”Ÿæˆ"""
        if config.num_beams <= 1:
            # é€€åŒ–ä¸ºè´ªå¿ƒæœç´¢
            greedy_gen = GreedyGenerator(self.model, self.tokenizer)
            return greedy_gen.generate(input_ids, config)
        
        self.model.eval()
        device = next(self.model.parameters()).device
        input_ids = input_ids.to(device)
        
        batch_size, seq_len = input_ids.size()
        beam_size = config.num_beams
        
        # æ‰©å±•è¾“å…¥åˆ°beamç»´åº¦
        input_ids = input_ids.unsqueeze(1).repeat(1, beam_size, 1)
        input_ids = input_ids.view(batch_size * beam_size, seq_len)
        
        # åˆå§‹åŒ–beamçŠ¶æ€
        beam_scores = torch.zeros(batch_size, beam_size, device=device)
        beam_scores[:, 1:] = -1e9  # åªä¿ç•™ç¬¬ä¸€ä¸ªbeamæ´»è·ƒ
        beam_scores = beam_scores.view(-1)  # [batch_size * beam_size]
        
        # å®Œæˆçš„åºåˆ—
        done = [False] * batch_size
        
        for step in range(config.max_new_tokens):
            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
            if hasattr(self.model, 'config'):
                max_len = self.model.config.max_seq_len
                if input_ids.size(1) > max_len:
                    input_ids = input_ids[:, -max_len:]
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(input_ids)
            logits = outputs["logits"][:, -1, :]  # [batch_size * beam_size, vocab_size]
            
            vocab_size = logits.size(-1)
            
            # è®¡ç®—logæ¦‚ç‡
            log_probs = F.log_softmax(logits, dim=-1)
            
            # åŠ ä¸Šä¹‹å‰çš„åˆ†æ•°
            log_probs = log_probs + beam_scores.unsqueeze(-1)
            
            # é‡å¡‘ä¸º [batch_size, beam_size * vocab_size]
            log_probs = log_probs.view(batch_size, beam_size * vocab_size)
            
            # é€‰æ‹©top-k candidates
            top_log_probs, top_indices = torch.topk(log_probs, beam_size, dim=1)
            
            # è®¡ç®—beamå’Œtokenç´¢å¼•
            beam_indices = top_indices // vocab_size  # å“ªä¸ªbeam
            token_indices = top_indices % vocab_size   # å“ªä¸ªtoken
            
            # æ›´æ–°beam_scores
            beam_scores = top_log_probs.view(-1)
            
            # é‡æ–°æ’åˆ—input_ids
            beam_indices_expanded = beam_indices.view(-1) + torch.arange(
                0, batch_size * beam_size, beam_size, device=device
            ).unsqueeze(-1).expand(-1, beam_size).reshape(-1)
            
            input_ids = input_ids[beam_indices_expanded]
            
            # æ·»åŠ æ–°tokens
            new_tokens = token_indices.view(-1, 1)
            input_ids = torch.cat([input_ids, new_tokens], dim=1)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if config.eos_token_id:
                eos_mask = new_tokens.squeeze(-1) == config.eos_token_id
                if eos_mask.any() and config.early_stopping:
                    break
        
        # è¿”å›æœ€ä½³åºåˆ—
        best_beam_indices = beam_scores.view(batch_size, beam_size).argmax(dim=1)
        best_sequences = []
        
        for i in range(batch_size):
            best_idx = i * beam_size + best_beam_indices[i]
            best_sequences.append(input_ids[best_idx])
        
        return torch.stack(best_sequences)


class ContrastiveSearchGenerator(BaseGenerator):
    """å¯¹æ¯”æœç´¢ç”Ÿæˆå™¨"""
    
    @torch.no_grad()
    def generate(self, 
                 input_ids: torch.Tensor, 
                 config: GenerationConfig,
                 alpha: float = 0.6,
                 k: int = 4) -> torch.Tensor:
        """
        å¯¹æ¯”æœç´¢ç”Ÿæˆ
        
        Args:
            alpha: å¯¹æ¯”æƒé‡
            k: å€™é€‰æ•°é‡
        """
        self.model.eval()
        device = next(self.model.parameters()).device
        input_ids = input_ids.to(device)
        
        for _ in range(config.max_new_tokens):
            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
            if hasattr(self.model, 'config'):
                max_len = self.model.config.max_seq_len
                if input_ids.size(1) > max_len:
                    input_ids = input_ids[:, -max_len:]
            
            # å‰å‘ä¼ æ’­è·å–logitså’Œhidden states
            outputs = self.model(input_ids, output_hidden_states=True)
            logits = outputs["logits"][:, -1, :]
            
            # Top-kå€™é€‰
            top_k_logits, top_k_indices = torch.topk(logits, k)
            probs = F.softmax(top_k_logits, dim=-1)
            
            # è®¡ç®—å¯¹æ¯”åˆ†æ•° (è¿™é‡Œç®€åŒ–å®ç°)
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦è®¡ç®—tokenè¡¨ç¤ºä¹‹é—´çš„ç›¸ä¼¼æ€§
            contrastive_scores = torch.zeros_like(probs)
            
            # ç»“åˆæ¦‚ç‡å’Œå¯¹æ¯”åˆ†æ•°
            final_scores = alpha * probs + (1 - alpha) * contrastive_scores
            
            # é€‰æ‹©æœ€ä½³å€™é€‰
            best_idx = torch.argmax(final_scores, dim=-1)
            next_token = top_k_indices[torch.arange(input_ids.size(0)), best_idx].unsqueeze(-1)
            
            # æ·»åŠ æ–°token
            input_ids = torch.cat([input_ids, next_token], dim=1)
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if config.eos_token_id and next_token.item() == config.eos_token_id:
                break
        
        return input_ids


class BatchGenerator:
    """æ‰¹é‡ç”Ÿæˆå™¨ - æ”¯æŒåŒæ—¶ç”Ÿæˆå¤šä¸ªåºåˆ—"""
    
    def __init__(self, model, tokenizer=None):
        self.model = model
        self.tokenizer = tokenizer
    
    @torch.no_grad()
    def generate_batch(self, 
                      prompts: List[str], 
                      config: GenerationConfig) -> List[List[str]]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬"""
        if self.tokenizer is None:
            raise ValueError("éœ€è¦æä¾›tokenizeræ¥å¤„ç†æ–‡æœ¬")
        
        # ç¼–ç æ‰€æœ‰prompts
        input_ids_list = []
        max_len = 0
        
        for prompt in prompts:
            ids = self.tokenizer.encode(prompt)
            input_ids_list.append(ids)
            max_len = max(max_len, len(ids))
        
        # Paddingåˆ°ç›¸åŒé•¿åº¦
        padded_inputs = []
        attention_masks = []
        
        pad_id = config.pad_token_id or 0
        
        for ids in input_ids_list:
            padding_length = max_len - len(ids)
            padded_ids = ids + [pad_id] * padding_length
            mask = [1] * len(ids) + [0] * padding_length
            
            padded_inputs.append(padded_ids)
            attention_masks.append(mask)
        
        # è½¬æ¢ä¸ºtensor
        input_ids = torch.tensor(padded_inputs, dtype=torch.long)
        attention_mask = torch.tensor(attention_masks, dtype=torch.long)
        
        # é€‰æ‹©ç”Ÿæˆå™¨
        if config.num_beams > 1:
            generator = BeamSearchGenerator(self.model, self.tokenizer)
        elif config.do_sample:
            generator = SamplingGenerator(self.model, self.tokenizer)
        else:
            generator = GreedyGenerator(self.model, self.tokenizer)
        
        # æ‰¹é‡ç”Ÿæˆ
        results = []
        batch_size = config.batch_size
        
        for i in range(0, len(prompts), batch_size):
            batch_inputs = input_ids[i:i+batch_size]
            batch_outputs = generator.generate(batch_inputs, config)
            
            # è§£ç è¾“å‡º
            batch_results = []
            for output in batch_outputs:
                text = self.tokenizer.decode(output.tolist())
                batch_results.append([text])  # åŒ…è£…ä¸ºåˆ—è¡¨ä»¥ä¿æŒä¸€è‡´æ€§
            
            results.extend(batch_results)
        
        return results


# ä¾¿æ·å‡½æ•°
def create_generator(strategy: str, model, tokenizer=None):
    """åˆ›å»ºæŒ‡å®šç­–ç•¥çš„ç”Ÿæˆå™¨"""
    generators = {
        'greedy': GreedyGenerator,
        'sampling': SamplingGenerator,
        'beam_search': BeamSearchGenerator,
        'contrastive': ContrastiveSearchGenerator,
    }
    
    if strategy not in generators:
        raise ValueError(f"ä¸æ”¯æŒçš„ç”Ÿæˆç­–ç•¥: {strategy}")
    
    return generators[strategy](model, tokenizer)


def generate_text(model, 
                 prompt: str, 
                 tokenizer=None,
                 strategy: str = "sampling",
                 **kwargs) -> str:
    """ä¾¿æ·çš„æ–‡æœ¬ç”Ÿæˆå‡½æ•°"""
    
    # åˆ›å»ºé…ç½®
    config = GenerationConfig(**kwargs)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = create_generator(strategy, model, tokenizer)
    
    # ç”Ÿæˆæ–‡æœ¬
    results = generator.generate_text(prompt, config)
    
    return results[0] if results else ""


if __name__ == "__main__":
    # æµ‹è¯•ç”Ÿæˆå™¨
    print("ğŸ¯ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆå™¨")
    
    # è¿™é‡Œéœ€è¦å¯¼å…¥æ¨¡å‹è¿›è¡Œæµ‹è¯•
    # åœ¨å®é™…ä½¿ç”¨æ—¶å–æ¶ˆæ³¨é‡Šç›¸å…³ä»£ç 
    
    print("âœ… ç”Ÿæˆå™¨æµ‹è¯•å®Œæˆ!")