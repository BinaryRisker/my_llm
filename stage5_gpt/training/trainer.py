"""
GPTè®­ç»ƒå™¨å®ç°
=============

åŒ…å«ï¼š
- é¢„è®­ç»ƒå¾ªç¯
- å¾®è°ƒåŠŸèƒ½  
- è¯„ä¼°æŒ‡æ ‡
- æ£€æŸ¥ç‚¹ç®¡ç†
- æ€§èƒ½ç›‘æ§
"""

import os
import math
import time
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from typing import Dict, Optional, List, Tuple, Any
from dataclasses import dataclass, field
import json
import logging
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class GPTTrainingConfig:
    """GPTè®­ç»ƒé…ç½®"""
    
    # åŸºæœ¬é…ç½®
    output_dir: str = "./gpt_checkpoints"
    run_name: str = "gpt_mini_training"
    
    # è®­ç»ƒå‚æ•°
    num_epochs: int = 10
    batch_size: int = 8
    learning_rate: float = 3e-4
    weight_decay: float = 0.01
    beta1: float = 0.9
    beta2: float = 0.95
    
    # å­¦ä¹ ç‡è°ƒåº¦
    warmup_steps: int = 1000
    max_steps: Optional[int] = None
    
    # æ­£åˆ™åŒ–
    gradient_clip_val: float = 1.0
    dropout: float = 0.1
    
    # è¯„ä¼°
    eval_steps: int = 500
    save_steps: int = 1000
    logging_steps: int = 100
    
    # æ•°æ®å¤„ç†
    max_seq_len: int = 512
    num_workers: int = 4
    
    # è®¾å¤‡è®¾ç½®
    device: str = "auto"  # auto, cpu, cuda
    mixed_precision: bool = True
    
    # å…¶ä»–
    seed: int = 42
    resume_from_checkpoint: Optional[str] = None
    save_only_best: bool = True
    
    def __post_init__(self):
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        if self.device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"


class LanguageModelingDataset(Dataset):
    """è¯­è¨€å»ºæ¨¡æ•°æ®é›†"""
    
    def __init__(self, 
                 texts: List[str], 
                 tokenizer, 
                 max_length: int = 512,
                 stride: int = None):
        """
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            stride: æ»‘åŠ¨çª—å£æ­¥é•¿
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.stride = stride or max_length // 2
        
        # å¤„ç†æ–‡æœ¬æ•°æ®
        self.examples = self._process_texts(texts)
        
    def _process_texts(self, texts: List[str]) -> List[torch.Tensor]:
        """å¤„ç†æ–‡æœ¬æ•°æ®ä¸ºè®­ç»ƒæ ·æœ¬"""
        examples = []
        
        for text in tqdm(texts, desc="å¤„ç†æ–‡æœ¬æ•°æ®"):
            # åˆ†è¯
            tokens = self.tokenizer.encode(text)
            
            # æ»‘åŠ¨çª—å£åˆ‡åˆ†
            for i in range(0, len(tokens) - self.max_length + 1, self.stride):
                chunk = tokens[i:i + self.max_length]
                if len(chunk) == self.max_length:
                    examples.append(torch.tensor(chunk, dtype=torch.long))
        
        logger.info(f"ç”Ÿæˆ {len(examples)} ä¸ªè®­ç»ƒæ ·æœ¬")
        return examples
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        return self.examples[idx]


class GPTTrainer:
    """GPTè®­ç»ƒå™¨"""
    
    def __init__(self, 
                 model,
                 config: GPTTrainingConfig,
                 train_dataset: Dataset,
                 eval_dataset: Optional[Dataset] = None,
                 tokenizer = None):
        
        self.model = model
        self.config = config  
        self.tokenizer = tokenizer
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device(config.device)
        self.model.to(self.device)
        
        # æ•°æ®åŠ è½½å™¨
        self.train_dataloader = DataLoader(
            train_dataset,
            batch_size=config.batch_size,
            shuffle=True,
            num_workers=config.num_workers,
            pin_memory=True if config.device == "cuda" else False
        )
        
        self.eval_dataloader = None
        if eval_dataset:
            self.eval_dataloader = DataLoader(
                eval_dataset,
                batch_size=config.batch_size,
                shuffle=False,
                num_workers=config.num_workers,
                pin_memory=True if config.device == "cuda" else False
            )
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # æ··åˆç²¾åº¦
        self.scaler = torch.cuda.amp.GradScaler() if config.mixed_precision else None
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.current_epoch = 0
        self.best_eval_loss = float('inf')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config.output_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        self._save_config()
        
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        # åˆ†ç»„å‚æ•° (ä¸å¯¹LayerNormå’Œbiasåº”ç”¨æƒé‡è¡°å‡)
        decay_params = []
        nodecay_params = []
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                if 'bias' in name or 'ln' in name or 'layernorm' in name:
                    nodecay_params.append(param)
                else:
                    decay_params.append(param)
        
        optimizer_grouped_parameters = [
            {"params": decay_params, "weight_decay": self.config.weight_decay},
            {"params": nodecay_params, "weight_decay": 0.0}
        ]
        
        return AdamW(
            optimizer_grouped_parameters,
            lr=self.config.learning_rate,
            betas=(self.config.beta1, self.config.beta2)
        )
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        total_steps = self.config.max_steps
        if total_steps is None:
            total_steps = len(self.train_dataloader) * self.config.num_epochs
            
        return CosineAnnealingLR(self.optimizer, T_max=total_steps)
    
    def _save_config(self):
        """ä¿å­˜è®­ç»ƒé…ç½®"""
        config_path = os.path.join(self.config.output_dir, "training_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢ä¸ºå­—å…¸ (å¤„ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡)
            config_dict = {}
            for key, value in self.config.__dict__.items():
                if isinstance(value, (str, int, float, bool, list, dict)) or value is None:
                    config_dict[key] = value
                else:
                    config_dict[key] = str(value)
            
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info(f"ğŸš€ å¼€å§‹GPTè®­ç»ƒ")
        logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        logger.info(f"è®­ç»ƒé…ç½®: {self.config}")
        
        # æ¢å¤æ£€æŸ¥ç‚¹
        if self.config.resume_from_checkpoint:
            self._load_checkpoint(self.config.resume_from_checkpoint)
        
        # è®­ç»ƒå¾ªç¯
        self.model.train()
        total_loss = 0.0
        
        progress_bar = tqdm(range(self.config.num_epochs), desc="è®­ç»ƒè¿›åº¦")
        
        for epoch in range(self.current_epoch, self.config.num_epochs):
            self.current_epoch = epoch
            epoch_loss = self._train_epoch()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'epoch': epoch + 1,
                'loss': f'{epoch_loss:.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
            progress_bar.update(1)
            
            # è¯„ä¼°
            if self.eval_dataloader and (epoch + 1) % max(1, self.config.num_epochs // 5) == 0:
                eval_results = self.evaluate()
                logger.info(f"Epoch {epoch + 1} è¯„ä¼°ç»“æœ: {eval_results}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if eval_results['eval_loss'] < self.best_eval_loss:
                    self.best_eval_loss = eval_results['eval_loss']
                    self._save_checkpoint(is_best=True)
        
        progress_bar.close()
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_checkpoint(is_final=True)
    
    def _train_epoch(self) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        epoch_loss = 0.0
        num_batches = len(self.train_dataloader)
        
        for batch_idx, batch in enumerate(self.train_dataloader):
            loss = self._train_step(batch)
            epoch_loss += loss
            
            # æ—¥å¿—è®°å½•
            if self.global_step % self.config.logging_steps == 0:
                logger.info(
                    f"Step {self.global_step}, "
                    f"Loss: {loss:.4f}, "
                    f"LR: {self.optimizer.param_groups[0]['lr']:.2e}"
                )
            
            # è¯„ä¼°
            if (self.eval_dataloader and 
                self.global_step % self.config.eval_steps == 0 and 
                self.global_step > 0):
                
                eval_results = self.evaluate()
                logger.info(f"Step {self.global_step} è¯„ä¼°: {eval_results}")
                self.model.train()
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.global_step % self.config.save_steps == 0 and self.global_step > 0:
                self._save_checkpoint()
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
            if self.config.max_steps and self.global_step >= self.config.max_steps:
                break
        
        return epoch_loss / num_batches
    
    def _train_step(self, batch: torch.Tensor) -> float:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        input_ids = batch.to(self.device)
        
        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        if self.scaler:
            with torch.cuda.amp.autocast():
                outputs = self.model(input_ids, targets=input_ids)
                loss = outputs["loss"]
        else:
            outputs = self.model(input_ids, targets=input_ids)
            loss = outputs["loss"]
        
        # åå‘ä¼ æ’­
        if self.scaler:
            self.scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ª
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_val)
            
            # æ›´æ–°å‚æ•°
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_val)
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
        
        # æ¸…é›¶æ¢¯åº¦å’Œæ›´æ–°è°ƒåº¦å™¨
        self.optimizer.zero_grad()
        self.scheduler.step()
        
        self.global_step += 1
        
        return loss.item()
    
    @torch.no_grad()
    def evaluate(self) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        if not self.eval_dataloader:
            return {}
        
        self.model.eval()
        
        total_loss = 0.0
        total_tokens = 0
        
        for batch in tqdm(self.eval_dataloader, desc="è¯„ä¼°ä¸­"):
            input_ids = batch.to(self.device)
            
            outputs = self.model(input_ids, targets=input_ids)
            loss = outputs["loss"]
            
            total_loss += loss.item() * input_ids.numel()
            total_tokens += input_ids.numel()
        
        avg_loss = total_loss / total_tokens
        perplexity = math.exp(avg_loss)
        
        return {
            "eval_loss": avg_loss,
            "perplexity": perplexity,
        }
    
    def _save_checkpoint(self, is_best: bool = False, is_final: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'global_step': self.global_step,
            'current_epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_eval_loss': self.best_eval_loss,
            'config': self.config.__dict__,
        }
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # æ–‡ä»¶å
        if is_final:
            filename = "final_checkpoint.pt"
        elif is_best:
            filename = "best_checkpoint.pt"
        else:
            filename = f"checkpoint_step_{self.global_step}.pt"
        
        save_path = os.path.join(self.config.output_dir, filename)
        torch.save(checkpoint, save_path)
        
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")
    
    def _load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.global_step = checkpoint['global_step']
        self.current_epoch = checkpoint['current_epoch']
        self.best_eval_loss = checkpoint['best_eval_loss']
        
        if self.scaler and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {checkpoint_path}")
        logger.info(f"å…¨å±€æ­¥æ•°: {self.global_step}, å½“å‰epoch: {self.current_epoch}")


def compute_metrics(model, dataloader, device: str = "cpu") -> Dict[str, float]:
    """è®¡ç®—æ¨¡å‹æŒ‡æ ‡"""
    model.eval()
    
    total_loss = 0.0
    total_tokens = 0
    correct_predictions = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è®¡ç®—æŒ‡æ ‡"):
            input_ids = batch.to(device)
            
            outputs = model(input_ids, targets=input_ids)
            loss = outputs["loss"]
            logits = outputs["logits"]
            
            # ç´¯ç§¯æŸå¤±
            total_loss += loss.item() * input_ids.numel()
            total_tokens += input_ids.numel()
            
            # è®¡ç®—å‡†ç¡®ç‡ (ä¸‹ä¸€ä¸ªtokené¢„æµ‹)
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = input_ids[..., 1:].contiguous()
            
            predictions = shift_logits.argmax(dim=-1)
            correct = (predictions == shift_labels).sum().item()
            correct_predictions += correct
    
    avg_loss = total_loss / total_tokens
    accuracy = correct_predictions / total_tokens
    perplexity = math.exp(avg_loss)
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "perplexity": perplexity,
    }


if __name__ == "__main__":
    # æµ‹è¯•è®­ç»ƒå™¨
    print("ğŸ§ª æµ‹è¯•GPTè®­ç»ƒå™¨")
    
    # è¿™é‡Œéœ€è¦å¯¼å…¥æ¨¡å‹ï¼Œåœ¨å®é™…ä½¿ç”¨æ—¶å–æ¶ˆæ³¨é‡Š
    # from ..models.gpt_mini import GPTMini, GPTConfig
    
    # # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®
    # config = GPTConfig.gpt_mini()
    # model = GPTMini(config)
    
    # # åˆ›å»ºè™šæ‹Ÿæ•°æ®
    # texts = [f"è¿™æ˜¯ç¬¬ {i} ä¸ªè®­ç»ƒæ ·æœ¬ã€‚" * 10 for i in range(100)]
    
    # # ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨ (æ¼”ç¤ºç”¨)
    # class SimpleTokenizer:
    #     def __init__(self):
    #         self.char_to_idx = {}
    #         self.idx_to_char = {}
    
    #     def fit(self, texts):
    #         chars = set(''.join(texts))
    #         self.char_to_idx = {c: i for i, c in enumerate(sorted(chars))}
    #         self.idx_to_char = {i: c for c, i in self.char_to_idx.items()}
    
    #     def encode(self, text):
    #         return [self.char_to_idx.get(c, 0) for c in text]
    
    # tokenizer = SimpleTokenizer()
    # tokenizer.fit(texts)
    
    # # åˆ›å»ºæ•°æ®é›†
    # train_dataset = LanguageModelingDataset(texts[:80], tokenizer, max_length=64)
    # eval_dataset = LanguageModelingDataset(texts[80:], tokenizer, max_length=64)
    
    # # è®­ç»ƒé…ç½®
    # training_config = GPTTrainingConfig(
    #     num_epochs=2,
    #     batch_size=4,
    #     learning_rate=1e-4,
    #     output_dir="./test_gpt_output"
    # )
    
    # # åˆ›å»ºè®­ç»ƒå™¨
    # trainer = GPTTrainer(
    #     model=model,
    #     config=training_config,
    #     train_dataset=train_dataset,
    #     eval_dataset=eval_dataset,
    #     tokenizer=tokenizer
    # )
    
    # # å¼€å§‹è®­ç»ƒ
    # trainer.train()
    
    print("âœ… è®­ç»ƒå™¨æµ‹è¯•å®Œæˆ!")