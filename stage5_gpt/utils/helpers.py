"""
GPTå·¥å…·å‡½æ•°é›†åˆ
================

åŒ…å«ï¼š
- æ©ç ç›¸å…³åŠŸèƒ½
- æ–‡æœ¬é¢„å¤„ç†
- è¯„ä¼°æŒ‡æ ‡
- å¯è§†åŒ–å·¥å…·
"""

import torch
import torch.nn.functional as F
import numpy as np
import math
import re
from typing import List, Dict, Tuple, Optional, Union
from collections import defaultdict


# ==================== æ©ç ç›¸å…³ ====================

def create_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:
    """
    åˆ›å»ºå› æœæ©ç 
    
    Args:
        seq_len: åºåˆ—é•¿åº¦
        device: è®¾å¤‡
        
    Returns:
        å› æœæ©ç  [seq_len, seq_len]
    """
    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)
    return mask == 0  # ä¸‹ä¸‰è§’ä¸ºTrue


def apply_causal_mask(attention_scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
    """
    åº”ç”¨å› æœæ©ç åˆ°æ³¨æ„åŠ›åˆ†æ•°
    
    Args:
        attention_scores: æ³¨æ„åŠ›åˆ†æ•° [batch, heads, seq_len, seq_len]
        mask: æ©ç  [seq_len, seq_len]
        
    Returns:
        æ©ç åçš„æ³¨æ„åŠ›åˆ†æ•°
    """
    return attention_scores.masked_fill(~mask, -torch.inf)


def create_padding_mask(input_ids: torch.Tensor, pad_token_id: int) -> torch.Tensor:
    """
    åˆ›å»ºpaddingæ©ç 
    
    Args:
        input_ids: è¾“å…¥token IDs [batch, seq_len]
        pad_token_id: padding token ID
        
    Returns:
        paddingæ©ç  [batch, seq_len]
    """
    return input_ids != pad_token_id


# ==================== æŸå¤±å‡½æ•° ====================

def autoregressive_loss(logits: torch.Tensor, 
                       targets: torch.Tensor, 
                       ignore_index: int = -100) -> torch.Tensor:
    """
    è®¡ç®—è‡ªå›å½’è¯­è¨€æ¨¡å‹æŸå¤±
    
    Args:
        logits: æ¨¡å‹è¾“å‡º [batch_size, seq_len, vocab_size]
        targets: ç›®æ ‡tokens [batch_size, seq_len]
        ignore_index: å¿½ç•¥çš„tokenç´¢å¼•
        
    Returns:
        æŸå¤±å€¼
    """
    # ç§»ä½ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªtoken
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = targets[..., 1:].contiguous()
    
    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    loss = F.cross_entropy(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1),
        ignore_index=ignore_index
    )
    
    return loss


def compute_perplexity(loss: float) -> float:
    """ä»æŸå¤±è®¡ç®—å›°æƒ‘åº¦"""
    return math.exp(loss)


# ==================== æ–‡æœ¬é¢„å¤„ç† ====================

def prepare_training_data(texts: List[str], 
                         tokenizer,
                         max_length: int = 512,
                         stride: int = None,
                         min_length: int = 10) -> List[List[int]]:
    """
    å‡†å¤‡GPTé¢„è®­ç»ƒæ•°æ®
    
    Args:
        texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        stride: æ»‘åŠ¨çª—å£æ­¥é•¿
        min_length: æœ€å°åºåˆ—é•¿åº¦
        
    Returns:
        tokenåºåˆ—åˆ—è¡¨
    """
    if stride is None:
        stride = max_length // 2
    
    sequences = []
    
    for text in texts:
        # åŸºæœ¬æ¸…æ´—
        text = clean_text(text)
        if len(text.strip()) < min_length:
            continue
            
        # åˆ†è¯
        tokens = tokenizer.encode(text)
        
        # æ»‘åŠ¨çª—å£åˆ‡åˆ†
        for i in range(0, len(tokens) - max_length + 1, stride):
            chunk = tokens[i:i + max_length]
            if len(chunk) == max_length:
                sequences.append(chunk)
    
    return sequences


def clean_text(text: str) -> str:
    """åŸºæœ¬çš„æ–‡æœ¬æ¸…æ´—"""
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text.strip())
    
    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    return text


def tokenize_batch(texts: List[str], 
                  tokenizer,
                  max_length: int = 512,
                  padding: bool = True,
                  truncation: bool = True) -> Dict[str, torch.Tensor]:
    """
    æ‰¹é‡åˆ†è¯
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§é•¿åº¦
        padding: æ˜¯å¦padding
        truncation: æ˜¯å¦æˆªæ–­
        
    Returns:
        åŒ…å«input_idså’Œattention_maskçš„å­—å…¸
    """
    input_ids_list = []
    attention_masks = []
    
    for text in texts:
        # åˆ†è¯
        tokens = tokenizer.encode(text)
        
        # æˆªæ–­
        if truncation and len(tokens) > max_length:
            tokens = tokens[:max_length]
        
        # Padding
        if padding:
            pad_length = max_length - len(tokens)
            attention_mask = [1] * len(tokens) + [0] * pad_length
            tokens = tokens + [tokenizer.pad_token_id or 0] * pad_length
        else:
            attention_mask = [1] * len(tokens)
        
        input_ids_list.append(tokens)
        attention_masks.append(attention_mask)
    
    return {
        'input_ids': torch.tensor(input_ids_list, dtype=torch.long),
        'attention_mask': torch.tensor(attention_masks, dtype=torch.long)
    }


# ==================== è¯„ä¼°æŒ‡æ ‡ ====================

def compute_token_accuracy(logits: torch.Tensor, 
                          targets: torch.Tensor,
                          ignore_index: int = -100) -> float:
    """
    è®¡ç®—tokençº§åˆ«å‡†ç¡®ç‡
    
    Args:
        logits: æ¨¡å‹è¾“å‡º [batch_size, seq_len, vocab_size]
        targets: ç›®æ ‡tokens [batch_size, seq_len]
        ignore_index: å¿½ç•¥çš„ç´¢å¼•
        
    Returns:
        å‡†ç¡®ç‡
    """
    # ç§»ä½é¢„æµ‹
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = targets[..., 1:].contiguous()
    
    # é¢„æµ‹
    predictions = shift_logits.argmax(dim=-1)
    
    # è®¡ç®—å‡†ç¡®ç‡ (å¿½ç•¥æŒ‡å®šç´¢å¼•)
    mask = shift_labels != ignore_index
    correct = (predictions == shift_labels) & mask
    
    accuracy = correct.sum().float() / mask.sum().float()
    return accuracy.item()


def compute_sequence_accuracy(predictions: List[str], 
                            references: List[str]) -> float:
    """è®¡ç®—åºåˆ—çº§åˆ«å‡†ç¡®ç‡"""
    if len(predictions) != len(references):
        raise ValueError("é¢„æµ‹å’Œå‚è€ƒåºåˆ—æ•°é‡ä¸åŒ¹é…")
    
    correct = sum(pred == ref for pred, ref in zip(predictions, references))
    return correct / len(predictions)


def compute_bleu_score(predictions: List[str], 
                      references: List[str],
                      n_gram: int = 4) -> float:
    """
    è®¡ç®—BLEUåˆ†æ•° (ç®€åŒ–ç‰ˆæœ¬)
    
    Args:
        predictions: é¢„æµ‹åºåˆ—åˆ—è¡¨
        references: å‚è€ƒåºåˆ—åˆ—è¡¨  
        n_gram: n-gramçš„n
        
    Returns:
        BLEUåˆ†æ•°
    """
    def get_ngrams(tokens: List[str], n: int) -> Dict[tuple, int]:
        ngrams = defaultdict(int)
        for i in range(len(tokens) - n + 1):
            ngram = tuple(tokens[i:i+n])
            ngrams[ngram] += 1
        return dict(ngrams)
    
    def compute_precision(pred_tokens: List[str], ref_tokens: List[str], n: int) -> float:
        pred_ngrams = get_ngrams(pred_tokens, n)
        ref_ngrams = get_ngrams(ref_tokens, n)
        
        if not pred_ngrams:
            return 0.0
        
        matches = 0
        total = 0
        
        for ngram, count in pred_ngrams.items():
            matches += min(count, ref_ngrams.get(ngram, 0))
            total += count
        
        return matches / total if total > 0 else 0.0
    
    # è®¡ç®—å„é˜¶n-gramç²¾åº¦
    precisions = []
    
    for pred, ref in zip(predictions, references):
        pred_tokens = pred.split()
        ref_tokens = ref.split()
        
        ngram_precisions = []
        for n in range(1, n_gram + 1):
            precision = compute_precision(pred_tokens, ref_tokens, n)
            ngram_precisions.append(precision)
        
        precisions.append(ngram_precisions)
    
    # å¹³å‡ç²¾åº¦
    avg_precisions = [
        sum(p[i] for p in precisions) / len(precisions)
        for i in range(n_gram)
    ]
    
    # å‡ ä½•å¹³å‡
    if all(p > 0 for p in avg_precisions):
        bleu = math.exp(sum(math.log(p) for p in avg_precisions) / len(avg_precisions))
    else:
        bleu = 0.0
    
    return bleu


# ==================== ç”Ÿæˆè¯„ä¼° ====================

def evaluate_generation_diversity(texts: List[str]) -> Dict[str, float]:
    """
    è¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§
    
    Args:
        texts: ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        å¤šæ ·æ€§æŒ‡æ ‡å­—å…¸
    """
    # æ”¶é›†æ‰€æœ‰n-gram
    unigrams = set()
    bigrams = set()
    trigrams = set()
    
    total_tokens = 0
    
    for text in texts:
        tokens = text.split()
        total_tokens += len(tokens)
        
        # Unigrams
        unigrams.update(tokens)
        
        # Bigrams  
        for i in range(len(tokens) - 1):
            bigrams.add((tokens[i], tokens[i+1]))
        
        # Trigrams
        for i in range(len(tokens) - 2):
            trigrams.add((tokens[i], tokens[i+1], tokens[i+2]))
    
    return {
        'unique_unigrams': len(unigrams),
        'unique_bigrams': len(bigrams), 
        'unique_trigrams': len(trigrams),
        'total_tokens': total_tokens,
        'unigram_diversity': len(unigrams) / total_tokens if total_tokens > 0 else 0,
        'bigram_diversity': len(bigrams) / max(1, total_tokens - len(texts)),
        'trigram_diversity': len(trigrams) / max(1, total_tokens - 2 * len(texts))
    }


def evaluate_repetition(text: str, n: int = 3) -> float:
    """
    è¯„ä¼°æ–‡æœ¬ä¸­çš„n-gramé‡å¤ç‡
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        n: n-gramçš„n
        
    Returns:
        é‡å¤ç‡ (0-1)
    """
    tokens = text.split()
    
    if len(tokens) < n:
        return 0.0
    
    ngrams = []
    for i in range(len(tokens) - n + 1):
        ngram = tuple(tokens[i:i+n])
        ngrams.append(ngram)
    
    unique_ngrams = len(set(ngrams))
    total_ngrams = len(ngrams)
    
    repetition_rate = 1.0 - (unique_ngrams / total_ngrams)
    return repetition_rate


# ==================== æ¨¡å‹å·¥å…· ====================

def count_parameters(model) -> Dict[str, int]:
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return {
        'total': total_params,
        'trainable': trainable_params,
        'non_trainable': total_params - trainable_params
    }


def get_model_size_mb(model) -> float:
    """è·å–æ¨¡å‹å¤§å° (MB)"""
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    size_mb = (param_size + buffer_size) / 1024 / 1024
    return size_mb


def save_model_summary(model, save_path: str):
    """ä¿å­˜æ¨¡å‹æ‘˜è¦ä¿¡æ¯"""
    summary = {
        'model_class': model.__class__.__name__,
        'parameters': count_parameters(model),
        'size_mb': get_model_size_mb(model),
    }
    
    # å¦‚æœæœ‰configå±æ€§
    if hasattr(model, 'config'):
        summary['config'] = model.config.__dict__
    
    import json
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)


# ==================== åˆ†è¯å™¨ç›¸å…³ ====================

class SimpleTokenizer:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨ (ç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•)"""
    
    def __init__(self):
        self.char_to_idx = {}
        self.idx_to_char = {}
        self.vocab_size = 0
        
        # ç‰¹æ®Štoken
        self.pad_token = '<pad>'
        self.unk_token = '<unk>'
        self.eos_token = '<eos>'
        self.bos_token = '<bos>'
        
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.eos_token_id = 2
        self.bos_token_id = 3
    
    def fit(self, texts: List[str]):
        """ä»æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨"""
        chars = set()
        for text in texts:
            chars.update(text)
        
        # æ·»åŠ ç‰¹æ®Štoken
        vocab = [self.pad_token, self.unk_token, self.eos_token, self.bos_token]
        vocab.extend(sorted(chars))
        
        self.char_to_idx = {char: idx for idx, char in enumerate(vocab)}
        self.idx_to_char = {idx: char for idx, char in enumerate(vocab)}
        self.vocab_size = len(vocab)
    
    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:
        """ç¼–ç æ–‡æœ¬"""
        tokens = []
        
        if add_special_tokens:
            tokens.append(self.bos_token_id)
        
        for char in text:
            tokens.append(self.char_to_idx.get(char, self.unk_token_id))
        
        if add_special_tokens:
            tokens.append(self.eos_token_id)
        
        return tokens
    
    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """è§£ç tokenåºåˆ—"""
        chars = []
        special_ids = {self.pad_token_id, self.unk_token_id, self.eos_token_id, self.bos_token_id}
        
        for token_id in token_ids:
            if skip_special_tokens and token_id in special_ids:
                continue
            char = self.idx_to_char.get(token_id, self.unk_token)
            chars.append(char)
        
        return ''.join(chars)


if __name__ == "__main__":
    # æµ‹è¯•å·¥å…·å‡½æ•°
    print("ğŸ§ª æµ‹è¯•GPTå·¥å…·å‡½æ•°")
    
    # æµ‹è¯•å› æœæ©ç 
    seq_len = 4
    device = torch.device('cpu')
    mask = create_causal_mask(seq_len, device)
    print(f"å› æœæ©ç :\n{mask}")
    
    # æµ‹è¯•ç®€å•åˆ†è¯å™¨
    texts = ["Hello world!", "GPT is great!", "Deep learning rocks!"]
    tokenizer = SimpleTokenizer()
    tokenizer.fit(texts)
    
    print(f"\nåˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # ç¼–ç è§£ç æµ‹è¯•
    test_text = "Hello GPT!"
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)
    
    print(f"åŸæ–‡: {test_text}")
    print(f"ç¼–ç : {encoded}")
    print(f"è§£ç : {decoded}")
    
    # æµ‹è¯•å¤šæ ·æ€§è¯„ä¼°
    sample_texts = [
        "The cat sat on the mat",
        "A dog ran in the park", 
        "Birds fly in the sky"
    ]
    diversity = evaluate_generation_diversity(sample_texts)
    print(f"\næ–‡æœ¬å¤šæ ·æ€§: {diversity}")
    
    print("\nâœ… å·¥å…·å‡½æ•°æµ‹è¯•å®Œæˆ!")