"""
SuperGLUEè¯„ä¼°æµ‹è¯•è„šæœ¬

æµ‹è¯•SuperGLUEè¯„ä¼°æ¡†æ¶çš„åŸºæœ¬åŠŸèƒ½
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from evaluation.superglue import SuperGLUEEvaluator, SuperGLUEMetrics, list_all_tasks
from evaluation.superglue.tasks import get_task, convert_examples_to_text2text, extract_predictions
from evaluation.superglue.data_loader import SuperGLUEDataLoader


def test_task_conversion():
    """æµ‹è¯•ä»»åŠ¡æ•°æ®è½¬æ¢"""
    print("æµ‹è¯•ä»»åŠ¡æ•°æ®è½¬æ¢...")
    
    # æµ‹è¯•æ¯ä¸ªä»»åŠ¡çš„è½¬æ¢
    tasks_to_test = ['boolq', 'cb', 'copa', 'rte', 'wic']
    
    for task_name in tasks_to_test:
        print(f"\næµ‹è¯• {task_name} ä»»åŠ¡:")
        
        # è·å–ä»»åŠ¡å¤„ç†å™¨
        task = get_task(task_name)
        print(f"  ä»»åŠ¡ç±»å‹: {task.task_type}")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        data_loader = SuperGLUEDataLoader()
        mock_data = data_loader._generate_mock_data(task_name, 'validation')
        print(f"  ç”Ÿæˆäº† {len(mock_data)} ä¸ªæ¨¡æ‹Ÿæ ·æœ¬")
        
        if mock_data:
            # è½¬æ¢ç¬¬ä¸€ä¸ªæ ·æœ¬
            example = mock_data[0]
            converted = task.convert_to_text2text(example)
            
            print(f"  è¾“å…¥æ–‡æœ¬: {converted['input_text'][:100]}...")
            print(f"  ç›®æ ‡æ–‡æœ¬: {converted['target_text']}")
            
            # æµ‹è¯•ç­”æ¡ˆæå–
            prediction = task.extract_answer(converted['target_text'])
            print(f"  æå–çš„ç­”æ¡ˆ: {prediction}")


def test_metrics_computation():
    """æµ‹è¯•æŒ‡æ ‡è®¡ç®—"""
    print("\næµ‹è¯•æŒ‡æ ‡è®¡ç®—...")
    
    # æµ‹è¯•å‡†ç¡®ç‡è®¡ç®—
    predictions = [0, 1, 1, 0, 1]
    references = [0, 1, 0, 0, 1]
    
    accuracy = SuperGLUEMetrics.compute_task_metrics('boolq', predictions, references)
    print(f"BoolQå‡†ç¡®ç‡: {accuracy}")
    
    # æµ‹è¯•F1åˆ†æ•°è®¡ç®—  
    cb_predictions = [0, 1, 2, 1, 0]
    cb_references = [0, 1, 2, 2, 0]
    
    f1_macro = SuperGLUEMetrics.compute_task_metrics('cb', cb_predictions, cb_references)
    print(f"CB F1åˆ†æ•°: {f1_macro}")
    
    # æµ‹è¯•å­—ç¬¦ä¸²åŒ¹é…
    record_predictions = ['answer1', 'answer2', 'answer3']
    record_references = ['answer1', 'different', 'answer3']
    
    em_f1 = SuperGLUEMetrics.compute_task_metrics('record', record_predictions, record_references)
    print(f"ReCoRD EM/F1: {em_f1}")


def test_data_loader():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    print("\næµ‹è¯•æ•°æ®åŠ è½½å™¨...")
    
    data_loader = SuperGLUEDataLoader()
    
    # æµ‹è¯•åŠ è½½æ¨¡æ‹Ÿæ•°æ®
    for task_name in ['boolq', 'cb', 'copa'][:2]:  # åªæµ‹è¯•å‰2ä¸ªä»»åŠ¡
        print(f"\nåŠ è½½ {task_name} æ•°æ®:")
        
        data = data_loader.load_task_data(
            task_name=task_name,
            split='validation',
            data_source='mock',
            max_samples=5
        )
        
        print(f"  åŠ è½½äº† {len(data)} ä¸ªæ ·æœ¬")
        
        # éªŒè¯æ•°æ®
        is_valid, errors = data_loader.validate_data(task_name, data)
        print(f"  æ•°æ®éªŒè¯: {'é€šè¿‡' if is_valid else 'å¤±è´¥'}")
        if errors:
            print(f"  é”™è¯¯: {errors}")


def test_evaluator():
    """æµ‹è¯•è¯„ä¼°å™¨"""
    print("\næµ‹è¯•SuperGLUEè¯„ä¼°å™¨...")
    
    # åˆ›å»ºè¯„ä¼°å™¨ï¼ˆä¸ä½¿ç”¨çœŸå®æ¨¡å‹ï¼‰
    evaluator = SuperGLUEEvaluator(
        model=None,  # ä½¿ç”¨æ¨¡æ‹Ÿé¢„æµ‹
        tokenizer=None,
        device='cpu',
        max_length=512,
        batch_size=4
    )
    
    print(f"æ”¯æŒçš„ä»»åŠ¡: {evaluator.list_supported_tasks()}")
    
    # æµ‹è¯•å•ä¸ªä»»åŠ¡è¯„ä¼°
    print("\næµ‹è¯•å•ä¸ªä»»åŠ¡è¯„ä¼°:")
    result = evaluator.evaluate_task('boolq', split='validation', save_predictions=False)
    print(f"BoolQè¯„ä¼°ç»“æœ: {result}")
    
    # æµ‹è¯•å¤šä»»åŠ¡è¯„ä¼°ï¼ˆåªæµ‹è¯•å‡ ä¸ªä»»åŠ¡ä»¥èŠ‚çœæ—¶é—´ï¼‰
    test_tasks = ['boolq', 'cb', 'copa']
    print(f"\næµ‹è¯•å¤šä»»åŠ¡è¯„ä¼°: {test_tasks}")
    
    summary = evaluator.evaluate_all(
        tasks=test_tasks,
        split='validation', 
        save_predictions=False
    )
    
    print(f"\næ€»ä½“è¯„ä¼°ç»“æœ:")
    print(f"  æ€»ä½“åˆ†æ•°: {summary['overall_score']:.4f}")
    print(f"  å®Œæˆä»»åŠ¡æ•°: {summary['num_tasks_completed']}")
    print(f"  å¤±è´¥ä»»åŠ¡æ•°: {summary['num_tasks_failed']}")


def test_text2text_conversion():
    """æµ‹è¯•Text-to-Textè½¬æ¢"""
    print("\næµ‹è¯•æ‰¹é‡Text-to-Textè½¬æ¢...")
    
    # ç”Ÿæˆä¸€äº›æµ‹è¯•æ•°æ®
    test_data = [
        {
            'question': 'Is Python a programming language?',
            'passage': 'Python is a high-level programming language known for its simplicity.',
            'label': 1
        },
        {
            'question': 'Is the sky green?', 
            'passage': 'The sky appears blue during the day due to light scattering.',
            'label': 0
        }
    ]
    
    # è½¬æ¢ä¸ºText-to-Textæ ¼å¼
    converted = convert_examples_to_text2text('boolq', test_data)
    
    print(f"è½¬æ¢äº† {len(converted)} ä¸ªæ ·æœ¬:")
    for i, item in enumerate(converted):
        print(f"  æ ·æœ¬ {i}:")
        print(f"    è¾“å…¥: {item['input_text'][:80]}...")
        print(f"    ç›®æ ‡: {item['target_text']}")
    
    # æµ‹è¯•ç­”æ¡ˆæå–
    generated_texts = ['True', 'False', 'True']
    predictions = extract_predictions('boolq', generated_texts)
    
    print(f"\nç­”æ¡ˆæå–æµ‹è¯•:")
    for i, (text, pred) in enumerate(zip(generated_texts, predictions)):
        print(f"  ç”Ÿæˆæ–‡æœ¬: '{text}' -> é¢„æµ‹: {pred}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹SuperGLUEè¯„ä¼°æ¡†æ¶æµ‹è¯•")
    print("=" * 60)
    
    try:
        test_task_conversion()
        test_metrics_computation()
        test_data_loader()
        test_text2text_conversion()
        test_evaluator()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰SuperGLUEæµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    main()