"""
T5æ¨¡å‹æµ‹è¯•è„šæœ¬

æµ‹è¯•T5æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡å‹åˆå§‹åŒ–
- å‰å‘ä¼ æ’­
- ç¼–ç å™¨-è§£ç å™¨æ¶æ„
- ç°ä»£æŠ€æœ¯é›†æˆ
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.stage7_t5.t5_config import T5Config
from models.stage7_t5.t5_model import T5Model
from models.modern_techniques import RoPE, RMSNorm, SwiGLU, FlashAttention, ALiBi

def test_t5_config():
    """æµ‹è¯•T5é…ç½®"""
    print("æµ‹è¯•T5é…ç½®...")
    
    # æµ‹è¯•é»˜è®¤é…ç½®
    config = T5Config()
    print(f"é»˜è®¤é…ç½®: vocab_size={config.vocab_size}, d_model={config.d_model}")
    
    # æµ‹è¯•é¢„å®šä¹‰é…ç½®
    small_config = T5Config.get_config('small')
    print(f"Smallé…ç½®: vocab_size={small_config.vocab_size}, d_model={small_config.d_model}")
    
    base_config = T5Config.get_config('base')
    print(f"Baseé…ç½®: vocab_size={base_config.vocab_size}, d_model={base_config.d_model}")
    
    print("T5é…ç½®æµ‹è¯•é€šè¿‡ âœ“")


def test_modern_techniques():
    """æµ‹è¯•ç°ä»£æŠ€æœ¯ç»„ä»¶"""
    print("\næµ‹è¯•ç°ä»£æŠ€æœ¯ç»„ä»¶...")
    
    batch_size, seq_len, embed_dim, num_heads = 2, 32, 512, 8
    head_dim = embed_dim // num_heads
    
    # æµ‹è¯•RoPE
    print("æµ‹è¯•RoPE...")
    rope = RoPE(dim=head_dim, max_seq_length=seq_len)
    q = torch.randn(batch_size, seq_len, num_heads, head_dim)
    k = torch.randn(batch_size, seq_len, num_heads, head_dim)
    q_rot, k_rot = rope(q, k)
    assert q_rot.shape == q.shape
    assert k_rot.shape == k.shape
    print("RoPEæµ‹è¯•é€šè¿‡ âœ“")
    
    # æµ‹è¯•RMSNorm
    print("æµ‹è¯•RMSNorm...")
    rmsnorm = RMSNorm(embed_dim)
    x = torch.randn(batch_size, seq_len, embed_dim)
    x_norm = rmsnorm(x)
    assert x_norm.shape == x.shape
    print("RMSNormæµ‹è¯•é€šè¿‡ âœ“")
    
    # æµ‹è¯•SwiGLU
    print("æµ‹è¯•SwiGLU...")
    swiglu = SwiGLU(embed_dim * 2)  # è¾“å…¥ç»´åº¦æ˜¯è¾“å‡ºçš„2å€
    x_swiglu = torch.randn(batch_size, seq_len, embed_dim * 2)
    x_out = swiglu(x_swiglu)
    assert x_out.shape == (batch_size, seq_len, embed_dim)
    print("SwiGLUæµ‹è¯•é€šè¿‡ âœ“")
    
    # æµ‹è¯•FlashAttention
    print("æµ‹è¯•FlashAttention...")
    flash_attn = FlashAttention(embed_dim, num_heads)
    attn_out, _ = flash_attn(x, x, x)
    assert attn_out.shape == x.shape
    print("FlashAttentionæµ‹è¯•é€šè¿‡ âœ“")
    
    # æµ‹è¯•ALiBi
    print("æµ‹è¯•ALiBi...")
    alibi = ALiBi(num_heads)
    bias = alibi(seq_len, device=x.device, dtype=x.dtype)
    assert bias.shape == (1, num_heads, seq_len, seq_len)
    print("ALiBiæµ‹è¯•é€šè¿‡ âœ“")
    
    print("æ‰€æœ‰ç°ä»£æŠ€æœ¯ç»„ä»¶æµ‹è¯•é€šè¿‡ âœ“")


def test_t5_model_basic():
    """æµ‹è¯•T5æ¨¡å‹åŸºæœ¬åŠŸèƒ½"""
    print("\næµ‹è¯•T5æ¨¡å‹åŸºæœ¬åŠŸèƒ½...")
    
    # ä½¿ç”¨å°é…ç½®è¿›è¡Œæµ‹è¯•
    config = T5Config.get_config('small')
    config.d_model = 256  # å‡å°æ¨¡å‹å°ºå¯¸ç”¨äºæµ‹è¯•
    config.d_ff = 512
    config.num_layers = 2
    config.num_heads = 4
    config.vocab_size = 1000
    
    model = T5Model(config)
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•è¾“å…¥
    batch_size = 2
    src_len = 16
    tgt_len = 12
    
    input_ids = torch.randint(0, config.vocab_size, (batch_size, src_len))
    decoder_input_ids = torch.randint(0, config.vocab_size, (batch_size, tgt_len))
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    print(f"è§£ç å™¨è¾“å…¥å½¢çŠ¶: {decoder_input_ids.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            decoder_input_ids=decoder_input_ids
        )
    
    print(f"è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")
    expected_shape = (batch_size, tgt_len, config.vocab_size)
    assert outputs.logits.shape == expected_shape, f"æœŸæœ›å½¢çŠ¶ {expected_shape}, å¾—åˆ° {outputs.logits.shape}"
    
    print("T5æ¨¡å‹åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡ âœ“")


def test_t5_encoder_decoder():
    """æµ‹è¯•T5ç¼–ç å™¨å’Œè§£ç å™¨åˆ†ç¦»åŠŸèƒ½"""
    print("\næµ‹è¯•T5ç¼–ç å™¨-è§£ç å™¨åˆ†ç¦»åŠŸèƒ½...")
    
    config = T5Config.get_config('small')
    config.d_model = 128
    config.d_ff = 256
    config.num_layers = 2
    config.num_heads = 4
    config.vocab_size = 500
    
    model = T5Model(config)
    
    batch_size = 2
    src_len = 10
    tgt_len = 8
    
    input_ids = torch.randint(0, config.vocab_size, (batch_size, src_len))
    decoder_input_ids = torch.randint(0, config.vocab_size, (batch_size, tgt_len))
    
    with torch.no_grad():
        # æµ‹è¯•åªç¼–ç å™¨
        encoder_outputs = model.encoder(input_ids)
        print(f"ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {encoder_outputs.last_hidden_state.shape}")
        expected_enc_shape = (batch_size, src_len, config.d_model)
        assert encoder_outputs.last_hidden_state.shape == expected_enc_shape
        
        # æµ‹è¯•è§£ç å™¨ï¼ˆä½¿ç”¨ç¼–ç å™¨è¾“å‡ºï¼‰
        decoder_outputs = model.decoder(
            input_ids=decoder_input_ids,
            encoder_hidden_states=encoder_outputs.last_hidden_state
        )
        print(f"è§£ç å™¨è¾“å‡ºå½¢çŠ¶: {decoder_outputs.last_hidden_state.shape}")
        expected_dec_shape = (batch_size, tgt_len, config.d_model)
        assert decoder_outputs.last_hidden_state.shape == expected_dec_shape
        
        # æµ‹è¯•è¯­è¨€æ¨¡å‹å¤´
        lm_logits = model.lm_head(decoder_outputs.last_hidden_state)
        print(f"è¯­è¨€æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {lm_logits.shape}")
        expected_lm_shape = (batch_size, tgt_len, config.vocab_size)
        assert lm_logits.shape == expected_lm_shape
    
    print("T5ç¼–ç å™¨-è§£ç å™¨åˆ†ç¦»åŠŸèƒ½æµ‹è¯•é€šè¿‡ âœ“")


def test_t5_generation():
    """æµ‹è¯•T5ç”ŸæˆåŠŸèƒ½"""
    print("\næµ‹è¯•T5ç”ŸæˆåŠŸèƒ½...")
    
    config = T5Config.get_config('small')
    config.d_model = 128
    config.d_ff = 256
    config.num_layers = 1
    config.num_heads = 4
    config.vocab_size = 100
    
    model = T5Model(config)
    model.eval()
    
    batch_size = 1
    src_len = 8
    max_length = 10
    
    input_ids = torch.randint(1, config.vocab_size - 1, (batch_size, src_len))
    
    with torch.no_grad():
        # ç¼–ç è¾“å…¥
        encoder_outputs = model.encoder(input_ids)
        
        # è´ªå©ªè§£ç ç”Ÿæˆ
        decoder_input_ids = torch.zeros((batch_size, 1), dtype=torch.long)  # å¼€å§‹æ ‡è®°
        generated_ids = [decoder_input_ids]
        
        for _ in range(max_length - 1):
            current_input = torch.cat(generated_ids, dim=1)
            
            outputs = model(
                input_ids=input_ids,
                decoder_input_ids=current_input
            )
            
            next_token_logits = outputs.logits[:, -1, :]
            next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            
            generated_ids.append(next_token_id)
            
            # ç®€å•çš„åœæ­¢æ¡ä»¶
            if next_token_id.item() == 0:  # å‡è®¾0æ˜¯ç»“æŸæ ‡è®°
                break
        
        generated_sequence = torch.cat(generated_ids, dim=1)
        print(f"ç”Ÿæˆçš„åºåˆ—å½¢çŠ¶: {generated_sequence.shape}")
        print(f"ç”Ÿæˆçš„åºåˆ—: {generated_sequence.tolist()}")
    
    print("T5ç”ŸæˆåŠŸèƒ½æµ‹è¯•é€šè¿‡ âœ“")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹T5æ¨¡å‹æµ‹è¯•...")
    print("=" * 50)
    
    try:
        test_t5_config()
        test_modern_techniques()
        test_t5_model_basic()
        test_t5_encoder_decoder()
        test_t5_generation()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰T5æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    main()