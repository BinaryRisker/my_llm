"""
é˜¶æ®µ4: Transformeræ¨¡å‹è®­ç»ƒè„šæœ¬
==============================

ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡çš„Transformeræ¨¡å‹è®­ç»ƒï¼ŒåŒ…å«ï¼š
- è‹±æ³•ç¿»è¯‘æ•°æ®å¤„ç†
- Transformeræ¨¡å‹è®­ç»ƒ
- BLEUè¯„ä¼°
- ä¸LSTM Seq2Seqå¯¹æ¯”
"""

import os
import json
import time
import math
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
from typing import List, Tuple, Dict, Optional
import random
from tqdm import tqdm
import numpy as np
from collections import Counter

# å¯¼å…¥æ¨¡å‹
from models.transformer import Transformer, TransformerConfig, create_transformer_small


class TranslationDataset(Dataset):
    """æœºå™¨ç¿»è¯‘æ•°æ®é›†"""
    
    def __init__(self, 
                 src_sentences: List[str],
                 tgt_sentences: List[str], 
                 src_vocab: Dict[str, int],
                 tgt_vocab: Dict[str, int],
                 max_length: int = 100):
        
        self.src_sentences = src_sentences
        self.tgt_sentences = tgt_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.max_length = max_length
        
        # ç‰¹æ®Štoken
        self.src_pad_idx = src_vocab.get('<pad>', 0)
        self.src_unk_idx = src_vocab.get('<unk>', 1)
        self.src_sos_idx = src_vocab.get('<sos>', 2)
        self.src_eos_idx = src_vocab.get('<eos>', 3)
        
        self.tgt_pad_idx = tgt_vocab.get('<pad>', 0)
        self.tgt_unk_idx = tgt_vocab.get('<unk>', 1)
        self.tgt_sos_idx = tgt_vocab.get('<sos>', 2)
        self.tgt_eos_idx = tgt_vocab.get('<eos>', 3)
        
    def __len__(self):
        return len(self.src_sentences)
    
    def __getitem__(self, idx):
        src_tokens = self._tokenize_and_encode(
            self.src_sentences[idx], 
            self.src_vocab, 
            self.src_sos_idx, 
            self.src_eos_idx,
            self.src_unk_idx
        )
        
        tgt_tokens = self._tokenize_and_encode(
            self.tgt_sentences[idx], 
            self.tgt_vocab, 
            self.tgt_sos_idx, 
            self.tgt_eos_idx,
            self.tgt_unk_idx
        )
        
        return {
            'src': torch.tensor(src_tokens, dtype=torch.long),
            'tgt': torch.tensor(tgt_tokens, dtype=torch.long)
        }
    
    def _tokenize_and_encode(self, sentence: str, vocab: Dict[str, int], 
                            sos_idx: int, eos_idx: int, unk_idx: int) -> List[int]:
        """åˆ†è¯å¹¶ç¼–ç """
        # ç®€å•åˆ†è¯ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨æ›´å¤æ‚çš„åˆ†è¯å™¨ï¼‰
        tokens = sentence.lower().split()[:self.max_length-2]
        
        # æ·»åŠ ç‰¹æ®Štokenå¹¶ç¼–ç 
        encoded = [sos_idx]
        for token in tokens:
            encoded.append(vocab.get(token, unk_idx))
        encoded.append(eos_idx)
        
        return encoded


def build_vocab(sentences: List[str], min_freq: int = 2) -> Dict[str, int]:
    """æ„å»ºè¯æ±‡è¡¨"""
    # ç»Ÿè®¡è¯é¢‘
    counter = Counter()
    for sentence in sentences:
        tokens = sentence.lower().split()
        counter.update(tokens)
    
    # æ„å»ºè¯æ±‡è¡¨
    vocab = {
        '<pad>': 0,
        '<unk>': 1, 
        '<sos>': 2,
        '<eos>': 3
    }
    
    for token, freq in counter.items():
        if freq >= min_freq:
            vocab[token] = len(vocab)
    
    return vocab


def create_sample_data(num_samples: int = 1000) -> Tuple[List[str], List[str]]:
    """åˆ›å»ºç¤ºä¾‹è‹±æ³•ç¿»è¯‘æ•°æ®"""
    
    # ç®€å•çš„è‹±æ³•å¯¹ç…§è¯å…¸
    en_fr_dict = {
        'hello': 'bonjour',
        'world': 'monde', 
        'good': 'bon',
        'morning': 'matin',
        'evening': 'soir',
        'thank': 'merci',
        'you': 'vous',
        'please': 'sil vous plait',
        'yes': 'oui',
        'no': 'non',
        'cat': 'chat',
        'dog': 'chien',
        'house': 'maison',
        'car': 'voiture',
        'book': 'livre',
        'water': 'eau',
        'food': 'nourriture',
        'love': 'amour',
        'life': 'vie',
        'time': 'temps',
        'day': 'jour',
        'night': 'nuit',
        'sun': 'soleil',
        'moon': 'lune',
        'red': 'rouge',
        'blue': 'bleu',
        'green': 'vert',
        'big': 'grand',
        'small': 'petit',
        'beautiful': 'beau',
        'happy': 'heureux',
        'sad': 'triste'
    }
    
    # æ¨¡å¼
    patterns = [
        ("hello world", "bonjour monde"),
        ("good morning", "bon matin"),
        ("good evening", "bon soir"), 
        ("thank you", "merci vous"),
        ("the cat is beautiful", "le chat est beau"),
        ("the dog is big", "le chien est grand"),
        ("the house is small", "la maison est petit"),
        ("i love you", "je vous aime"),
        ("life is beautiful", "la vie est beau"),
        ("the sun is red", "le soleil est rouge"),
        ("the moon is blue", "la lune est bleu"),
        ("water is good", "l'eau est bon")
    ]
    
    en_sentences = []
    fr_sentences = []
    
    # ç”ŸæˆåŸºç¡€æ¨¡å¼æ•°æ®
    for _ in range(num_samples // 2):
        en_pattern, fr_pattern = random.choice(patterns)
        en_sentences.append(en_pattern)
        fr_sentences.append(fr_pattern)
    
    # ç”Ÿæˆéšæœºç»„åˆæ•°æ®
    words = list(en_fr_dict.keys())
    for _ in range(num_samples - len(en_sentences)):
        # éšæœºé€‰æ‹©1-4ä¸ªè¯
        num_words = random.randint(1, 4)
        en_words = random.sample(words, num_words)
        fr_words = [en_fr_dict[w] for w in en_words]
        
        en_sentences.append(' '.join(en_words))
        fr_sentences.append(' '.join(fr_words))
    
    return en_sentences, fr_sentences


def collate_fn(batch):
    """æ‰¹é‡æ•°æ®å¤„ç†å‡½æ•°"""
    src_batch = [item['src'] for item in batch]
    tgt_batch = [item['tgt'] for item in batch]
    
    # Padding
    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)
    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)
    
    return {
        'src': src_batch,
        'tgt': tgt_batch
    }


class LabelSmoothingLoss(nn.Module):
    """æ ‡ç­¾å¹³æ»‘æŸå¤±"""
    
    def __init__(self, size: int, padding_idx: int, smoothing: float = 0.1):
        super().__init__()
        self.criterion = nn.KLDivLoss(reduction='sum')
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        
    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        
        return self.criterion(x, true_dist)


class TransformerTrainer:
    """Transformerè®­ç»ƒå™¨"""
    
    def __init__(self, 
                 model: nn.Module,
                 train_loader: DataLoader,
                 val_loader: DataLoader,
                 src_vocab: Dict[str, int],
                 tgt_vocab: Dict[str, int],
                 device: torch.device,
                 learning_rate: float = 1e-4,
                 label_smoothing: float = 0.1):
        
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.device = device
        
        # åå‘è¯å…¸
        self.tgt_idx2word = {idx: word for word, idx in tgt_vocab.items()}
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            model.parameters(), 
            lr=learning_rate, 
            betas=(0.9, 0.98), 
            eps=1e-9
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer, 
            step_size=10, 
            gamma=0.95
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = LabelSmoothingLoss(
            size=len(tgt_vocab),
            padding_idx=tgt_vocab['<pad>'],
            smoothing=label_smoothing
        )
        
        # è®­ç»ƒè®°å½•
        self.train_losses = []
        self.val_losses = []
        self.bleu_scores = []
        
    def train_epoch(self) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = len(self.train_loader)
        
        progress_bar = tqdm(self.train_loader, desc="è®­ç»ƒä¸­")
        
        for batch in progress_bar:
            src = batch['src'].to(self.device)  # [batch, src_len]
            tgt = batch['tgt'].to(self.device)  # [batch, tgt_len]
            
            # å‡†å¤‡decoderè¾“å…¥å’Œæ ‡ç­¾
            tgt_input = tgt[:, :-1]  # å»æ‰æœ€åä¸€ä¸ªtokenä½œä¸ºè¾“å…¥
            tgt_output = tgt[:, 1:]  # å»æ‰ç¬¬ä¸€ä¸ªtokenä½œä¸ºæ ‡ç­¾
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            logits = self.model(src, tgt_input)  # [batch, tgt_len-1, vocab_size]
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(
                logits.contiguous().view(-1, logits.size(-1)),
                tgt_output.contiguous().view(-1)
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{total_loss / (progress_bar.n + 1):.4f}'
            })
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def validate(self) -> Tuple[float, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        num_batches = len(self.val_loader)
        
        translations = []
        references = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="éªŒè¯ä¸­"):
                src = batch['src'].to(self.device)
                tgt = batch['tgt'].to(self.device)
                
                # è®¡ç®—æŸå¤±
                tgt_input = tgt[:, :-1]
                tgt_output = tgt[:, 1:]
                
                logits = self.model(src, tgt_input)
                loss = self.criterion(
                    logits.contiguous().view(-1, logits.size(-1)),
                    tgt_output.contiguous().view(-1)
                )
                
                total_loss += loss.item()
                
                # ç”Ÿæˆç¿»è¯‘ç”¨äºBLEUè®¡ç®—
                for i in range(min(5, src.size(0))):  # åªè®¡ç®—å‰5ä¸ªæ ·æœ¬çš„BLEU
                    src_seq = src[i:i+1]
                    translated = self.translate(src_seq)
                    reference = self._decode_sequence(tgt[i])
                    
                    translations.append(translated)
                    references.append(reference)
        
        avg_loss = total_loss / num_batches
        bleu_score = self.compute_bleu(references, translations)
        
        return avg_loss, bleu_score
    
    def translate(self, src: torch.Tensor, max_length: int = 50) -> str:
        """ç¿»è¯‘å•ä¸ªåºåˆ—"""
        self.model.eval()
        
        with torch.no_grad():
            batch_size = src.size(0)
            
            # åˆå§‹åŒ–decoderè¾“å…¥
            tgt_tokens = [self.tgt_vocab['<sos>']]
            
            for _ in range(max_length):
                tgt_tensor = torch.tensor([tgt_tokens], dtype=torch.long, device=self.device)
                
                # å‰å‘ä¼ æ’­
                logits = self.model(src, tgt_tensor)  # [1, cur_len, vocab_size]
                
                # è·å–ä¸‹ä¸€ä¸ªtoken
                next_token = logits[0, -1, :].argmax(dim=-1).item()
                
                tgt_tokens.append(next_token)
                
                # æ£€æŸ¥ç»“æŸæ¡ä»¶
                if next_token == self.tgt_vocab['<eos>']:
                    break
            
            # è§£ç 
            return self._decode_sequence(torch.tensor(tgt_tokens))
    
    def _decode_sequence(self, sequence: torch.Tensor) -> str:
        """å°†tokenåºåˆ—è§£ç ä¸ºæ–‡æœ¬"""
        tokens = []
        for token_id in sequence:
            token_id = token_id.item() if hasattr(token_id, 'item') else token_id
            if token_id in [self.tgt_vocab['<sos>'], self.tgt_vocab['<eos>'], self.tgt_vocab['<pad>']]:
                continue
            token = self.tgt_idx2word.get(token_id, '<unk>')
            tokens.append(token)
        
        return ' '.join(tokens)
    
    def compute_bleu(self, references: List[str], candidates: List[str]) -> float:
        """è®¡ç®—BLEUåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if len(references) != len(candidates):
            return 0.0
        
        total_score = 0
        count = 0
        
        for ref, cand in zip(references, candidates):
            ref_tokens = ref.split()
            cand_tokens = cand.split()
            
            if len(cand_tokens) == 0:
                continue
            
            # è®¡ç®—1-gramç²¾åº¦
            common_tokens = set(ref_tokens) & set(cand_tokens)
            precision = len(common_tokens) / len(cand_tokens) if cand_tokens else 0
            
            # ç®€åŒ–çš„BLEUï¼ˆåªè€ƒè™‘1-gramï¼‰
            total_score += precision
            count += 1
        
        return total_score / count if count > 0 else 0.0
    
    def train(self, num_epochs: int, save_dir: str = './checkpoints'):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        best_bleu = 0
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒTransformer...")
        print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
        for epoch in range(num_epochs):
            print(f"\nğŸ“… Epoch {epoch+1}/{num_epochs}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch()
            self.train_losses.append(train_loss)
            
            # éªŒè¯
            val_loss, bleu_score = self.validate()
            self.val_losses.append(val_loss)
            self.bleu_scores.append(bleu_score)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"BLEUåˆ†æ•°: {bleu_score:.4f}")
            print(f"å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if bleu_score > best_bleu:
                best_bleu = bleu_score
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'epoch': epoch,
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'bleu_score': bleu_score,
                    'src_vocab': self.src_vocab,
                    'tgt_vocab': self.tgt_vocab
                }, os.path.join(save_dir, 'best_model.pt'))
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (BLEU: {best_bleu:.4f})")
            
            # ç¤ºä¾‹ç¿»è¯‘
            if (epoch + 1) % 5 == 0:
                self.show_examples()
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³BLEU: {best_bleu:.4f}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'bleu_scores': self.bleu_scores
        }
        
        with open(os.path.join(save_dir, 'training_history.json'), 'w') as f:
            json.dump(history, f, indent=2)
    
    def show_examples(self, num_examples: int = 3):
        """æ˜¾ç¤ºç¿»è¯‘ç¤ºä¾‹"""
        print("\nğŸ¯ ç¿»è¯‘ç¤ºä¾‹:")
        
        self.model.eval()
        examples_shown = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                if examples_shown >= num_examples:
                    break
                
                src = batch['src'].to(self.device)
                tgt = batch['tgt']
                
                for i in range(min(num_examples - examples_shown, src.size(0))):
                    src_seq = src[i:i+1]
                    src_text = self._decode_sequence(batch['src'][i])
                    tgt_text = self._decode_sequence(tgt[i])
                    translated = self.translate(src_seq)
                    
                    print(f"  è¾“å…¥: {src_text}")
                    print(f"  å‚è€ƒ: {tgt_text}")
                    print(f"  ç¿»è¯‘: {translated}")
                    print()
                    
                    examples_shown += 1
                    
                if examples_shown >= num_examples:
                    break


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    random.seed(42)
    np.random.seed(42)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. å‡†å¤‡æ•°æ®
    print("ğŸ“š å‡†å¤‡æ•°æ®...")
    en_sentences, fr_sentences = create_sample_data(2000)
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    split_idx = int(0.8 * len(en_sentences))
    train_en, val_en = en_sentences[:split_idx], en_sentences[split_idx:]
    train_fr, val_fr = fr_sentences[:split_idx], fr_sentences[split_idx:]
    
    # æ„å»ºè¯æ±‡è¡¨
    print("ğŸ”¤ æ„å»ºè¯æ±‡è¡¨...")
    src_vocab = build_vocab(train_en, min_freq=1)
    tgt_vocab = build_vocab(train_fr, min_freq=1)
    
    print(f"è‹±è¯­è¯æ±‡é‡: {len(src_vocab)}")
    print(f"æ³•è¯­è¯æ±‡é‡: {len(tgt_vocab)}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TranslationDataset(train_en, train_fr, src_vocab, tgt_vocab)
    val_dataset = TranslationDataset(val_en, val_fr, src_vocab, tgt_vocab)
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True, 
        collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=32, 
        shuffle=False, 
        collate_fn=collate_fn
    )
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸ åˆ›å»ºTransformeræ¨¡å‹...")
    
    config = TransformerConfig(
        src_vocab_size=len(src_vocab),
        tgt_vocab_size=len(tgt_vocab),
        d_model=512,
        nhead=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        dim_feedforward=2048,
        max_seq_length=100,
        dropout=0.1
    )
    
    model = Transformer(config).to(device)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # 3. è®­ç»ƒ
    trainer = TransformerTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        src_vocab=src_vocab,
        tgt_vocab=tgt_vocab,
        device=device,
        learning_rate=1e-4
    )
    
    trainer.train(num_epochs=20, save_dir='./transformer_checkpoints')
    
    print("âœ… è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()