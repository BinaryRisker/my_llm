"""
é˜¶æ®µ4: Transformer vs LSTM Seq2Seq æ¨¡å‹å¯¹æ¯”è¯„ä¼°
================================================

æ¯”è¾ƒTransformerå’ŒLSTM Seq2Seqåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼š
- BLEUåˆ†æ•°å¯¹æ¯”
- æ¨ç†é€Ÿåº¦å¯¹æ¯”  
- ç¿»è¯‘è´¨é‡åˆ†æ
- å¯è§†åŒ–å¯¹æ¯”ç»“æœ
"""

import os
import json
import time
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
import seaborn as sns
from tqdm import tqdm

# å¯¼å…¥æ¨¡å‹
from models.transformer import Transformer, TransformerConfig
from train import TranslationDataset, build_vocab, create_sample_data, collate_fn
from torch.utils.data import DataLoader


class SimpleLSTMSeq2Seq(nn.Module):
    """ç®€åŒ–çš„LSTM Seq2Seqæ¨¡å‹ç”¨äºå¯¹æ¯”"""
    
    def __init__(self, 
                 src_vocab_size: int,
                 tgt_vocab_size: int, 
                 hidden_size: int = 256,
                 num_layers: int = 2,
                 dropout: float = 0.1):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Encoder
        self.encoder_embedding = nn.Embedding(src_vocab_size, hidden_size)
        self.encoder_lstm = nn.LSTM(
            hidden_size, hidden_size, num_layers, 
            dropout=dropout, batch_first=True
        )
        
        # Decoder
        self.decoder_embedding = nn.Embedding(tgt_vocab_size, hidden_size)
        self.decoder_lstm = nn.LSTM(
            hidden_size, hidden_size, num_layers,
            dropout=dropout, batch_first=True
        )
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Linear(hidden_size, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src, tgt):
        # Encoder
        src_emb = self.dropout(self.encoder_embedding(src))
        encoder_output, (hidden, cell) = self.encoder_lstm(src_emb)
        
        # Decoder
        tgt_emb = self.dropout(self.decoder_embedding(tgt))
        decoder_output, _ = self.decoder_lstm(tgt_emb, (hidden, cell))
        
        # è¾“å‡º
        logits = self.output_projection(decoder_output)
        
        return logits


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, device: torch.device):
        self.device = device
        self.results = defaultdict(dict)
    
    def load_transformer(self, checkpoint_path: str, config: TransformerConfig) -> Transformer:
        """åŠ è½½Transformeræ¨¡å‹"""
        model = Transformer(config).to(self.device)
        
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… åŠ è½½Transformeræ¨¡å‹: {checkpoint_path}")
        else:
            print(f"âš ï¸ Transformeræ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
        
        return model
    
    def create_lstm_baseline(self, src_vocab_size: int, tgt_vocab_size: int) -> SimpleLSTMSeq2Seq:
        """åˆ›å»ºLSTM Seq2SeqåŸºçº¿æ¨¡å‹"""
        model = SimpleLSTMSeq2Seq(
            src_vocab_size=src_vocab_size,
            tgt_vocab_size=tgt_vocab_size,
            hidden_size=256,
            num_layers=2,
            dropout=0.1
        ).to(self.device)
        
        print("âœ… åˆ›å»ºLSTM Seq2SeqåŸºçº¿æ¨¡å‹")
        return model
    
    def compute_bleu_score(self, references: List[str], candidates: List[str]) -> Dict[str, float]:
        """è®¡ç®—è¯¦ç»†çš„BLEUåˆ†æ•°"""
        if len(references) != len(candidates):
            return {'bleu-1': 0.0, 'bleu-2': 0.0, 'bleu-3': 0.0, 'bleu-4': 0.0}
        
        def get_ngrams(tokens: List[str], n: int) -> Dict[tuple, int]:
            ngrams = defaultdict(int)
            for i in range(len(tokens) - n + 1):
                ngram = tuple(tokens[i:i+n])
                ngrams[ngram] += 1
            return dict(ngrams)
        
        def compute_precision(ref_tokens: List[str], cand_tokens: List[str], n: int) -> float:
            if len(cand_tokens) < n:
                return 0.0
            
            ref_ngrams = get_ngrams(ref_tokens, n)
            cand_ngrams = get_ngrams(cand_tokens, n)
            
            matches = 0
            total = 0
            
            for ngram, count in cand_ngrams.items():
                matches += min(count, ref_ngrams.get(ngram, 0))
                total += count
            
            return matches / total if total > 0 else 0.0
        
        # è®¡ç®—å„é˜¶n-gram BLEU
        bleu_scores = {'bleu-1': 0.0, 'bleu-2': 0.0, 'bleu-3': 0.0, 'bleu-4': 0.0}
        
        for n in range(1, 5):
            total_precision = 0
            valid_count = 0
            
            for ref, cand in zip(references, candidates):
                ref_tokens = ref.split()
                cand_tokens = cand.split()
                
                precision = compute_precision(ref_tokens, cand_tokens, n)
                total_precision += precision
                valid_count += 1
            
            bleu_scores[f'bleu-{n}'] = total_precision / valid_count if valid_count > 0 else 0.0
        
        return bleu_scores
    
    def translate_with_transformer(self, 
                                  model: Transformer, 
                                  src: torch.Tensor, 
                                  tgt_vocab: Dict[str, int],
                                  max_length: int = 50) -> str:
        """ä½¿ç”¨Transformerç¿»è¯‘"""
        model.eval()
        
        with torch.no_grad():
            # åˆå§‹åŒ–decoderè¾“å…¥
            tgt_tokens = [tgt_vocab['<sos>']]
            
            for _ in range(max_length):
                tgt_tensor = torch.tensor([tgt_tokens], dtype=torch.long, device=self.device)
                
                # å‰å‘ä¼ æ’­
                logits = model(src, tgt_tensor)
                
                # è·å–ä¸‹ä¸€ä¸ªtoken
                next_token = logits[0, -1, :].argmax(dim=-1).item()
                tgt_tokens.append(next_token)
                
                # æ£€æŸ¥ç»“æŸæ¡ä»¶
                if next_token == tgt_vocab['<eos>']:
                    break
            
            return tgt_tokens[1:-1]  # å»æ‰<sos>å’Œ<eos>
    
    def translate_with_lstm(self, 
                           model: SimpleLSTMSeq2Seq,
                           src: torch.Tensor,
                           tgt_vocab: Dict[str, int],
                           max_length: int = 50) -> str:
        """ä½¿ç”¨LSTM Seq2Seqç¿»è¯‘"""
        model.eval()
        
        with torch.no_grad():
            # Encoder
            src_emb = model.encoder_embedding(src)
            encoder_output, (hidden, cell) = model.encoder_lstm(src_emb)
            
            # Decoder
            tgt_tokens = [tgt_vocab['<sos>']]
            
            for _ in range(max_length):
                tgt_input = torch.tensor([[tgt_tokens[-1]]], dtype=torch.long, device=self.device)
                tgt_emb = model.decoder_embedding(tgt_input)
                
                decoder_output, (hidden, cell) = model.decoder_lstm(tgt_emb, (hidden, cell))
                logits = model.output_projection(decoder_output)
                
                next_token = logits[0, -1, :].argmax(dim=-1).item()
                tgt_tokens.append(next_token)
                
                if next_token == tgt_vocab['<eos>']:
                    break
            
            return tgt_tokens[1:-1]  # å»æ‰<sos>å’Œ<eos>
    
    def evaluate_model(self, 
                      model, 
                      model_name: str,
                      test_loader: DataLoader, 
                      src_vocab: Dict[str, int],
                      tgt_vocab: Dict[str, int],
                      is_transformer: bool = True) -> Dict[str, float]:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        
        print(f"\nğŸ” è¯„ä¼°{model_name}æ¨¡å‹...")
        
        # åå‘è¯å…¸
        tgt_idx2word = {idx: word for word, idx in tgt_vocab.items()}
        
        def decode_sequence(sequence):
            if isinstance(sequence, torch.Tensor):
                sequence = sequence.tolist()
            tokens = []
            for token_id in sequence:
                if token_id in [tgt_vocab['<sos>'], tgt_vocab['<eos>'], tgt_vocab['<pad>']]:
                    continue
                token = tgt_idx2word.get(token_id, '<unk>')
                tokens.append(token)
            return ' '.join(tokens)
        
        references = []
        candidates = []
        inference_times = []
        
        model.eval()
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(test_loader, desc=f"è¯„ä¼°{model_name}")):
                if batch_idx >= 50:  # åªè¯„ä¼°å‰50ä¸ªbatchä»¥èŠ‚çœæ—¶é—´
                    break
                
                src = batch['src'].to(self.device)
                tgt = batch['tgt']
                
                batch_size = src.size(0)
                
                for i in range(min(5, batch_size)):  # æ¯ä¸ªbatchåªå–å‰5ä¸ªæ ·æœ¬
                    src_seq = src[i:i+1]
                    
                    # è®°å½•æ¨ç†æ—¶é—´
                    start_time = time.time()
                    
                    if is_transformer:
                        translated_tokens = self.translate_with_transformer(
                            model, src_seq, tgt_vocab, max_length=50
                        )
                    else:
                        translated_tokens = self.translate_with_lstm(
                            model, src_seq, tgt_vocab, max_length=50
                        )
                    
                    end_time = time.time()
                    inference_times.append(end_time - start_time)
                    
                    # è§£ç 
                    reference = decode_sequence(tgt[i])
                    candidate = decode_sequence(translated_tokens)
                    
                    references.append(reference)
                    candidates.append(candidate)
        
        # è®¡ç®—æŒ‡æ ‡
        bleu_scores = self.compute_bleu_score(references, candidates)
        avg_inference_time = np.mean(inference_times) if inference_times else 0
        
        results = {
            'bleu_scores': bleu_scores,
            'avg_inference_time': avg_inference_time,
            'num_params': sum(p.numel() for p in model.parameters()),
            'sample_translations': list(zip(references[:5], candidates[:5]))
        }
        
        self.results[model_name] = results
        
        return results
    
    def compare_models(self,
                      transformer_checkpoint: str,
                      test_loader: DataLoader,
                      src_vocab: Dict[str, int], 
                      tgt_vocab: Dict[str, int]):
        """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""
        
        print("ğŸ†š å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°...")
        
        # 1. åŠ è½½/åˆ›å»ºæ¨¡å‹
        transformer_config = TransformerConfig(
            src_vocab_size=len(src_vocab),
            tgt_vocab_size=len(tgt_vocab),
            d_model=512,
            nhead=8,
            num_encoder_layers=6,
            num_decoder_layers=6,
            dim_feedforward=2048,
            max_seq_length=100,
            dropout=0.1
        )
        
        transformer = self.load_transformer(transformer_checkpoint, transformer_config)
        lstm_seq2seq = self.create_lstm_baseline(len(src_vocab), len(tgt_vocab))
        
        # 2. è¯„ä¼°æ¨¡å‹
        transformer_results = self.evaluate_model(
            transformer, "Transformer", test_loader, src_vocab, tgt_vocab, is_transformer=True
        )
        
        lstm_results = self.evaluate_model(
            lstm_seq2seq, "LSTM Seq2Seq", test_loader, src_vocab, tgt_vocab, is_transformer=False
        )
        
        # 3. æ‰“å°å¯¹æ¯”ç»“æœ
        self.print_comparison()
        
        # 4. å¯è§†åŒ–
        self.visualize_comparison()
        
        return self.results
    
    def print_comparison(self):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
        print("="*60)
        
        for model_name, results in self.results.items():
            print(f"\nğŸ”¹ {model_name}:")
            print(f"   å‚æ•°é‡: {results['num_params']:,}")
            print(f"   æ¨ç†æ—¶é—´: {results['avg_inference_time']:.4f}s")
            
            bleu = results['bleu_scores']
            print(f"   BLEU-1: {bleu['bleu-1']:.4f}")
            print(f"   BLEU-2: {bleu['bleu-2']:.4f}")  
            print(f"   BLEU-3: {bleu['bleu-3']:.4f}")
            print(f"   BLEU-4: {bleu['bleu-4']:.4f}")
        
        # å¯¹æ¯”åˆ†æ
        if len(self.results) == 2:
            models = list(self.results.keys())
            transformer_results = self.results[models[0]] if 'Transformer' in models[0] else self.results[models[1]]
            lstm_results = self.results[models[1]] if 'LSTM' in models[1] else self.results[models[0]]
            
            print(f"\nğŸ” æ€§èƒ½å¯¹æ¯”:")
            
            bleu_improvement = (
                transformer_results['bleu_scores']['bleu-4'] - 
                lstm_results['bleu_scores']['bleu-4']
            ) * 100
            
            speed_ratio = lstm_results['avg_inference_time'] / transformer_results['avg_inference_time']
            
            print(f"   BLEU-4æå‡: {bleu_improvement:+.2f}%")
            print(f"   é€Ÿåº¦å¯¹æ¯”: Transformeræ˜¯LSTMçš„{speed_ratio:.2f}å€")
        
        print("\nğŸ¯ ç¿»è¯‘ç¤ºä¾‹å¯¹æ¯”:")
        for model_name, results in self.results.items():
            print(f"\n{model_name}ç¿»è¯‘ç¤ºä¾‹:")
            for i, (ref, cand) in enumerate(results['sample_translations'][:3]):
                print(f"  ç¤ºä¾‹{i+1}:")
                print(f"    å‚è€ƒ: {ref}")
                print(f"    ç¿»è¯‘: {cand}")
    
    def visualize_comparison(self):
        """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
        if len(self.results) < 2:
            return
        
        # è®¾ç½®å›¾å½¢æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Transformer vs LSTM Seq2Seq å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
        
        models = list(self.results.keys())
        
        # 1. BLEUåˆ†æ•°å¯¹æ¯”
        ax1 = axes[0, 0]
        bleu_metrics = ['bleu-1', 'bleu-2', 'bleu-3', 'bleu-4']
        x = np.arange(len(bleu_metrics))
        width = 0.35
        
        for i, model_name in enumerate(models):
            bleu_values = [self.results[model_name]['bleu_scores'][metric] for metric in bleu_metrics]
            ax1.bar(x + i*width, bleu_values, width, label=model_name, alpha=0.8)
        
        ax1.set_xlabel('BLEUæŒ‡æ ‡')
        ax1.set_ylabel('åˆ†æ•°')
        ax1.set_title('BLEUåˆ†æ•°å¯¹æ¯”')
        ax1.set_xticks(x + width/2)
        ax1.set_xticklabels(bleu_metrics)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å‚æ•°é‡å¯¹æ¯”
        ax2 = axes[0, 1]
        param_counts = [self.results[model]['num_params'] for model in models]
        colors = ['skyblue', 'lightcoral']
        
        bars = ax2.bar(models, param_counts, color=colors, alpha=0.8)
        ax2.set_ylabel('å‚æ•°é‡')
        ax2.set_title('æ¨¡å‹å‚æ•°é‡å¯¹æ¯”')
        ax2.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, param_counts):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{count:,}', ha='center', va='bottom')
        
        # 3. æ¨ç†æ—¶é—´å¯¹æ¯”
        ax3 = axes[1, 0]
        inference_times = [self.results[model]['avg_inference_time'] for model in models]
        
        bars = ax3.bar(models, inference_times, color=['lightgreen', 'lightsalmon'], alpha=0.8)
        ax3.set_ylabel('å¹³å‡æ¨ç†æ—¶é—´ (ç§’)')
        ax3.set_title('æ¨ç†é€Ÿåº¦å¯¹æ¯”')
        ax3.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time_val in zip(bars, inference_times):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height,
                    f'{time_val:.4f}s', ha='center', va='bottom')
        
        # 4. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
        ax4 = axes[1, 1]
        
        # å½’ä¸€åŒ–æŒ‡æ ‡ (0-1)
        def normalize_metric(values, higher_better=True):
            min_val, max_val = min(values), max(values)
            if max_val == min_val:
                return [0.5] * len(values)
            if higher_better:
                return [(v - min_val) / (max_val - min_val) for v in values]
            else:  # lower is better
                return [(max_val - v) / (max_val - min_val) for v in values]
        
        metrics = ['BLEU-4', 'é€Ÿåº¦', 'æ•ˆç‡']
        
        bleu4_scores = [self.results[model]['bleu_scores']['bleu-4'] for model in models]
        speed_scores = normalize_metric([1/self.results[model]['avg_inference_time'] for model in models])
        efficiency_scores = normalize_metric([
            self.results[model]['bleu_scores']['bleu-4'] / (self.results[model]['num_params'] / 1e6)
            for model in models
        ])
        
        bleu4_normalized = normalize_metric(bleu4_scores)
        
        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))  # é—­åˆ
        
        for i, model in enumerate(models):
            values = [bleu4_normalized[i], speed_scores[i], efficiency_scores[i]]
            values = np.concatenate((values, [values[0]]))  # é—­åˆ
            
            ax4.plot(angles, values, 'o-', linewidth=2, label=model)
            ax4.fill(angles, values, alpha=0.25)
        
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(metrics)
        ax4.set_ylim(0, 1)
        ax4.set_title('ç»¼åˆæ€§èƒ½å¯¹æ¯”')
        ax4.legend()
        ax4.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        os.makedirs('./evaluation_results', exist_ok=True)
        plt.savefig('./evaluation_results/model_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“ˆ å¯¹æ¯”å›¾å·²ä¿å­˜åˆ° ./evaluation_results/model_comparison.png")
    
    def save_results(self, save_path: str = './evaluation_results/comparison_results.json'):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # å¤„ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        serializable_results = {}
        for model_name, results in self.results.items():
            serializable_results[model_name] = {
                'bleu_scores': results['bleu_scores'],
                'avg_inference_time': results['avg_inference_time'],
                'num_params': results['num_params'],
                'sample_translations': results['sample_translations'][:3]  # åªä¿å­˜å‰3ä¸ªç¤ºä¾‹
            }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {save_path}")


def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    print("ğŸ“š å‡†å¤‡æµ‹è¯•æ•°æ®...")
    en_sentences, fr_sentences = create_sample_data(500)  # æ›´å°‘çš„æ•°æ®ç”¨äºå¿«é€Ÿè¯„ä¼°
    
    # æ„å»ºè¯æ±‡è¡¨
    src_vocab = build_vocab(en_sentences, min_freq=1)
    tgt_vocab = build_vocab(fr_sentences, min_freq=1)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = TranslationDataset(en_sentences, fr_sentences, src_vocab, tgt_vocab)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(device)
    
    # å¯¹æ¯”è¯„ä¼°
    transformer_checkpoint = './transformer_checkpoints/best_model.pt'
    
    results = evaluator.compare_models(
        transformer_checkpoint=transformer_checkpoint,
        test_loader=test_loader,
        src_vocab=src_vocab,
        tgt_vocab=tgt_vocab
    )
    
    # ä¿å­˜ç»“æœ
    evaluator.save_results()
    
    print("\nâœ… è¯„ä¼°å®Œæˆ!")


if __name__ == "__main__":
    main()