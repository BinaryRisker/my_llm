"""
é˜¶æ®µ4: Transformerå®éªŒå¿«é€Ÿè¿è¡Œè„šæœ¬
=================================

æä¾›ç®€å•çš„æ¥å£æ¥è¿è¡Œå®Œæ•´çš„Transformerå®éªŒï¼š
- è®­ç»ƒTransformeræ¨¡å‹
- è¯„ä¼°å’Œå¯¹æ¯”
- ç”ŸæˆæŠ¥å‘Š
"""

import os
import sys
import argparse
import time
from typing import Optional

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    required_packages = ['torch', 'numpy', 'matplotlib', 'tqdm', 'seaborn']
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…:")
        for pkg in missing_packages:
            print(f"   - {pkg}")
        print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    return True


def run_training(epochs: int = 10, batch_size: int = 32):
    """è¿è¡Œè®­ç»ƒ"""
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒTransformer (epochs={epochs}, batch_size={batch_size})")
    
    try:
        from train import main as train_main
        
        # ä¸´æ—¶ä¿®æ”¹å‚æ•°
        import train
        original_main = train.main
        
        def modified_main():
            """ä¿®æ”¹åçš„è®­ç»ƒä¸»å‡½æ•°"""
            # è®¾ç½®éšæœºç§å­
            import torch
            import random
            import numpy as np
            
            torch.manual_seed(42)
            random.seed(42)
            np.random.seed(42)
            
            # è®¾å¤‡
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
            
            # 1. å‡†å¤‡æ•°æ®
            print("ğŸ“š å‡†å¤‡æ•°æ®...")
            en_sentences, fr_sentences = train.create_sample_data(1000)  # å‡å°‘æ•°æ®é‡ä»¥åŠ å¿«è®­ç»ƒ
            
            # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
            split_idx = int(0.8 * len(en_sentences))
            train_en, val_en = en_sentences[:split_idx], en_sentences[split_idx:]
            train_fr, val_fr = fr_sentences[:split_idx], fr_sentences[split_idx:]
            
            # æ„å»ºè¯æ±‡è¡¨
            print("ğŸ”¤ æ„å»ºè¯æ±‡è¡¨...")
            src_vocab = train.build_vocab(train_en, min_freq=1)
            tgt_vocab = train.build_vocab(train_fr, min_freq=1)
            
            print(f"è‹±è¯­è¯æ±‡é‡: {len(src_vocab)}")
            print(f"æ³•è¯­è¯æ±‡é‡: {len(tgt_vocab)}")
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = train.TranslationDataset(train_en, train_fr, src_vocab, tgt_vocab)
            val_dataset = train.TranslationDataset(val_en, val_fr, src_vocab, tgt_vocab)
            
            # æ•°æ®åŠ è½½å™¨
            from torch.utils.data import DataLoader
            train_loader = DataLoader(
                train_dataset, 
                batch_size=batch_size, 
                shuffle=True, 
                collate_fn=train.collate_fn
            )
            val_loader = DataLoader(
                val_dataset, 
                batch_size=batch_size, 
                shuffle=False, 
                collate_fn=train.collate_fn
            )
            
            # 2. åˆ›å»ºæ¨¡å‹
            print("ğŸ—ï¸ åˆ›å»ºTransformeræ¨¡å‹...")
            
            config = train.TransformerConfig(
                src_vocab_size=len(src_vocab),
                tgt_vocab_size=len(tgt_vocab),
                d_model=256,  # å‡å°æ¨¡å‹ä»¥åŠ å¿«è®­ç»ƒ
                nhead=8,
                num_encoder_layers=4,  # å‡å°‘å±‚æ•°
                num_decoder_layers=4,
                dim_feedforward=1024,  # å‡å°FFNç»´åº¦
                max_seq_length=100,
                dropout=0.1
            )
            
            model = train.Transformer(config).to(device)
            
            print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
            
            # 3. è®­ç»ƒ
            trainer = train.TransformerTrainer(
                model=model,
                train_loader=train_loader,
                val_loader=val_loader,
                src_vocab=src_vocab,
                tgt_vocab=tgt_vocab,
                device=device,
                learning_rate=2e-4
            )
            
            trainer.train(num_epochs=epochs, save_dir='./transformer_checkpoints')
            
            print("âœ… è®­ç»ƒå®Œæˆ!")
            return True
        
        # è¿è¡Œä¿®æ”¹åçš„è®­ç»ƒ
        success = modified_main()
        return success
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_evaluation():
    """è¿è¡Œè¯„ä¼°"""
    print("\nğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”...")
    
    try:
        from evaluate import main as eval_main
        eval_main()
        return True
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_report():
    """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
    print("\nğŸ“‹ ç”Ÿæˆå®éªŒæŠ¥å‘Š...")
    
    report_content = f"""
# é˜¶æ®µ4: Transformeræ¨¡å‹å®éªŒæŠ¥å‘Š

## å®éªŒæ¦‚è¿°
æœ¬å®éªŒå®ç°äº†å®Œæ•´çš„Transformeræ¨¡å‹ï¼Œå¹¶ä¸LSTM Seq2Seqè¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ã€‚

## å®éªŒè®¾ç½®
- ä»»åŠ¡: è‹±æ³•æœºå™¨ç¿»è¯‘
- æ•°æ®: äººå·¥æ„å»ºçš„ç®€åŒ–è‹±æ³•å¯¹ç…§æ•°æ®
- è¯„ä¼°æŒ‡æ ‡: BLEUåˆ†æ•°ã€æ¨ç†é€Ÿåº¦ã€å‚æ•°é‡

## æ¨¡å‹æ¶æ„

### Transformer
- Encoder-Decoderæ¶æ„
- å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- ä½ç½®ç¼–ç 
- æ®‹å·®è¿æ¥å’ŒLayerNorm

### LSTM Seq2Seq (åŸºçº¿)
- Encoder-Decoder LSTM
- éšè—å±‚ä¼ é€’ä¸Šä¸‹æ–‡
- ç®€å•çš„å‰é¦ˆè¾“å‡ºå±‚

## å®éªŒç»“æœ

### æ¨¡å‹æ€§èƒ½å¯¹æ¯”
è¯¦ç»†çš„å¯¹æ¯”ç»“æœè¯·æŸ¥çœ‹ `./evaluation_results/comparison_results.json`

### å¯è§†åŒ–ç»“æœ
å¯¹æ¯”å›¾è¡¨ä¿å­˜åœ¨ `./evaluation_results/model_comparison.png`

## ä¸»è¦å‘ç°

1. **ç¿»è¯‘è´¨é‡**: Transformeråœ¨BLEUåˆ†æ•°ä¸Šé€šå¸¸ä¼˜äºLSTM Seq2Seq
2. **å¹¶è¡Œæ€§**: Transformeræ”¯æŒå¹¶è¡Œè®­ç»ƒï¼Œè®­ç»ƒæ•ˆç‡æ›´é«˜
3. **é•¿åºåˆ—å»ºæ¨¡**: Transformeråœ¨å¤„ç†é•¿åºåˆ—æ—¶è¡¨ç°æ›´ä½³
4. **æ³¨æ„åŠ›æœºåˆ¶**: æä¾›äº†æ›´å¥½çš„å¯¹é½å’Œå¯è§£é‡Šæ€§

## æŠ€æœ¯å®ç°è¦ç‚¹

1. **å› æœæ©ç **: ç¡®ä¿decoderçš„è‡ªå›å½’ç‰¹æ€§
2. **æ ‡ç­¾å¹³æ»‘**: æé«˜è®­ç»ƒç¨³å®šæ€§
3. **å­¦ä¹ ç‡è°ƒåº¦**: ä½¿ç”¨warmupå’Œè¡°å‡ç­–ç•¥
4. **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

## ç»“è®º

Transformeræ¶æ„åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šå±•ç°å‡ºäº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ï¼š
- ç¿»è¯‘è´¨é‡æ–¹é¢æœ‰æ˜æ˜¾æå‡
- è®­ç»ƒå¹¶è¡Œæ€§æ›´å¥½
- å¯¹é•¿è·ç¦»ä¾èµ–çš„å»ºæ¨¡èƒ½åŠ›æ›´å¼º

è¿™éªŒè¯äº†Transformerä½œä¸ºç°ä»£NLPä»»åŠ¡åŸºç¡€æ¶æ„çš„æœ‰æ•ˆæ€§ã€‚

---
ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    # ä¿å­˜æŠ¥å‘Š
    os.makedirs('./experiment_results', exist_ok=True)
    with open('./experiment_results/experiment_report.md', 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print("ğŸ“„ å®éªŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: ./experiment_results/experiment_report.md")
    return True


def main():
    parser = argparse.ArgumentParser(description='é˜¶æ®µ4 Transformerå®éªŒè¿è¡Œå™¨')
    parser.add_argument('--mode', choices=['train', 'eval', 'all'], default='all',
                       help='è¿è¡Œæ¨¡å¼: train=åªè®­ç»ƒ, eval=åªè¯„ä¼°, all=å…¨éƒ¨')
    parser.add_argument('--epochs', type=int, default=10,
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 10)')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 32)')
    parser.add_argument('--skip-deps-check', action='store_true',
                       help='è·³è¿‡ä¾èµ–æ£€æŸ¥')
    
    args = parser.parse_args()
    
    print("ğŸ¯ é˜¶æ®µ4: Transformeræ¨¡å‹å®éªŒ")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not args.skip_deps_check:
        if not check_dependencies():
            print("è¯·å…ˆå®‰è£…å¿…è¦ä¾èµ–")
            return False
    
    success = True
    
    # æ‰§è¡Œç›¸åº”æ¨¡å¼
    if args.mode in ['train', 'all']:
        print(f"\nğŸ“– å‡†å¤‡è®­ç»ƒ (epochs={args.epochs}, batch_size={args.batch_size})")
        if not run_training(args.epochs, args.batch_size):
            success = False
    
    if args.mode in ['eval', 'all'] and success:
        print("\nğŸ“Š å‡†å¤‡è¯„ä¼°")
        if not run_evaluation():
            success = False
    
    if success:
        generate_report()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ å®éªŒå®Œæˆ!")
        print("\nğŸ“ ç»“æœæ–‡ä»¶:")
        
        files_to_check = [
            "./transformer_checkpoints/best_model.pt",
            "./transformer_checkpoints/training_history.json", 
            "./evaluation_results/comparison_results.json",
            "./evaluation_results/model_comparison.png",
            "./experiment_results/experiment_report.md"
        ]
        
        for file_path in files_to_check:
            if os.path.exists(file_path):
                print(f"   âœ… {file_path}")
            else:
                print(f"   âŒ {file_path} (æœªç”Ÿæˆ)")
        
        print("\nğŸš€ å»ºè®®ä¸‹ä¸€æ­¥:")
        print("   1. æŸ¥çœ‹è®­ç»ƒå†å²: ./transformer_checkpoints/training_history.json")
        print("   2. æŸ¥çœ‹å¯¹æ¯”ç»“æœ: ./evaluation_results/comparison_results.json")
        print("   3. æŸ¥çœ‹å¯è§†åŒ–å›¾è¡¨: ./evaluation_results/model_comparison.png")
        print("   4. é˜…è¯»å®éªŒæŠ¥å‘Š: ./experiment_results/experiment_report.md")
        
    else:
        print("\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
        return False
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)