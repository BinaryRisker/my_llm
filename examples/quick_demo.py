#!/usr/bin/env python3
"""
LLMé¡¹ç›®å¿«é€Ÿæ¼”ç¤ºè„šæœ¬

å±•ç¤ºé¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- é…ç½®ç³»ç»Ÿä½¿ç”¨
- æ¨¡å‹è¯„ä¼°
- GLUEåŸºå‡†æµ‹è¯•
- APIæœåŠ¡
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import get_config
from evaluation.evaluation_metrics import EvaluationMetrics
from evaluation.glue_benchmark import GLUEBenchmark


def print_header(title):
    """æ‰“å°æ ¼å¼åŒ–çš„æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ {title}")
    print(f"{'='*60}")


def print_section(section):
    """æ‰“å°æ®µè½æ ‡é¢˜"""
    print(f"\nğŸ“‹ {section}")
    print("-" * 40)


def demo_config_system():
    """æ¼”ç¤ºé…ç½®ç³»ç»Ÿ"""
    print_header("é…ç½®ç³»ç»Ÿæ¼”ç¤º")
    
    # åŠ è½½å¼€å‘ç¯å¢ƒé…ç½®
    config = get_config(env='development')
    
    print_section("é¡¹ç›®åŸºç¡€ä¿¡æ¯")
    project_info = config.get('project', {})
    print(f"é¡¹ç›®åç§°: {project_info.get('name', 'æœªçŸ¥')}")
    print(f"ç‰ˆæœ¬: {project_info.get('version', 'æœªçŸ¥')}")
    print(f"æè¿°: {project_info.get('description', 'æœªçŸ¥')}")
    
    print_section("æ¨¡å‹é…ç½®")
    bert_config = config.get('model', {}).get('bert', {})
    print(f"BERTéšè—å±‚ç»´åº¦: {bert_config.get('hidden_size', 'æœªé…ç½®')}")
    print(f"BERTå±‚æ•°: {bert_config.get('num_layers', 'æœªé…ç½®')}")
    print(f"æ³¨æ„åŠ›å¤´æ•°: {bert_config.get('num_attention_heads', 'æœªé…ç½®')}")
    
    print_section("è®­ç»ƒé…ç½®")
    training_config = config.get('training', {})
    print(f"æ‰¹æ¬¡å¤§å°: {training_config.get('batch_size', 'æœªé…ç½®')}")
    print(f"å­¦ä¹ ç‡: {training_config.get('learning_rate', 'æœªé…ç½®')}")
    print(f"è®¾å¤‡: {training_config.get('device', 'æœªé…ç½®')}")


def demo_evaluation_metrics():
    """æ¼”ç¤ºè¯„ä¼°æŒ‡æ ‡"""
    print_header("è¯„ä¼°æŒ‡æ ‡æ¼”ç¤º")
    
    metrics = EvaluationMetrics()
    
    print_section("åˆ†ç±»ä»»åŠ¡è¯„ä¼°")
    # æ¨¡æ‹Ÿåˆ†ç±»ç»“æœ
    predictions = [1, 0, 1, 1, 0, 1, 0, 0]
    labels = [1, 0, 1, 0, 0, 1, 0, 1]
    
    accuracy = metrics.classification_accuracy(predictions, labels)
    f1_score = metrics.classification_f1_score(predictions, labels)
    precision = metrics.classification_precision(predictions, labels)
    recall = metrics.classification_recall(predictions, labels)
    
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"F1åˆ†æ•°: {f1_score:.4f}")
    print(f"ç²¾ç¡®ç‡: {precision:.4f}")
    print(f"å¬å›ç‡: {recall:.4f}")
    
    print_section("å›å½’ä»»åŠ¡è¯„ä¼°")
    # æ¨¡æ‹Ÿå›å½’ç»“æœ
    pred_scores = [2.1, 3.5, 1.8, 4.2, 2.9]
    true_scores = [2.0, 3.7, 1.9, 4.0, 3.1]
    
    mse = metrics.regression_mse(pred_scores, true_scores)
    rmse = metrics.regression_rmse(pred_scores, true_scores)
    mae = metrics.regression_mae(pred_scores, true_scores)
    
    print(f"å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.4f}")
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.4f}")
    
    print_section("æ–‡æœ¬ç”Ÿæˆè¯„ä¼°")
    # æ¨¡æ‹Ÿæ–‡æœ¬ç”Ÿæˆç»“æœ
    predictions = ["the cat is on the mat"]
    references = [["the cat is on the mat"]]
    
    bleu = metrics.text_generation_bleu_score(predictions, references)
    print(f"BLEUåˆ†æ•°: {bleu:.4f}")
    
    # å›°æƒ‘åº¦
    loss = 2.5
    perplexity = metrics.perplexity(loss)
    print(f"å›°æƒ‘åº¦ (loss={loss}): {perplexity:.4f}")


def demo_glue_benchmark():
    """æ¼”ç¤ºGLUEåŸºå‡†æµ‹è¯•"""
    print_header("GLUEåŸºå‡†æµ‹è¯•æ¼”ç¤º")
    
    glue = GLUEBenchmark()
    
    print_section("GLUEä»»åŠ¡ä¿¡æ¯")
    tasks = glue.get_task_names()
    print(f"æ”¯æŒçš„GLUEä»»åŠ¡æ•°é‡: {len(tasks)}")
    print("ä»»åŠ¡åˆ—è¡¨:")
    for task in tasks[:5]:  # æ˜¾ç¤ºå‰5ä¸ªä»»åŠ¡
        task_type = glue.get_task_type(task)
        task_metrics = glue.get_task_metrics(task)
        print(f"  - {task}: {task_type} ä»»åŠ¡, æŒ‡æ ‡: {list(task_metrics.keys())}")
    print(f"  ... è¿˜æœ‰ {len(tasks)-5} ä¸ªä»»åŠ¡")
    
    print_section("æ¨¡æ‹ŸGLUEè¯„ä¼°ç»“æœ")
    # æ¨¡æ‹Ÿå„ä»»åŠ¡çš„è¯„ä¼°ç»“æœ
    simulated_results = {
        'CoLA': {'matthews_corrcoef': 0.52},
        'SST-2': {'accuracy': 0.91},
        'MRPC': {'accuracy': 0.83, 'f1_score': 0.87},
        'STS-B': {'pearson_corrcoef': 0.85, 'spearman_corrcoef': 0.84},
        'QQP': {'accuracy': 0.89, 'f1_score': 0.86},
        'MNLI': {'accuracy': 0.84},
        'QNLI': {'accuracy': 0.90},
        'RTE': {'accuracy': 0.68},
        'WNLI': {'accuracy': 0.56}
    }
    
    for task, results in simulated_results.items():
        metrics_str = ", ".join([f"{k}: {v:.3f}" for k, v in results.items()])
        print(f"  {task}: {metrics_str}")
    
    # è®¡ç®—æ€»GLUEåˆ†æ•°
    glue_score = glue.compute_glue_score(simulated_results)
    print(f"\nğŸ¯ GLUEæ€»åˆ†: {glue_score:.2f}/100")


def demo_sample_data():
    """æ¼”ç¤ºæ ·æœ¬æ•°æ®"""
    print_header("æ ·æœ¬æ•°æ®æ¼”ç¤º")
    
    print_section("æ–‡æœ¬åˆ†ç±»æ ·æœ¬")
    classification_samples = [
        ("I love this movie!", "POSITIVE"),
        ("This is terrible", "NEGATIVE"), 
        ("Great product, highly recommend", "POSITIVE"),
        ("Waste of money", "NEGATIVE"),
        ("Average quality, nothing special", "NEUTRAL")
    ]
    
    for text, label in classification_samples:
        print(f"  '{text}' -> {label}")
    
    print_section("å‘½åå®ä½“è¯†åˆ«æ ·æœ¬")
    ner_samples = [
        ("Apple Inc. is located in Cupertino.", "B-ORG I-ORG O O O B-LOC"),
        ("John works at Microsoft", "B-PER O O B-ORG"),
        ("Visit New York City next week", "O B-LOC I-LOC I-LOC O O")
    ]
    
    for text, labels in ner_samples:
        print(f"  '{text}'")
        print(f"    -> {labels}")
    
    print_section("é—®ç­”æ ·æœ¬")
    qa_samples = [
        {
            "context": "The Eiffel Tower is located in Paris, France.",
            "question": "Where is the Eiffel Tower?",
            "answer": "Paris, France"
        },
        {
            "context": "Machine learning is a subset of artificial intelligence.",
            "question": "What is machine learning?", 
            "answer": "a subset of artificial intelligence"
        }
    ]
    
    for sample in qa_samples:
        print(f"  é—®é¢˜: {sample['question']}")
        print(f"  ä¸Šä¸‹æ–‡: {sample['context']}")
        print(f"  ç­”æ¡ˆ: {sample['answer']}")
        print()


def demo_api_usage():
    """æ¼”ç¤ºAPIä½¿ç”¨æ–¹æ³•"""
    print_header("APIä½¿ç”¨æ¼”ç¤º")
    
    print_section("å¯åŠ¨APIæœåŠ¡")
    print("å¯åŠ¨å‘½ä»¤:")
    print("  python api/main.py")
    print("\nAPIæ–‡æ¡£åœ°å€:")
    print("  http://localhost:8000/docs")
    print("\nAPIå¥åº·æ£€æŸ¥:")
    print("  GET http://localhost:8000/health")
    
    print_section("ä¸»è¦APIç«¯ç‚¹")
    endpoints = [
        ("GET /", "APIæ ¹ç«¯ç‚¹ï¼Œè¿”å›æ¬¢è¿ä¿¡æ¯"),
        ("GET /models", "åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"),
        ("GET /glue/tasks", "è·å–GLUEä»»åŠ¡åˆ—è¡¨"),
        ("POST /evaluate/classification", "è¯„ä¼°åˆ†ç±»ä»»åŠ¡"),
        ("POST /evaluate/regression", "è¯„ä¼°å›å½’ä»»åŠ¡"),
        ("POST /predict/text-classification", "æ–‡æœ¬åˆ†ç±»é¢„æµ‹"),
        ("POST /predict/text-generation", "æ–‡æœ¬ç”Ÿæˆé¢„æµ‹"),
        ("GET /stats", "è·å–APIç»Ÿè®¡ä¿¡æ¯")
    ]
    
    for endpoint, description in endpoints:
        print(f"  {endpoint:35} - {description}")
    
    print_section("ä½¿ç”¨ç¤ºä¾‹")
    print("Pythonå®¢æˆ·ç«¯ç¤ºä¾‹:")
    print("""
import requests

# å¥åº·æ£€æŸ¥
response = requests.get("http://localhost:8000/health")
print(response.json())

# æ–‡æœ¬åˆ†ç±»é¢„æµ‹
data = {"text": "This is a great product!"}
response = requests.post("http://localhost:8000/predict/text-classification", json=data)
print(response.json())
    """)


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ‰ æ¬¢è¿ä½¿ç”¨LLMä»é›¶å®ç°é¡¹ç›®ï¼")
    print("è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¤§è¯­è¨€æ¨¡å‹å¼€å‘å’Œè®­ç»ƒå¹³å°çš„æ¼”ç¤º")
    
    try:
        # é…ç½®ç³»ç»Ÿæ¼”ç¤º
        demo_config_system()
        
        # è¯„ä¼°æŒ‡æ ‡æ¼”ç¤º
        demo_evaluation_metrics()
        
        # GLUEåŸºå‡†æµ‹è¯•æ¼”ç¤º  
        demo_glue_benchmark()
        
        # æ ·æœ¬æ•°æ®æ¼”ç¤º
        demo_sample_data()
        
        # APIä½¿ç”¨æ¼”ç¤º
        demo_api_usage()
        
        print_header("æ¼”ç¤ºå®Œæˆ")
        print("ğŸ¯ é¡¹ç›®ç‰¹ç‚¹:")
        print("  âœ… 6ä¸ªä¸»è¦é˜¶æ®µå…¨éƒ¨å®Œæˆ")
        print("  âœ… 24ä¸ªæ ¸å¿ƒæ¨¡å—")
        print("  âœ… å®Œæ•´çš„è¯„ä¼°ä½“ç³»")
        print("  âœ… GLUEåŸºå‡†æµ‹è¯•æ”¯æŒ")
        print("  âœ… åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ")
        print("  âœ… Webç•Œé¢å’ŒAPIæ¥å£")
        print("  âœ… å·¥ä¸šçº§ä»£ç è´¨é‡")
        
        print("\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("  1. è¿è¡Œ 'python test_bert.py' æµ‹è¯•BERTæ¨¡å‹")
        print("  2. å¯åŠ¨ 'python web_interface/gradio_demo.py' æŸ¥çœ‹Webç•Œé¢")
        print("  3. å¯åŠ¨ 'python api/main.py' ä½¿ç”¨APIæœåŠ¡")
        print("  4. æŸ¥çœ‹ README.md äº†è§£æ›´å¤šä¿¡æ¯")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥é¡¹ç›®ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)