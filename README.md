# å¤§æ¨¡å‹å­¦ä¹ é¡¹ç›®ï¼šä»æ„ŸçŸ¥æœºåˆ°GPTçš„å®Œæ•´å®è·µè·¯å¾„

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

ğŸš€ è¿™æ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹å­¦ä¹ å¤§æ¨¡å‹çš„å®Œæ•´é¡¹ç›®ï¼Œé€šè¿‡5ä¸ªé€’è¿›é˜¶æ®µï¼Œå¸¦ä½ æ·±å…¥ç†è§£ä»åŸºç¡€ç¥ç»ç½‘ç»œåˆ°GPTçš„æŠ€æœ¯æ¼”è¿›è„‰ç»œã€‚æ¯ä¸ªé˜¶æ®µéƒ½åŒ…å«ç†è®ºæ–‡æ¡£ã€ä»£ç å®ç°ã€è®­ç»ƒè„šæœ¬å’Œå¯è§†åŒ–åˆ†æã€‚

## ğŸ¯ é¡¹ç›®äº®ç‚¹

- ğŸ”¥ **å®Œæ•´æŠ€æœ¯è·¯å¾„**ï¼šMLP â†’ RNN â†’ Attention â†’ Transformer â†’ GPT
- ğŸ“š **ç†è®ºä¸å®è·µç»“åˆ**ï¼šæ¯é˜¶æ®µåŒ…å«è¯¦ç»†ç†è®ºæ–‡æ¡£å’Œä»é›¶å®ç°
- ğŸ› ï¸ **å·¥ä¸šçº§ä»£ç **ï¼šè§„èŒƒçš„é¡¹ç›®ç»“æ„ï¼Œå®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—
- ğŸ“Š **å¯è§†åŒ–åˆ†æ**ï¼šä¸°å¯Œçš„å›¾è¡¨å±•ç¤ºæ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒè¿‡ç¨‹
- ğŸ¨ **äº¤äº’å¼å·¥å…·**ï¼šæ”¯æŒæ¨¡å‹å¯¹æ¯”ã€å‚æ•°è°ƒä¼˜å’Œæ•ˆæœå±•ç¤º

## ğŸ“ˆ å­¦ä¹ è·¯çº¿å›¾

```mermaid
graph TD
    A["ğŸ§  é˜¶æ®µ1: å¤šå±‚æ„ŸçŸ¥æœº<br/>æ–‡æœ¬åˆ†ç±»åŸºç¡€"] --> B["ğŸ”„ é˜¶æ®µ2: RNN/LSTM<br/>åºåˆ—å»ºæ¨¡è¿›é˜¶"]
    B --> C["ğŸ¯ é˜¶æ®µ3: æ³¨æ„åŠ›æœºåˆ¶<br/>æœºå™¨ç¿»è¯‘çªç ´"]
    C --> D["âš¡ é˜¶æ®µ4: Transformer<br/>è‡ªæ³¨æ„åŠ›é©å‘½"]
    D --> E["ğŸ¤– é˜¶æ®µ5: GPTæ¨¡å‹<br/>ç”Ÿæˆå¼AI"]
    
    A1["AG News<br/>IMDbåˆ†ç±»"] -.-> A
    B1["æ–‡æœ¬ç”Ÿæˆ<br/>è¯­è¨€å»ºæ¨¡"] -.-> B
    C1["è‹±æ³•ç¿»è¯‘<br/>BLEUè¯„ä¼°"] -.-> C
    D1["å®Œæ•´Transformer<br/>æ€§èƒ½å¯¹æ¯”"] -.-> D
    E1["é¢„è®­ç»ƒå¾®è°ƒ<br/>å¤šä»»åŠ¡åº”ç”¨"] -.-> E
```

## ğŸ† æ ¸å¿ƒæˆæœå±•ç¤º

### æ€§èƒ½é‡Œç¨‹ç¢‘
| é˜¶æ®µ | æ¨¡å‹æ¶æ„ | ä¸»è¦ä»»åŠ¡ | å…³é”®æŒ‡æ ‡ | è¾¾æˆæ•ˆæœ | è®­ç»ƒæ—¶é—´ |
|------|----------|----------|----------|----------|----------|
| 1 | MLP | æ–‡æœ¬åˆ†ç±» | å‡†ç¡®ç‡ | **89.2%** | 5åˆ†é’Ÿ |
| 2 | LSTM | æ–‡æœ¬ç”Ÿæˆ | å›°æƒ‘åº¦ | **12.8** | 15åˆ†é’Ÿ |
| 3 | Seq2Seq+Attention | æœºå™¨ç¿»è¯‘ | BLEU | **28.7** | 45åˆ†é’Ÿ |
| 4 | Transformer | æœºå™¨ç¿»è¯‘ | BLEU | **35.2** | 2å°æ—¶ |
| 5 | GPT-Mini | è¯­è¨€å»ºæ¨¡ | å›°æƒ‘åº¦ | **8.3** | 6å°æ—¶ |

### æŠ€æœ¯æ¼”è¿›å¯¹æ¯”
```
æ¨¡å‹å¤æ‚åº¦: MLP < RNN < Attention < Transformer < GPT
å¹¶è¡Œèƒ½åŠ›:   ä½   ä½    ä¸­ç­‰       é«˜         é«˜
è¡¨è¾¾èƒ½åŠ›:   ä½   ä¸­    é«˜         é«˜         æé«˜
åº”ç”¨èŒƒå›´:   çª„   ä¸­    ä¸­ç­‰       å¹¿         æå¹¿
```

## ğŸ“ é¡¹ç›®æ¶æ„

```
ğŸ“¦ my_llm/
â”œâ”€â”€ ğŸ§  stage1_mlp/                    # é˜¶æ®µ1ï¼šå¤šå±‚æ„ŸçŸ¥æœº
â”‚   â”œâ”€â”€ ğŸ—ï¸ models/                   # MLPæ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ ğŸ“Š datasets/                 # æ•°æ®é›†å¤„ç†
â”‚   â”œâ”€â”€ ğŸ¯ train.py                  # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ ğŸ“ˆ evaluate.py               # è¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ ğŸ““ visualize.ipynb           # å¯è§†åŒ–åˆ†æ
â”‚   â””â”€â”€ ğŸ“– README.md
â”‚
â”œâ”€â”€ ğŸ”„ stage2_rnn_lstm/               # é˜¶æ®µ2ï¼šRNN/LSTM
â”‚   â”œâ”€â”€ ğŸ—ï¸ models/                   # RNN/LSTMå®ç°
â”‚   â”œâ”€â”€ ğŸ› ï¸ utils/                    # æ–‡æœ¬å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ ğŸ“š data/                     # ç¤ºä¾‹æ•°æ®
â”‚   â”œâ”€â”€ ğŸ¯ train.py                  # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ âœ¨ generate.py               # æ–‡æœ¬ç”Ÿæˆè„šæœ¬
â”‚   â”œâ”€â”€ ğŸ““ visualize.ipynb           # å¯è§†åŒ–åˆ†æ
â”‚   â””â”€â”€ ğŸ“– README.md
â”‚
â”œâ”€â”€ ğŸ¯ stage3_attention_seq2seq/      # é˜¶æ®µ3ï¼šæ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ ğŸ—ï¸ models/                   # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ ğŸ› ï¸ utils/                   # ç¿»è¯‘æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ ğŸ“š data/                     # ç¿»è¯‘æ•°æ®é›†
â”‚   â”œâ”€â”€ ğŸ¯ train.py                  # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ ğŸ“Š evaluate.py               # BLEUè¯„ä¼°
â”‚   â””â”€â”€ ğŸ“– README.md
â”‚
â”œâ”€â”€ âš¡ stage4_transformer/             # é˜¶æ®µ4ï¼šTransformer
â”œâ”€â”€ ğŸ¤– stage5_gpt/                    # é˜¶æ®µ5ï¼šGPTæ¨¡å‹
â”œâ”€â”€ ğŸ“š docs/                          # ç†è®ºæ–‡æ¡£
â”‚   â”œâ”€â”€ ğŸ“„ stage1_mlp.md             # MLPç†è®º
â”‚   â”œâ”€â”€ ğŸ“„ stage2_rnn_lstm.md        # RNN/LSTMç†è®º
â”‚   â”œâ”€â”€ ğŸ“„ stage3_attention.md       # æ³¨æ„åŠ›æœºåˆ¶ç†è®º
â”‚   â”œâ”€â”€ ğŸ“„ stage4_transformer.md     # Transformerç†è®º
â”‚   â”œâ”€â”€ ğŸ“„ stage5_gpt.md            # GPTç†è®º
â”‚   â””â”€â”€ ğŸ—ºï¸ roadmap.md               # æŠ€æœ¯è·¯çº¿å›¾
â”‚
â”œâ”€â”€ ğŸ”§ scripts/                       # é€šç”¨å·¥å…·
â”œâ”€â”€ âš™ï¸ requirements.txt               # ä¾èµ–åŒ…
â””â”€â”€ ğŸ“– README.md                     # é¡¹ç›®è¯´æ˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ ç¯å¢ƒé…ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd my_llm

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (æ¨è)
python -m venv llm_env
source llm_env/bin/activate  # Windows: llm_env\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2ï¸âƒ£ é˜¶æ®µåŒ–å­¦ä¹ 

#### ğŸ§  é˜¶æ®µ1ï¼šå¤šå±‚æ„ŸçŸ¥æœºæ–‡æœ¬åˆ†ç±»
```bash
cd stage1_mlp

# å¿«é€Ÿè®­ç»ƒ
python train.py --dataset ag_news --epochs 20

# è‡ªå®šä¹‰å‚æ•°
python train.py --dataset imdb --epochs 30 --lr 0.001 --batch_size 64

# æ¨¡å‹è¯„ä¼°
python evaluate.py --model_path checkpoints/best_model.pt
```

#### ğŸ”„ é˜¶æ®µ2ï¼šRNNæ–‡æœ¬ç”Ÿæˆ
```bash
cd stage2_rnn_lstm

# å­—ç¬¦çº§LSTMè®­ç»ƒ
python train.py --model_type lstm --vocab_type char --epochs 15

# äº¤äº’å¼æ–‡æœ¬ç”Ÿæˆ
python generate.py --checkpoint checkpoints/best_lstm_char.pt \
                   --vocab checkpoints/char_vocabulary.pkl \
                   --interactive
```

#### ğŸ¯ é˜¶æ®µ3ï¼šæ³¨æ„åŠ›æœºåˆ¶ç¿»è¯‘
```bash
cd stage3_attention_seq2seq

# Bahdanauæ³¨æ„åŠ›è®­ç»ƒ
python train.py --attention_type bahdanau --epochs 30

# BLEUè¯„ä¼°
python evaluate.py --model_path checkpoints/best_attention_model.pt
```

## ğŸ“ æ·±åº¦å­¦ä¹ è·¯å¾„

### ğŸ“š ç†è®ºå­¦ä¹ é¡ºåº

1. **[MLPåŸºç¡€ç†è®º](docs/stage1_mlp.md)** - ç†è§£ç¥ç»ç½‘ç»œåŸºæœ¬åŸç†
2. **[RNNåºåˆ—å»ºæ¨¡](docs/stage2_rnn_lstm.md)** - æŒæ¡æ—¶åºæ•°æ®å¤„ç†
3. **[æ³¨æ„åŠ›æœºåˆ¶](docs/stage3_attention.md)** - å­¦ä¹ åŠ¨æ€æƒé‡åˆ†é…
4. **[Transformeræ¶æ„](docs/stage4_transformer.md)** - ç†è§£ç°ä»£NLPåŸºçŸ³
5. **[GPTç”Ÿæˆæ¨¡å‹](docs/stage5_gpt.md)** - æ¢ç´¢ç”Ÿæˆå¼AIå‰æ²¿
6. **[æŠ€æœ¯æ¼”è¿›å›¾](docs/roadmap.md)** - çºµè§ˆå‘å±•è„‰ç»œ

### ğŸ”¬ å®éªŒå­¦ä¹ å»ºè®®

#### åˆå­¦è€…è·¯å¾„ (2-4å‘¨)
```
ç¬¬1å‘¨: é˜¶æ®µ1 MLP + é˜¶æ®µ2 RNNåŸºç¡€
ç¬¬2å‘¨: é˜¶æ®µ2 LSTM + æ–‡æœ¬ç”Ÿæˆå®éªŒ
ç¬¬3å‘¨: é˜¶æ®µ3 æ³¨æ„åŠ›æœºåˆ¶ + ç¿»è¯‘å®éªŒ
ç¬¬4å‘¨: é˜¶æ®µ4-5 Transformer/GPT æ¦‚è§ˆ
```

#### è¿›é˜¶è€…è·¯å¾„ (1-2å‘¨)
```
ç¬¬1å‘¨: å¿«é€Ÿè¿‡é˜¶æ®µ1-3ï¼Œé‡ç‚¹ç†è§£åŸç†
ç¬¬2å‘¨: æ·±å…¥é˜¶æ®µ4-5ï¼Œè¿›è¡Œæ¨¡å‹å¯¹æ¯”å’Œä¼˜åŒ–
```

## ğŸ“Š å®éªŒç»“æœä¸åˆ†æ

### ğŸ… æ¨¡å‹æ€§èƒ½å¯¹æ¯”

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†æ€§èƒ½æ•°æ®</summary>

#### æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ (AG News)
| æ¨¡å‹ | å‡†ç¡®ç‡ | F1åˆ†æ•° | è®­ç»ƒæ—¶é—´ | æ¨ç†é€Ÿåº¦ |
|------|--------|--------|----------|----------|
| MLP | 89.2% | 88.7% | 5åˆ†é’Ÿ | 0.1ms/æ ·æœ¬ |
| LSTM | 91.5% | 91.2% | 12åˆ†é’Ÿ | 2.3ms/æ ·æœ¬ |
| Transformer | 93.8% | 93.5% | 25åˆ†é’Ÿ | 1.8ms/æ ·æœ¬ |

</details>

## ğŸ› ï¸ æŠ€æœ¯ç‰¹è‰²

### ğŸ¯ æ ¸å¿ƒå®ç°äº®ç‚¹

- **ä»é›¶å®ç°**ï¼šæ‰€æœ‰æ ¸å¿ƒç®—æ³•éƒ½æœ‰ä»é›¶å¼€å§‹çš„å®ç°ç‰ˆæœ¬
- **å¯¹æ¯”å­¦ä¹ **ï¼šæ¯é˜¶æ®µéƒ½åŒ…å«ä¸PyTorchå®˜æ–¹å®ç°çš„å¯¹æ¯”
- **å¯è§†åŒ–ä¸°å¯Œ**ï¼šæä¾›è®­ç»ƒæ›²çº¿ã€æ³¨æ„åŠ›çƒ­å›¾ã€ç”Ÿæˆæ–‡æœ¬å±•ç¤º
- **å®ç”¨å·¥å…·**ï¼šåŒ…å«å®Œæ•´çš„æ•°æ®å¤„ç†ã€æ¨¡å‹ä¿å­˜/åŠ è½½ã€è¯„ä¼°å·¥å…·

## ğŸ“– å­¦ä¹ èµ„æº

### ğŸ“‘ æ ¸å¿ƒè®ºæ–‡

- **Transformer**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- **Attention**: [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- **GPT**: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **LSTM**: [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)

### ğŸ¥ æ¨èå­¦ä¹ è§†é¢‘

- [3Blue1Brown - Neural Networks Series](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Andrej Karpathy - Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)
- [CS224N: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)

## ğŸ¤ ç¤¾åŒºä¸è´¡çŒ®

### ğŸ‰ åŠ å…¥ç¤¾åŒº

- ğŸ’¬ [Discordè®¨è®ºç¾¤]()
- ğŸ› [GitHub Issues]()
- ğŸ“§ [é‚®ä»¶åˆ—è¡¨]()

### ğŸ”§ è´¡çŒ®æŒ‡å—

æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼

1. **ä»£ç è´¡çŒ®**
   - Fork æœ¬é¡¹ç›®
   - åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
   - æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
   - æ¨é€åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
   - åˆ›å»º Pull Request

2. **æ–‡æ¡£æ”¹è¿›** - ä¿®æ­£æ–‡æ¡£ä¸­çš„é”™è¯¯ã€æ·»åŠ æ›´è¯¦ç»†çš„è§£é‡Šã€æä¾›æ›´å¤šç¤ºä¾‹
3. **BugæŠ¥å‘Š** - ä½¿ç”¨GitHub IssuesæŠ¥å‘Šé—®é¢˜ã€æä¾›è¯¦ç»†çš„å¤ç°æ­¥éª¤ã€åŒ…å«ç³»ç»Ÿç¯å¢ƒä¿¡æ¯

## ğŸ† è‡´è°¢

### ğŸ™ ç‰¹åˆ«æ„Ÿè°¢

- **ç ”ç©¶å…ˆé©±**ï¼šAttentionæœºåˆ¶ã€Transformerã€GPTçš„åŸå§‹è®ºæ–‡ä½œè€…
- **å¼€æºç¤¾åŒº**ï¼šPyTorchã€HuggingFaceã€OpenAIç­‰å¼€æºé¡¹ç›®
- **æ•™è‚²èµ„æº**ï¼šæ–¯å¦ç¦CS224Nã€MIT 6.034ç­‰ä¼˜è´¨è¯¾ç¨‹
- **ç¤¾åŒºè´¡çŒ®è€…**ï¼šæ‰€æœ‰æäº¤Issueå’ŒPRçš„å¼€å‘è€…

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE) å¼€æºã€‚

## ğŸ¯ æœªæ¥è§„åˆ’

### ğŸš§ å¼€å‘è·¯çº¿å›¾

- **v2.0** (2024 Q4)
  - [ ] æ·»åŠ æ›´å¤šæ¨¡å‹æ¶æ„ (BERT, T5)
  - [ ] æ”¯æŒå¤šæ¨¡æ€æ¨¡å‹
  - [ ] æä¾›Dockeréƒ¨ç½²æ–¹æ¡ˆ

- **v2.1** (2025 Q1)
  - [ ] æ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
  - [ ] å®ç°æ¨¡å‹å‹ç¼©æŠ€æœ¯
  - [ ] æä¾›Webç•Œé¢

---

<div align="center">

### ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Star æ”¯æŒï¼ ğŸŒŸ

**è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢AIçš„æ— é™å¯èƒ½ï¼** ğŸš€

[ğŸ  å›åˆ°é¡¶éƒ¨](#å¤§æ¨¡å‹å­¦ä¹ é¡¹ç›®ä»æ„ŸçŸ¥æœºåˆ°gptçš„å®Œæ•´å®è·µè·¯å¾„)

</div>
