"""
æ ‡å‡†åŒ–è¯„ä¼°ç³»ç»Ÿ - è¯„ä¼°æŒ‡æ ‡æ¨¡å—
================================

å®ç°å„ç§NLPä»»åŠ¡çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼Œæ”¯æŒï¼š
- åˆ†ç±»ä»»åŠ¡æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ï¼‰
- åºåˆ—æ ‡æ³¨ä»»åŠ¡æŒ‡æ ‡ï¼ˆå®ä½“çº§F1ã€tokençº§å‡†ç¡®ç‡ï¼‰
- ç”Ÿæˆä»»åŠ¡æŒ‡æ ‡ï¼ˆBLEUã€ROUGEã€METEORï¼‰
- é—®ç­”ä»»åŠ¡æŒ‡æ ‡ï¼ˆEMã€F1ï¼‰
- GLUE/SuperGLUEåŸºå‡†æŒ‡æ ‡

å‚è€ƒæ ‡å‡†ï¼š
- GLUE: https://gluebenchmark.com/
- SuperGLUE: https://super.gluebenchmark.com/
- SQuAD: https://rajpurkar.github.io/SQuAD-explorer/

ä½¿ç”¨æ–¹æ³•:
    from evaluation.evaluation_metrics import ClassificationMetrics
    
    metrics = ClassificationMetrics()
    scores = metrics.compute(predictions, labels)
    print(f"Accuracy: {scores['accuracy']:.4f}")
"""

import math
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from collections import defaultdict, Counter
import re

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef
    from sklearn.metrics import mean_squared_error, mean_absolute_error
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BaseMetrics:
    """åŸºç¡€è¯„ä¼°æŒ‡æ ‡ç±»"""
    
    def __init__(self):
        self.name = "base_metrics"
        self.description = "Base metrics class"
    
    def compute(self, predictions: Any, references: Any, **kwargs) -> Dict[str, float]:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            references: å‚è€ƒæ ‡å‡†ç­”æ¡ˆ
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        raise NotImplementedError("å­ç±»éœ€è¦å®ç°computeæ–¹æ³•")
    
    def _safe_division(self, numerator: float, denominator: float) -> float:
        """å®‰å…¨é™¤æ³•ï¼Œé¿å…é™¤é›¶é”™è¯¯"""
        return numerator / denominator if denominator != 0 else 0.0


class ClassificationMetrics(BaseMetrics):
    """åˆ†ç±»ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self):
        super().__init__()
        self.name = "classification_metrics"
        self.description = "Classification task evaluation metrics"
    
    def compute(self, predictions: List[int], references: List[int], **kwargs) -> Dict[str, float]:
        """
        è®¡ç®—åˆ†ç±»ä»»åŠ¡æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾åˆ—è¡¨
            references: çœŸå®æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            åŒ…å«accuracy, precision, recall, f1ç­‰æŒ‡æ ‡çš„å­—å…¸
        """
        if len(predictions) != len(references):
            raise ValueError(f"é¢„æµ‹ç»“æœé•¿åº¦({len(predictions)})ä¸å‚è€ƒç­”æ¡ˆé•¿åº¦({len(references)})ä¸åŒ¹é…")
        
        results = {}
        
        # åŸºç¡€å‡†ç¡®ç‡
        correct = sum(p == r for p, r in zip(predictions, references))
        results['accuracy'] = correct / len(predictions)
        
        if SKLEARN_AVAILABLE:
            # ä½¿ç”¨sklearnè®¡ç®—æ›´è¯¦ç»†çš„æŒ‡æ ‡
            results['accuracy'] = accuracy_score(references, predictions)
            
            # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
            precision, recall, f1, _ = precision_recall_fscore_support(
                references, predictions, average='weighted', zero_division=0
            )
            results['precision'] = precision
            results['recall'] = recall
            results['f1'] = f1
            
            # å®å¹³å‡F1
            _, _, macro_f1, _ = precision_recall_fscore_support(
                references, predictions, average='macro', zero_division=0
            )
            results['macro_f1'] = macro_f1
            
            # Matthewsç›¸å…³ç³»æ•°ï¼ˆé€‚ç”¨äºä¸å¹³è¡¡æ•°æ®é›†ï¼‰
            try:
                results['matthews_corr'] = matthews_corrcoef(references, predictions)
            except:
                results['matthews_corr'] = 0.0
        else:
            # ç®€åŒ–ç‰ˆæœ¬çš„æŒ‡æ ‡è®¡ç®—
            results.update(self._compute_basic_metrics(predictions, references))
        
        return results
    
    def _compute_basic_metrics(self, predictions: List[int], references: List[int]) -> Dict[str, float]:
        """ä¸ä¾èµ–sklearnçš„åŸºç¡€æŒ‡æ ‡è®¡ç®—"""
        # è·å–æ‰€æœ‰ç±»åˆ«
        all_labels = set(predictions + references)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„TP, FP, FN
        per_class_metrics = {}
        for label in all_labels:
            tp = sum(1 for p, r in zip(predictions, references) if p == label and r == label)
            fp = sum(1 for p, r in zip(predictions, references) if p == label and r != label)
            fn = sum(1 for p, r in zip(predictions, references) if p != label and r == label)
            
            precision = self._safe_division(tp, tp + fp)
            recall = self._safe_division(tp, tp + fn)
            f1 = self._safe_division(2 * precision * recall, precision + recall)
            
            per_class_metrics[label] = {
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
        
        # è®¡ç®—åŠ æƒå¹³å‡
        total_samples = len(references)
        weighted_precision = sum(
            per_class_metrics[label]['precision'] * references.count(label) / total_samples
            for label in all_labels
        )
        weighted_recall = sum(
            per_class_metrics[label]['recall'] * references.count(label) / total_samples
            for label in all_labels
        )
        weighted_f1 = sum(
            per_class_metrics[label]['f1'] * references.count(label) / total_samples
            for label in all_labels
        )
        
        # è®¡ç®—å®å¹³å‡
        macro_precision = sum(per_class_metrics[label]['precision'] for label in all_labels) / len(all_labels)
        macro_recall = sum(per_class_metrics[label]['recall'] for label in all_labels) / len(all_labels)
        macro_f1 = sum(per_class_metrics[label]['f1'] for label in all_labels) / len(all_labels)
        
        return {
            'precision': weighted_precision,
            'recall': weighted_recall,
            'f1': weighted_f1,
            'macro_f1': macro_f1,
            'matthews_corr': 0.0  # ç®€åŒ–ç‰ˆä¸è®¡ç®—
        }


class RegressionMetrics(BaseMetrics):
    """å›å½’ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self):
        super().__init__()
        self.name = "regression_metrics"
        self.description = "Regression task evaluation metrics"
    
    def compute(self, predictions: List[float], references: List[float], **kwargs) -> Dict[str, float]:
        """
        è®¡ç®—å›å½’ä»»åŠ¡æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹å€¼åˆ—è¡¨
            references: çœŸå®å€¼åˆ—è¡¨
            
        Returns:
            åŒ…å«MSE, MAE, RMSE, RÂ²ç­‰æŒ‡æ ‡çš„å­—å…¸
        """
        if len(predictions) != len(references):
            raise ValueError(f"é¢„æµ‹ç»“æœé•¿åº¦({len(predictions)})ä¸å‚è€ƒç­”æ¡ˆé•¿åº¦({len(references)})ä¸åŒ¹é…")
        
        results = {}
        
        if SKLEARN_AVAILABLE:
            results['mse'] = mean_squared_error(references, predictions)
            results['mae'] = mean_absolute_error(references, predictions)
        else:
            # æ‰‹åŠ¨è®¡ç®—
            results['mse'] = sum((p - r) ** 2 for p, r in zip(predictions, references)) / len(predictions)
            results['mae'] = sum(abs(p - r) for p, r in zip(predictions, references)) / len(predictions)
        
        results['rmse'] = math.sqrt(results['mse'])
        
        # è®¡ç®—RÂ²ï¼ˆå†³å®šç³»æ•°ï¼‰
        mean_ref = sum(references) / len(references)
        ss_res = sum((r - p) ** 2 for r, p in zip(references, predictions))
        ss_tot = sum((r - mean_ref) ** 2 for r in references)
        results['r2'] = 1 - self._safe_division(ss_res, ss_tot)
        
        return results


class TokenClassificationMetrics(BaseMetrics):
    """tokenåˆ†ç±»ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡ï¼ˆé€‚ç”¨äºNERç­‰ï¼‰"""
    
    def __init__(self):
        super().__init__()
        self.name = "token_classification_metrics"
        self.description = "Token classification evaluation metrics (NER, POS tagging)"
    
    def compute(self, 
                predictions: List[List[str]], 
                references: List[List[str]], 
                scheme: str = "IOB2",
                **kwargs) -> Dict[str, float]:
        """
        è®¡ç®—tokenåˆ†ç±»æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾åºåˆ—åˆ—è¡¨
            references: çœŸå®æ ‡ç­¾åºåˆ—åˆ—è¡¨
            scheme: æ ‡æ³¨æ–¹æ¡ˆ ("IOB2", "IOBES", "BILOU")
            
        Returns:
            åŒ…å«tokençº§å’Œå®ä½“çº§å‡†ç¡®ç‡ã€F1ç­‰æŒ‡æ ‡çš„å­—å…¸
        """
        if len(predictions) != len(references):
            raise ValueError(f"é¢„æµ‹ç»“æœæ•°é‡({len(predictions)})ä¸å‚è€ƒç­”æ¡ˆæ•°é‡({len(references)})ä¸åŒ¹é…")
        
        results = {}
        
        # Tokençº§åˆ«å‡†ç¡®ç‡
        total_tokens = 0
        correct_tokens = 0
        
        for pred_seq, ref_seq in zip(predictions, references):
            if len(pred_seq) != len(ref_seq):
                logger.warning(f"åºåˆ—é•¿åº¦ä¸åŒ¹é…: pred={len(pred_seq)}, ref={len(ref_seq)}")
                min_len = min(len(pred_seq), len(ref_seq))
                pred_seq, ref_seq = pred_seq[:min_len], ref_seq[:min_len]
            
            total_tokens += len(ref_seq)
            correct_tokens += sum(p == r for p, r in zip(pred_seq, ref_seq))
        
        results['token_accuracy'] = self._safe_division(correct_tokens, total_tokens)
        
        # å®ä½“çº§åˆ«è¯„ä¼°
        pred_entities = self._extract_entities(predictions, scheme)
        ref_entities = self._extract_entities(references, scheme)
        
        entity_metrics = self._compute_entity_metrics(pred_entities, ref_entities)
        results.update(entity_metrics)
        
        return results
    
    def _extract_entities(self, sequences: List[List[str]], scheme: str) -> List[List[Tuple[int, int, str]]]:
        """ä»æ ‡æ³¨åºåˆ—ä¸­æå–å®ä½“"""
        all_entities = []
        
        for seq in sequences:
            entities = []
            current_entity = None
            
            for i, label in enumerate(seq):
                if scheme == "IOB2":
                    if label.startswith('B-'):
                        # å¼€å§‹æ–°å®ä½“
                        if current_entity:
                            entities.append(current_entity)
                        current_entity = [i, i, label[2:]]
                    elif label.startswith('I-') and current_entity and label[2:] == current_entity[2]:
                        # ç»§ç»­å½“å‰å®ä½“
                        current_entity[1] = i
                    else:
                        # ç»“æŸå½“å‰å®ä½“
                        if current_entity:
                            entities.append(tuple(current_entity))
                            current_entity = None
                
                elif scheme == "BILOU":
                    if label.startswith('B-'):
                        if current_entity:
                            entities.append(current_entity)
                        current_entity = [i, i, label[2:]]
                    elif label.startswith('I-') and current_entity and label[2:] == current_entity[2]:
                        current_entity[1] = i
                    elif label.startswith('L-') and current_entity and label[2:] == current_entity[2]:
                        current_entity[1] = i
                        entities.append(tuple(current_entity))
                        current_entity = None
                    elif label.startswith('U-'):
                        if current_entity:
                            entities.append(current_entity)
                        entities.append((i, i, label[2:]))
                        current_entity = None
                    else:
                        if current_entity:
                            entities.append(tuple(current_entity))
                            current_entity = None
            
            # å¤„ç†åºåˆ—æœ«å°¾çš„å®ä½“
            if current_entity:
                entities.append(tuple(current_entity))
            
            all_entities.append(entities)
        
        return all_entities
    
    def _compute_entity_metrics(self, pred_entities: List[List[Tuple]], ref_entities: List[List[Tuple]]) -> Dict[str, float]:
        """è®¡ç®—å®ä½“çº§åˆ«æŒ‡æ ‡"""
        pred_flat = []
        ref_flat = []
        
        # ä¸ºæ¯ä¸ªå®ä½“æ·»åŠ åºåˆ—ç´¢å¼•ï¼Œç¡®ä¿å¯ä»¥hash
        for seq_idx, seq in enumerate(pred_entities):
            for entity in seq:
                if isinstance(entity, (list, tuple)) and len(entity) >= 3:
                    pred_flat.append((seq_idx, entity[0], entity[1], entity[2]))
        
        for seq_idx, seq in enumerate(ref_entities):
            for entity in seq:
                if isinstance(entity, (list, tuple)) and len(entity) >= 3:
                    ref_flat.append((seq_idx, entity[0], entity[1], entity[2]))
        
        pred_set = set(pred_flat)
        ref_set = set(ref_flat)
        
        tp = len(pred_set & ref_set)
        fp = len(pred_set - ref_set)
        fn = len(ref_set - pred_set)
        
        precision = self._safe_division(tp, tp + fp)
        recall = self._safe_division(tp, tp + fn)
        f1 = self._safe_division(2 * precision * recall, precision + recall)
        
        return {
            'entity_precision': precision,
            'entity_recall': recall,
            'entity_f1': f1,
            'entity_count_pred': len(pred_flat),
            'entity_count_ref': len(ref_flat),
            'entity_count_correct': tp
        }


class QuestionAnsweringMetrics(BaseMetrics):
    """é—®ç­”ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self):
        super().__init__()
        self.name = "question_answering_metrics"
        self.description = "Question answering evaluation metrics (SQuAD style)"
    
    def compute(self, 
                predictions: List[str], 
                references: List[str], 
                **kwargs) -> Dict[str, float]:
        """
        è®¡ç®—é—®ç­”ä»»åŠ¡æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹ç­”æ¡ˆåˆ—è¡¨
            references: å‚è€ƒç­”æ¡ˆåˆ—è¡¨
            
        Returns:
            åŒ…å«EMï¼ˆç²¾ç¡®åŒ¹é…ï¼‰å’ŒF1åˆ†æ•°çš„å­—å…¸
        """
        if len(predictions) != len(references):
            raise ValueError(f"é¢„æµ‹ç»“æœæ•°é‡({len(predictions)})ä¸å‚è€ƒç­”æ¡ˆæ•°é‡({len(references)})ä¸åŒ¹é…")
        
        em_scores = []
        f1_scores = []
        
        for pred, ref in zip(predictions, references):
            # ç²¾ç¡®åŒ¹é…
            em_scores.append(1.0 if self._normalize_answer(pred) == self._normalize_answer(ref) else 0.0)
            
            # F1åˆ†æ•°
            f1_scores.append(self._compute_answer_f1(pred, ref))
        
        return {
            'exact_match': sum(em_scores) / len(em_scores),
            'f1': sum(f1_scores) / len(f1_scores)
        }
    
    def _normalize_answer(self, answer: str) -> str:
        """æ ‡å‡†åŒ–ç­”æ¡ˆæ–‡æœ¬"""
        # è½¬å°å†™
        answer = answer.lower()
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        answer = re.sub(r'[^\w\s]', '', answer)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        answer = ' '.join(answer.split())
        
        # ç§»é™¤å† è¯
        articles = {'a', 'an', 'the'}
        tokens = answer.split()
        tokens = [token for token in tokens if token not in articles]
        
        return ' '.join(tokens)
    
    def _compute_answer_f1(self, pred: str, ref: str) -> float:
        """è®¡ç®—ç­”æ¡ˆF1åˆ†æ•°"""
        pred_tokens = self._normalize_answer(pred).split()
        ref_tokens = self._normalize_answer(ref).split()
        
        if not pred_tokens and not ref_tokens:
            return 1.0
        if not pred_tokens or not ref_tokens:
            return 0.0
        
        pred_counter = Counter(pred_tokens)
        ref_counter = Counter(ref_tokens)
        
        common = pred_counter & ref_counter
        num_common = sum(common.values())
        
        if num_common == 0:
            return 0.0
        
        precision = num_common / len(pred_tokens)
        recall = num_common / len(ref_tokens)
        
        return self._safe_division(2 * precision * recall, precision + recall)


class TextGenerationMetrics(BaseMetrics):
    """æ–‡æœ¬ç”Ÿæˆä»»åŠ¡è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self):
        super().__init__()
        self.name = "text_generation_metrics"
        self.description = "Text generation evaluation metrics (BLEU, ROUGE)"
    
    def compute(self, 
                predictions: List[str], 
                references: List[List[str]], 
                **kwargs) -> Dict[str, float]:
        """
        è®¡ç®—æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡
        
        Args:
            predictions: ç”Ÿæˆæ–‡æœ¬åˆ—è¡¨
            references: å‚è€ƒæ–‡æœ¬åˆ—è¡¨ï¼ˆæ¯ä¸ªæ ·æœ¬å¯ä»¥æœ‰å¤šä¸ªå‚è€ƒï¼‰
            
        Returns:
            åŒ…å«BLEUå’ŒROUGEåˆ†æ•°çš„å­—å…¸
        """
        if len(predictions) != len(references):
            raise ValueError(f"é¢„æµ‹ç»“æœæ•°é‡({len(predictions)})ä¸å‚è€ƒç­”æ¡ˆæ•°é‡({len(references)})ä¸åŒ¹é…")
        
        results = {}
        
        # è®¡ç®—BLEUåˆ†æ•°
        bleu_scores = []
        for pred, refs in zip(predictions, references):
            bleu_scores.append(self._compute_bleu(pred, refs))
        
        results['bleu'] = sum(bleu_scores) / len(bleu_scores)
        
        # è®¡ç®—ROUGEåˆ†æ•°
        rouge_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}
        for pred, refs in zip(predictions, references):
            rouge = self._compute_rouge(pred, refs[0] if refs else "")  # ç®€åŒ–ç‰ˆåªç”¨ç¬¬ä¸€ä¸ªå‚è€ƒ
            for key in rouge_scores:
                rouge_scores[key].append(rouge.get(key, 0.0))
        
        for key in rouge_scores:
            results[key] = sum(rouge_scores[key]) / len(rouge_scores[key])
        
        return results
    
    def _compute_bleu(self, prediction: str, references: List[str], max_n: int = 4) -> float:
        """è®¡ç®—BLEUåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not prediction or not references:
            return 0.0
        
        pred_tokens = prediction.split()
        ref_tokens_list = [ref.split() for ref in references]
        
        if not pred_tokens:
            return 0.0
        
        # è®¡ç®—n-gramç²¾ç¡®åº¦
        precisions = []
        for n in range(1, max_n + 1):
            pred_ngrams = self._get_ngrams(pred_tokens, n)
            ref_ngrams_all = []
            for ref_tokens in ref_tokens_list:
                ref_ngrams_all.extend(self._get_ngrams(ref_tokens, n))
            
            if not pred_ngrams:
                precisions.append(0.0)
                continue
            
            ref_counter = Counter(ref_ngrams_all)
            pred_counter = Counter(pred_ngrams)
            
            common = pred_counter & ref_counter
            precision = sum(common.values()) / len(pred_ngrams)
            precisions.append(precision)
        
        if all(p == 0 for p in precisions):
            return 0.0
        
        # å‡ ä½•å¹³å‡
        bleu = math.exp(sum(math.log(p) if p > 0 else float('-inf') for p in precisions) / len(precisions))
        
        # ç®€åŒ–ç‰ˆä¸è®¡ç®—brevity penalty
        return bleu
    
    def _compute_rouge(self, prediction: str, reference: str) -> Dict[str, float]:
        """è®¡ç®—ROUGEåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        pred_tokens = prediction.split()
        ref_tokens = reference.split()
        
        results = {}
        
        # ROUGE-1 (unigram)
        results['rouge-1'] = self._compute_rouge_n(pred_tokens, ref_tokens, 1)
        
        # ROUGE-2 (bigram)
        results['rouge-2'] = self._compute_rouge_n(pred_tokens, ref_tokens, 2)
        
        # ROUGE-L (longest common subsequence)
        results['rouge-l'] = self._compute_rouge_l(pred_tokens, ref_tokens)
        
        return results
    
    def _compute_rouge_n(self, pred_tokens: List[str], ref_tokens: List[str], n: int) -> float:
        """è®¡ç®—ROUGE-nåˆ†æ•°"""
        pred_ngrams = self._get_ngrams(pred_tokens, n)
        ref_ngrams = self._get_ngrams(ref_tokens, n)
        
        if not ref_ngrams:
            return 0.0
        
        pred_counter = Counter(pred_ngrams)
        ref_counter = Counter(ref_ngrams)
        
        common = pred_counter & ref_counter
        overlap = sum(common.values())
        
        return overlap / len(ref_ngrams)
    
    def _compute_rouge_l(self, pred_tokens: List[str], ref_tokens: List[str]) -> float:
        """è®¡ç®—ROUGE-Låˆ†æ•°ï¼ˆåŸºäºLCSï¼‰"""
        if not pred_tokens or not ref_tokens:
            return 0.0
        
        lcs_length = self._lcs_length(pred_tokens, ref_tokens)
        
        if len(pred_tokens) == 0 or len(ref_tokens) == 0:
            return 0.0
        
        precision = lcs_length / len(pred_tokens)
        recall = lcs_length / len(ref_tokens)
        
        if precision + recall == 0:
            return 0.0
        
        return 2 * precision * recall / (precision + recall)
    
    def _get_ngrams(self, tokens: List[str], n: int) -> List[Tuple[str, ...]]:
        """è·å–n-gramåˆ—è¡¨"""
        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]
    
    def _lcs_length(self, seq1: List[str], seq2: List[str]) -> int:
        """è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦"""
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        return dp[m][n]


def demo():
    """è¯„ä¼°æŒ‡æ ‡æ¼”ç¤º"""
    print("ğŸš€ æ ‡å‡†åŒ–è¯„ä¼°ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # 1. åˆ†ç±»ä»»åŠ¡æ¼”ç¤º
    print("\nğŸ“Š 1. åˆ†ç±»ä»»åŠ¡è¯„ä¼°")
    print("-" * 30)
    
    classification_metrics = ClassificationMetrics()
    pred_labels = [0, 1, 2, 1, 0, 2, 1, 0, 2, 1]
    true_labels = [0, 1, 2, 2, 0, 1, 1, 0, 2, 1]
    
    class_results = classification_metrics.compute(pred_labels, true_labels)
    print(f"  å‡†ç¡®ç‡: {class_results['accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {class_results['precision']:.4f}")
    print(f"  å¬å›ç‡: {class_results['recall']:.4f}")
    print(f"  F1åˆ†æ•°: {class_results['f1']:.4f}")
    print(f"  å®å¹³å‡F1: {class_results['macro_f1']:.4f}")
    
    # 2. å›å½’ä»»åŠ¡æ¼”ç¤º
    print("\nğŸ“ˆ 2. å›å½’ä»»åŠ¡è¯„ä¼°")
    print("-" * 30)
    
    regression_metrics = RegressionMetrics()
    pred_values = [2.5, 3.1, 1.8, 4.2, 2.9, 3.7, 2.1, 3.5, 4.1, 2.8]
    true_values = [2.3, 3.2, 1.9, 4.0, 3.1, 3.6, 2.0, 3.4, 4.3, 2.7]
    
    reg_results = regression_metrics.compute(pred_values, true_values)
    print(f"  MSE: {reg_results['mse']:.4f}")
    print(f"  MAE: {reg_results['mae']:.4f}")
    print(f"  RMSE: {reg_results['rmse']:.4f}")
    print(f"  RÂ²: {reg_results['r2']:.4f}")
    
    # 3. Tokenåˆ†ç±»ä»»åŠ¡æ¼”ç¤º
    print("\nğŸ·ï¸ 3. Tokenåˆ†ç±»ä»»åŠ¡è¯„ä¼° (NER)")
    print("-" * 30)
    
    token_metrics = TokenClassificationMetrics()
    pred_sequences = [
        ["B-PER", "I-PER", "O", "B-LOC", "O"],
        ["O", "B-ORG", "I-ORG", "O", "B-PER"]
    ]
    true_sequences = [
        ["B-PER", "I-PER", "O", "B-LOC", "I-LOC"],
        ["O", "B-ORG", "I-ORG", "I-ORG", "B-PER"]
    ]
    
    token_results = token_metrics.compute(pred_sequences, true_sequences)
    print(f"  Tokenå‡†ç¡®ç‡: {token_results['token_accuracy']:.4f}")
    print(f"  å®ä½“ç²¾ç¡®ç‡: {token_results['entity_precision']:.4f}")
    print(f"  å®ä½“å¬å›ç‡: {token_results['entity_recall']:.4f}")
    print(f"  å®ä½“F1: {token_results['entity_f1']:.4f}")
    print(f"  é¢„æµ‹å®ä½“æ•°: {token_results['entity_count_pred']}")
    print(f"  å‚è€ƒå®ä½“æ•°: {token_results['entity_count_ref']}")
    
    # 4. é—®ç­”ä»»åŠ¡æ¼”ç¤º
    print("\nâ“ 4. é—®ç­”ä»»åŠ¡è¯„ä¼°")
    print("-" * 30)
    
    qa_metrics = QuestionAnsweringMetrics()
    pred_answers = ["Paris", "The capital of France", "France capital"]
    true_answers = ["Paris", "Paris", "Paris"]
    
    qa_results = qa_metrics.compute(pred_answers, true_answers)
    print(f"  ç²¾ç¡®åŒ¹é…: {qa_results['exact_match']:.4f}")
    print(f"  F1åˆ†æ•°: {qa_results['f1']:.4f}")
    
    # 5. æ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ¼”ç¤º
    print("\nğŸ“ 5. æ–‡æœ¬ç”Ÿæˆä»»åŠ¡è¯„ä¼°")
    print("-" * 30)
    
    generation_metrics = TextGenerationMetrics()
    pred_texts = [
        "The cat sat on the mat",
        "It is raining today"
    ]
    ref_texts = [
        ["A cat was sitting on the mat", "The cat is on the mat"],
        ["Today it is raining", "It rains today"]
    ]
    
    gen_results = generation_metrics.compute(pred_texts, ref_texts)
    print(f"  BLEUåˆ†æ•°: {gen_results['bleu']:.4f}")
    print(f"  ROUGE-1: {gen_results['rouge-1']:.4f}")
    print(f"  ROUGE-2: {gen_results['rouge-2']:.4f}")
    print(f"  ROUGE-L: {gen_results['rouge-l']:.4f}")
    
    print(f"\nğŸ‰ è¯„ä¼°æŒ‡æ ‡æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    demo()