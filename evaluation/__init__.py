"""
æ ‡å‡†åŒ–è¯„ä¼°ç³»ç»Ÿ
==============

å®Œæ•´çš„NLPä»»åŠ¡è¯„ä¼°ç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š
- æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼ˆåˆ†ç±»ã€å›å½’ã€ç”Ÿæˆã€é—®ç­”ç­‰ï¼‰
- GLUE/SuperGLUEåŸºå‡†æµ‹è¯•
- è‡ªå®šä¹‰è¯„ä¼°ç®¡é“
- ç»“æœåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ

ç»„ä»¶è¯´æ˜:
- evaluation_metrics: æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡å®ç°
- glue_benchmark: GLUE/SuperGLUEåŸºå‡†æµ‹è¯•ç³»ç»Ÿ
- custom_evaluator: è‡ªå®šä¹‰è¯„ä¼°å™¨

ä½¿ç”¨æ–¹æ³•:
    from evaluation import ClassificationMetrics, GLUEBenchmark
    
    # åˆ†ç±»ä»»åŠ¡è¯„ä¼°
    metrics = ClassificationMetrics()
    results = metrics.compute(predictions, references)
    
    # GLUEåŸºå‡†è¯„ä¼°
    glue = GLUEBenchmark()
    benchmark_results = glue.evaluate_task('sst2', pred, ref)

ç‰ˆæœ¬: 1.0.0
"""

# è¯„ä¼°æŒ‡æ ‡å¯¼å…¥
try:
    from .evaluation_metrics import (
        BaseMetrics,
        ClassificationMetrics,
        RegressionMetrics,
        TokenClassificationMetrics,
        QuestionAnsweringMetrics,
        TextGenerationMetrics,
        demo as metrics_demo
    )
    METRICS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥è¯„ä¼°æŒ‡æ ‡æ¨¡å—: {e}")
    BaseMetrics = ClassificationMetrics = RegressionMetrics = None
    TokenClassificationMetrics = QuestionAnsweringMetrics = TextGenerationMetrics = None
    metrics_demo = None
    METRICS_AVAILABLE = False

# GLUEåŸºå‡†å¯¼å…¥
try:
    from .glue_benchmark import (
        GLUEBenchmark,
        BenchmarkResult,
        BaseBenchmarkTask,
        CoLATask, SST2Task, MRPCTask, STSBTask,
        QQPTask, MNLITask, QNLITask, RTETask, WNLITask,
        demo as glue_demo
    )
    GLUE_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥GLUEåŸºå‡†æ¨¡å—: {e}")
    GLUEBenchmark = BenchmarkResult = BaseBenchmarkTask = None
    CoLATask = SST2Task = MRPCTask = STSBTask = None
    QQPTask = MNLITask = QNLITask = RTETask = WNLITask = None
    glue_demo = None
    GLUE_AVAILABLE = False

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "1.0.0"
__author__ = "LLM Implementation Team"
__description__ = "Comprehensive NLP evaluation system"

# å¯¼å‡ºçš„å…¬å…±æ¥å£
__all__ = [
    # åŸºç¡€è¯„ä¼°æŒ‡æ ‡
    'BaseMetrics',
    'ClassificationMetrics',
    'RegressionMetrics', 
    'TokenClassificationMetrics',
    'QuestionAnsweringMetrics',
    'TextGenerationMetrics',
    
    # GLUEåŸºå‡†
    'GLUEBenchmark',
    'BenchmarkResult',
    'BaseBenchmarkTask',
    'CoLATask', 'SST2Task', 'MRPCTask', 'STSBTask',
    'QQPTask', 'MNLITask', 'QNLITask', 'RTETask', 'WNLITask',
    
    # æ¼”ç¤ºå‡½æ•°
    'metrics_demo',
    'glue_demo',
    'demo',
    
    # å¯ç”¨æ€§æ ‡å¿—
    'METRICS_AVAILABLE',
    'GLUE_AVAILABLE',
]


def get_evaluation_info():
    """è·å–è¯„ä¼°ç³»ç»Ÿä¿¡æ¯"""
    return {
        "name": "Evaluation System",
        "version": __version__,
        "description": __description__,
        "author": __author__,
        "components": {
            "metrics": METRICS_AVAILABLE,
            "glue_benchmark": GLUE_AVAILABLE,
        },
        "supported_metrics": [
            "accuracy", "precision", "recall", "f1",
            "mse", "mae", "rmse", "r2", "pearson_corr",
            "exact_match", "entity_f1", "token_accuracy",
            "bleu", "rouge", "matthews_corr"
        ],
        "supported_benchmarks": [
            "GLUE", "CoLA", "SST-2", "MRPC", "STS-B",
            "QQP", "MNLI", "QNLI", "RTE", "WNLI"
        ]
    }


def list_available_metrics():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è¯„ä¼°æŒ‡æ ‡"""
    metrics = []
    
    if METRICS_AVAILABLE:
        metrics.extend([
            {
                "name": "ClassificationMetrics",
                "description": "åˆ†ç±»ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡",
                "metrics": ["accuracy", "precision", "recall", "f1", "macro_f1", "matthews_corr"],
                "available": True
            },
            {
                "name": "RegressionMetrics", 
                "description": "å›å½’ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡",
                "metrics": ["mse", "mae", "rmse", "r2"],
                "available": True
            },
            {
                "name": "TokenClassificationMetrics",
                "description": "tokenåˆ†ç±»ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡ï¼ˆNERç­‰ï¼‰",
                "metrics": ["token_accuracy", "entity_precision", "entity_recall", "entity_f1"],
                "available": True
            },
            {
                "name": "QuestionAnsweringMetrics",
                "description": "é—®ç­”ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡",
                "metrics": ["exact_match", "f1"],
                "available": True
            },
            {
                "name": "TextGenerationMetrics",
                "description": "æ–‡æœ¬ç”Ÿæˆä»»åŠ¡è¯„ä¼°æŒ‡æ ‡",
                "metrics": ["bleu", "rouge-1", "rouge-2", "rouge-l"],
                "available": True
            }
        ])
    
    return metrics


def list_available_benchmarks():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•"""
    benchmarks = []
    
    if GLUE_AVAILABLE:
        benchmarks.append({
            "name": "GLUE",
            "description": "General Language Understanding Evaluation",
            "tasks": [
                "CoLA", "SST-2", "MRPC", "STS-B", "QQP",
                "MNLI", "QNLI", "RTE", "WNLI"
            ],
            "available": True
        })
    
    return benchmarks


def create_evaluator(task_type: str, **kwargs):
    """
    åˆ›å»ºè¯„ä¼°å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        task_type: ä»»åŠ¡ç±»å‹ ("classification", "regression", "token_classification", 
                           "question_answering", "text_generation")
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ç›¸åº”çš„è¯„ä¼°å™¨å®ä¾‹
        
    Example:
        >>> evaluator = create_evaluator("classification")
        >>> results = evaluator.compute(predictions, references)
    """
    if not METRICS_AVAILABLE:
        raise ImportError("è¯„ä¼°æŒ‡æ ‡æ¨¡å—ä¸å¯ç”¨")
    
    evaluators = {
        "classification": ClassificationMetrics,
        "regression": RegressionMetrics,
        "token_classification": TokenClassificationMetrics,
        "question_answering": QuestionAnsweringMetrics,
        "text_generation": TextGenerationMetrics
    }
    
    if task_type not in evaluators:
        raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}. æ”¯æŒçš„ç±»å‹: {list(evaluators.keys())}")
    
    return evaluators[task_type](**kwargs)


def create_benchmark(benchmark_name: str, **kwargs):
    """
    åˆ›å»ºåŸºå‡†æµ‹è¯•çš„ä¾¿æ·å‡½æ•°
    
    Args:
        benchmark_name: åŸºå‡†åç§° ("glue")
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ç›¸åº”çš„åŸºå‡†æµ‹è¯•å®ä¾‹
        
    Example:
        >>> benchmark = create_benchmark("glue")
        >>> results = benchmark.evaluate_task("sst2", pred, ref)
    """
    if not GLUE_AVAILABLE:
        raise ImportError("GLUEåŸºå‡†æ¨¡å—ä¸å¯ç”¨")
    
    benchmarks = {
        "glue": GLUEBenchmark,
    }
    
    if benchmark_name not in benchmarks:
        raise ValueError(f"ä¸æ”¯æŒçš„åŸºå‡†: {benchmark_name}. æ”¯æŒçš„åŸºå‡†: {list(benchmarks.keys())}")
    
    return benchmarks[benchmark_name](**kwargs)


def demo():
    """è¿è¡Œå®Œæ•´çš„è¯„ä¼°ç³»ç»Ÿæ¼”ç¤º"""
    print("ğŸš€ æ ‡å‡†åŒ–è¯„ä¼°ç³»ç»Ÿå®Œæ•´æ¼”ç¤º")
    print("=" * 60)
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    info = get_evaluation_info()
    print("ğŸ“‹ ç³»ç»Ÿä¿¡æ¯:")
    print(f"  åç§°: {info['name']}")
    print(f"  ç‰ˆæœ¬: {info['version']}")
    print(f"  æè¿°: {info['description']}")
    
    print(f"\nğŸ§© ç»„ä»¶çŠ¶æ€:")
    for component, available in info['components'].items():
        status = "âœ… å¯ç”¨" if available else "âŒ ä¸å¯ç”¨"
        print(f"  {component}: {status}")
    
    # æ˜¾ç¤ºå¯ç”¨æŒ‡æ ‡
    print(f"\nğŸ“Š å¯ç”¨è¯„ä¼°æŒ‡æ ‡:")
    metrics = list_available_metrics()
    for metric in metrics:
        print(f"  â€¢ {metric['name']}: {metric['description']}")
        print(f"    æŒ‡æ ‡: {', '.join(metric['metrics'])}")
    
    # æ˜¾ç¤ºå¯ç”¨åŸºå‡†
    print(f"\nğŸ† å¯ç”¨åŸºå‡†æµ‹è¯•:")
    benchmarks = list_available_benchmarks()
    for benchmark in benchmarks:
        print(f"  â€¢ {benchmark['name']}: {benchmark['description']}")
        print(f"    ä»»åŠ¡: {', '.join(benchmark['tasks'])}")
    
    # è¿è¡Œå„ç»„ä»¶æ¼”ç¤º
    if METRICS_AVAILABLE and metrics_demo:
        print(f"\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°æŒ‡æ ‡æ¼”ç¤º")
        print("="*60)
        try:
            metrics_demo()
        except Exception as e:
            print(f"âŒ è¯„ä¼°æŒ‡æ ‡æ¼”ç¤ºå¤±è´¥: {e}")
    
    if GLUE_AVAILABLE and glue_demo:
        print(f"\n" + "="*60)
        print("ğŸ† GLUEåŸºå‡†æ¼”ç¤º")
        print("="*60)
        try:
            glue_demo()
        except Exception as e:
            print(f"âŒ GLUEåŸºå‡†æ¼”ç¤ºå¤±è´¥: {e}")
    
    print(f"\n" + "="*60)
    print("ğŸ‰ è¯„ä¼°ç³»ç»Ÿæ¼”ç¤ºå®Œæˆ!")
    print("="*60)


if __name__ == "__main__":
    demo()