"""
GLUE/SuperGLUEåŸºå‡†è¯„ä¼°ç³»ç»Ÿ
==========================

å®ç°æ ‡å‡†åŒ–çš„GLUEå’ŒSuperGLUEåŸºå‡†æµ‹è¯•è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼š
- GLUEä»»åŠ¡ï¼šCoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI
- SuperGLUEä»»åŠ¡ï¼šBoolQ, CB, COPA, MultiRC, ReCoRD, RTE, WiC, WSC
- æ ‡å‡†åŒ–è¯„ä¼°æŒ‡æ ‡å’ŒæŠ¥å‘Šç”Ÿæˆ

å‚è€ƒæ ‡å‡†ï¼š
- GLUE: https://gluebenchmark.com/
- SuperGLUE: https://super.gluebenchmark.com/
- è®ºæ–‡: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding

ä½¿ç”¨æ–¹æ³•:
    from evaluation.glue_benchmark import GLUEBenchmark
    
    benchmark = GLUEBenchmark()
    results = benchmark.evaluate_task('sst2', predictions, references)
    print(f"SST-2 Accuracy: {results['accuracy']:.4f}")
"""

import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass
from abc import ABC, abstractmethod

try:
    from .evaluation_metrics import (
        ClassificationMetrics, RegressionMetrics, 
        QuestionAnsweringMetrics, TokenClassificationMetrics
    )
    METRICS_AVAILABLE = True
except ImportError:
    try:
        from evaluation_metrics import (
            ClassificationMetrics, RegressionMetrics,
            QuestionAnsweringMetrics, TokenClassificationMetrics
        )
        METRICS_AVAILABLE = True
    except ImportError:
        METRICS_AVAILABLE = False
        ClassificationMetrics = RegressionMetrics = None
        QuestionAnsweringMetrics = TokenClassificationMetrics = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    task_name: str
    metric_name: str
    score: float
    details: Dict[str, Any]
    description: str


class BaseBenchmarkTask(ABC):
    """åŸºç¡€åŸºå‡†ä»»åŠ¡ç±»"""
    
    def __init__(self, task_name: str, description: str):
        self.task_name = task_name
        self.description = description
        self.metric_name = "score"
    
    @abstractmethod
    def evaluate(self, predictions: Any, references: Any, **kwargs) -> BenchmarkResult:
        """è¯„ä¼°ä»»åŠ¡æ€§èƒ½"""
        pass
    
    def get_task_info(self) -> Dict[str, str]:
        """è·å–ä»»åŠ¡ä¿¡æ¯"""
        return {
            "name": self.task_name,
            "description": self.description,
            "metric": self.metric_name
        }


class CoLATask(BaseBenchmarkTask):
    """CoLA - Corpus of Linguistic Acceptability"""
    
    def __init__(self):
        super().__init__(
            "cola",
            "è¯­è¨€å¯æ¥å—æ€§åˆ¤æ–­ - åˆ¤æ–­å¥å­æ˜¯å¦ç¬¦åˆè¯­æ³•"
        )
        self.metric_name = "matthews_corr"
        self.metrics = ClassificationMetrics() if METRICS_AVAILABLE else None
    
    def evaluate(self, predictions: List[int], references: List[int], **kwargs) -> BenchmarkResult:
        """
        CoLAä»»åŠ¡è¯„ä¼°
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾ (0: ä¸å¯æ¥å—, 1: å¯æ¥å—)
            references: çœŸå®æ ‡ç­¾
            
        Returns:
            åŒ…å«Matthewsç›¸å…³ç³»æ•°çš„ç»“æœ
        """
        if not self.metrics:
            return BenchmarkResult(
                task_name=self.task_name,
                metric_name=self.metric_name,
                score=0.0,
                details={"error": "è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨"},
                description="CoLAä»»åŠ¡è¯„ä¼°å¤±è´¥"
            )
        
        results = self.metrics.compute(predictions, references)
        
        return BenchmarkResult(
            task_name=self.task_name,
            metric_name=self.metric_name,
            score=results.get('matthews_corr', 0.0),
            details=results,
            description=f"CoLA Matthewsç›¸å…³ç³»æ•°: {results.get('matthews_corr', 0.0):.4f}"
        )


class SST2Task(BaseBenchmarkTask):
    """SST-2 - Stanford Sentiment Treebank"""
    
    def __init__(self):
        super().__init__(
            "sst2",
            "æƒ…æ„Ÿåˆ†æ - åˆ¤æ–­ç”µå½±è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘"
        )
        self.metric_name = "accuracy"
        self.metrics = ClassificationMetrics() if METRICS_AVAILABLE else None
    
    def evaluate(self, predictions: List[int], references: List[int], **kwargs) -> BenchmarkResult:
        """
        SST-2ä»»åŠ¡è¯„ä¼°
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾ (0: è´Ÿé¢, 1: æ­£é¢)
            references: çœŸå®æ ‡ç­¾
            
        Returns:
            åŒ…å«å‡†ç¡®ç‡çš„ç»“æœ
        """
        if not self.metrics:
            return BenchmarkResult(
                task_name=self.task_name,
                metric_name=self.metric_name,
                score=0.0,
                details={"error": "è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨"},
                description="SST-2ä»»åŠ¡è¯„ä¼°å¤±è´¥"
            )
        
        results = self.metrics.compute(predictions, references)
        
        return BenchmarkResult(
            task_name=self.task_name,
            metric_name=self.metric_name,
            score=results.get('accuracy', 0.0),
            details=results,
            description=f"SST-2 å‡†ç¡®ç‡: {results.get('accuracy', 0.0):.4f}"
        )


class MRPCTask(BaseBenchmarkTask):
    """MRPC - Microsoft Research Paraphrase Corpus"""
    
    def __init__(self):
        super().__init__(
            "mrpc",
            "é‡Šä¹‰æ£€æµ‹ - åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦æ„æ€ç›¸åŒ"
        )
        self.metric_name = "f1"
        self.metrics = ClassificationMetrics() if METRICS_AVAILABLE else None
    
    def evaluate(self, predictions: List[int], references: List[int], **kwargs) -> BenchmarkResult:
        """
        MRPCä»»åŠ¡è¯„ä¼°
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾ (0: éé‡Šä¹‰, 1: é‡Šä¹‰)
            references: çœŸå®æ ‡ç­¾
            
        Returns:
            åŒ…å«F1åˆ†æ•°å’Œå‡†ç¡®ç‡çš„ç»“æœ
        """
        if not self.metrics:
            return BenchmarkResult(
                task_name=self.task_name,
                metric_name=self.metric_name,
                score=0.0,
                details={"error": "è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨"},
                description="MRPCä»»åŠ¡è¯„ä¼°å¤±è´¥"
            )
        
        results = self.metrics.compute(predictions, references)
        
        # MRPCä½¿ç”¨F1å’Œå‡†ç¡®ç‡çš„å¹³å‡å€¼
        f1_score = results.get('f1', 0.0)
        accuracy = results.get('accuracy', 0.0)
        combined_score = (f1_score + accuracy) / 2
        
        results['combined_score'] = combined_score
        
        return BenchmarkResult(
            task_name=self.task_name,
            metric_name="f1_accuracy_avg",
            score=combined_score,
            details=results,
            description=f"MRPC F1/Accuracy: {combined_score:.4f} (F1: {f1_score:.4f}, Acc: {accuracy:.4f})"
        )


class STSBTask(BaseBenchmarkTask):
    """STS-B - Semantic Textual Similarity Benchmark"""
    
    def __init__(self):
        super().__init__(
            "stsb",
            "è¯­ä¹‰ç›¸ä¼¼åº¦ - é¢„æµ‹ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼åº¦åˆ†æ•°(0-5)"
        )
        self.metric_name = "pearson_corr"
        self.metrics = RegressionMetrics() if METRICS_AVAILABLE else None
    
    def evaluate(self, predictions: List[float], references: List[float], **kwargs) -> BenchmarkResult:
        """
        STS-Bä»»åŠ¡è¯„ä¼°
        
        Args:
            predictions: é¢„æµ‹ç›¸ä¼¼åº¦åˆ†æ•° (0-5)
            references: çœŸå®ç›¸ä¼¼åº¦åˆ†æ•°
            
        Returns:
            åŒ…å«çš®å°”é€Šç›¸å…³ç³»æ•°çš„ç»“æœ
        """
        if not self.metrics:
            return BenchmarkResult(
                task_name=self.task_name,
                metric_name=self.metric_name,
                score=0.0,
                details={"error": "è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨"},
                description="STS-Bä»»åŠ¡è¯„ä¼°å¤±è´¥"
            )
        
        results = self.metrics.compute(predictions, references)
        
        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        pearson_corr = self._compute_pearson_correlation(predictions, references)
        results['pearson_corr'] = pearson_corr
        
        return BenchmarkResult(
            task_name=self.task_name,
            metric_name=self.metric_name,
            score=pearson_corr,
            details=results,
            description=f"STS-B çš®å°”é€Šç›¸å…³ç³»æ•°: {pearson_corr:.4f}"
        )
    
    def _compute_pearson_correlation(self, x: List[float], y: List[float]) -> float:
        """è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°"""
        if len(x) != len(y) or len(x) == 0:
            return 0.0
        
        n = len(x)
        mean_x = sum(x) / n
        mean_y = sum(y) / n
        
        numerator = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n))
        
        sum_sq_x = sum((x[i] - mean_x) ** 2 for i in range(n))
        sum_sq_y = sum((y[i] - mean_y) ** 2 for i in range(n))
        
        denominator = (sum_sq_x * sum_sq_y) ** 0.5
        
        return numerator / denominator if denominator != 0 else 0.0


class QQPTask(BaseBenchmarkTask):
    """QQP - Quora Question Pairs"""
    
    def __init__(self):
        super().__init__(
            "qqp",
            "é—®é¢˜åŒ¹é… - åˆ¤æ–­ä¸¤ä¸ªé—®é¢˜æ˜¯å¦ç­‰ä»·"
        )
        self.metric_name = "f1"
        self.metrics = ClassificationMetrics() if METRICS_AVAILABLE else None
    
    def evaluate(self, predictions: List[int], references: List[int], **kwargs) -> BenchmarkResult:
        """
        QQPä»»åŠ¡è¯„ä¼°
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾ (0: ä¸ç­‰ä»·, 1: ç­‰ä»·)
            references: çœŸå®æ ‡ç­¾
            
        Returns:
            åŒ…å«F1åˆ†æ•°å’Œå‡†ç¡®ç‡çš„ç»“æœ
        """
        if not self.metrics:
            return BenchmarkResult(
                task_name=self.task_name,
                metric_name=self.metric_name,
                score=0.0,
                details={"error": "è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨"},
                description="QQPä»»åŠ¡è¯„ä¼°å¤±è´¥"
            )
        
        results = self.metrics.compute(predictions, references)
        
        return BenchmarkResult(
            task_name=self.task_name,
            metric_name=self.metric_name,
            score=results.get('f1', 0.0),
            details=results,
            description=f"QQP F1åˆ†æ•°: {results.get('f1', 0.0):.4f}"
        )


class MNLITask(BaseBenchmarkTask):
    """MNLI - Multi-Genre Natural Language Inference"""
    
    def __init__(self):
        super().__init__(
            "mnli",
            "è‡ªç„¶è¯­è¨€æ¨ç† - åˆ¤æ–­å‡è®¾ä¸å‰æçš„å…³ç³»"
        )
        self.metric_name = "accuracy"
        self.metrics = ClassificationMetrics() if METRICS_AVAILABLE else None
    
    def evaluate(self, predictions: List[int], references: List[int], **kwargs) -> BenchmarkResult:
        """
        MNLIä»»åŠ¡è¯„ä¼°
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾ (0: entailment, 1: neutral, 2: contradiction)
            references: çœŸå®æ ‡ç­¾
            
        Returns:
            åŒ…å«å‡†ç¡®ç‡çš„ç»“æœ
        """
        if not self.metrics:
            return BenchmarkResult(
                task_name=self.task_name,
                metric_name=self.metric_name,
                score=0.0,
                details={"error": "è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨"},
                description="MNLIä»»åŠ¡è¯„ä¼°å¤±è´¥"
            )
        
        results = self.metrics.compute(predictions, references)
        
        return BenchmarkResult(
            task_name=self.task_name,
            metric_name=self.metric_name,
            score=results.get('accuracy', 0.0),
            details=results,
            description=f"MNLI å‡†ç¡®ç‡: {results.get('accuracy', 0.0):.4f}"
        )


class QNLITask(BaseBenchmarkTask):
    """QNLI - Question Natural Language Inference"""
    
    def __init__(self):
        super().__init__(
            "qnli",
            "é—®é¢˜è‡ªç„¶è¯­è¨€æ¨ç† - åˆ¤æ–­æ®µè½æ˜¯å¦åŒ…å«é—®é¢˜çš„ç­”æ¡ˆ"
        )
        self.metric_name = "accuracy"
        self.metrics = ClassificationMetrics() if METRICS_AVAILABLE else None
    
    def evaluate(self, predictions: List[int], references: List[int], **kwargs) -> BenchmarkResult:
        """
        QNLIä»»åŠ¡è¯„ä¼°
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾ (0: not_entailment, 1: entailment)
            references: çœŸå®æ ‡ç­¾
            
        Returns:
            åŒ…å«å‡†ç¡®ç‡çš„ç»“æœ
        """
        if not self.metrics:
            return BenchmarkResult(
                task_name=self.task_name,
                metric_name=self.metric_name,
                score=0.0,
                details={"error": "è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨"},
                description="QNLIä»»åŠ¡è¯„ä¼°å¤±è´¥"
            )
        
        results = self.metrics.compute(predictions, references)
        
        return BenchmarkResult(
            task_name=self.task_name,
            metric_name=self.metric_name,
            score=results.get('accuracy', 0.0),
            details=results,
            description=f"QNLI å‡†ç¡®ç‡: {results.get('accuracy', 0.0):.4f}"
        )


class RTETask(BaseBenchmarkTask):
    """RTE - Recognizing Textual Entailment"""
    
    def __init__(self):
        super().__init__(
            "rte",
            "æ–‡æœ¬è•´å«è¯†åˆ« - åˆ¤æ–­æ–‡æœ¬æ˜¯å¦è•´å«å‡è®¾"
        )
        self.metric_name = "accuracy"
        self.metrics = ClassificationMetrics() if METRICS_AVAILABLE else None
    
    def evaluate(self, predictions: List[int], references: List[int], **kwargs) -> BenchmarkResult:
        """
        RTEä»»åŠ¡è¯„ä¼°
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾ (0: not_entailment, 1: entailment)
            references: çœŸå®æ ‡ç­¾
            
        Returns:
            åŒ…å«å‡†ç¡®ç‡çš„ç»“æœ
        """
        if not self.metrics:
            return BenchmarkResult(
                task_name=self.task_name,
                metric_name=self.metric_name,
                score=0.0,
                details={"error": "è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨"},
                description="RTEä»»åŠ¡è¯„ä¼°å¤±è´¥"
            )
        
        results = self.metrics.compute(predictions, references)
        
        return BenchmarkResult(
            task_name=self.task_name,
            metric_name=self.metric_name,
            score=results.get('accuracy', 0.0),
            details=results,
            description=f"RTE å‡†ç¡®ç‡: {results.get('accuracy', 0.0):.4f}"
        )


class WNLITask(BaseBenchmarkTask):
    """WNLI - Winograd Natural Language Inference"""
    
    def __init__(self):
        super().__init__(
            "wnli",
            "Winogradè‡ªç„¶è¯­è¨€æ¨ç† - å¤æ‚çš„å¸¸è¯†æ¨ç†ä»»åŠ¡"
        )
        self.metric_name = "accuracy"
        self.metrics = ClassificationMetrics() if METRICS_AVAILABLE else None
    
    def evaluate(self, predictions: List[int], references: List[int], **kwargs) -> BenchmarkResult:
        """
        WNLIä»»åŠ¡è¯„ä¼°
        
        Args:
            predictions: é¢„æµ‹æ ‡ç­¾ (0: not_entailment, 1: entailment)
            references: çœŸå®æ ‡ç­¾
            
        Returns:
            åŒ…å«å‡†ç¡®ç‡çš„ç»“æœ
        """
        if not self.metrics:
            return BenchmarkResult(
                task_name=self.task_name,
                metric_name=self.metric_name,
                score=0.0,
                details={"error": "è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨"},
                description="WNLIä»»åŠ¡è¯„ä¼°å¤±è´¥"
            )
        
        results = self.metrics.compute(predictions, references)
        
        return BenchmarkResult(
            task_name=self.task_name,
            metric_name=self.metric_name,
            score=results.get('accuracy', 0.0),
            details=results,
            description=f"WNLI å‡†ç¡®ç‡: {results.get('accuracy', 0.0):.4f}"
        )


class GLUEBenchmark:
    """GLUEåŸºå‡†è¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self):
        self.tasks = {
            'cola': CoLATask(),
            'sst2': SST2Task(),
            'mrpc': MRPCTask(),
            'stsb': STSBTask(),
            'qqp': QQPTask(),
            'mnli': MNLITask(),
            'qnli': QNLITask(),
            'rte': RTETask(),
            'wnli': WNLITask()
        }
        
        self.name = "GLUE"
        self.description = "General Language Understanding Evaluation"
        self.version = "1.0"
    
    def get_available_tasks(self) -> List[str]:
        """è·å–å¯ç”¨çš„ä»»åŠ¡åˆ—è¡¨"""
        return list(self.tasks.keys())
    
    def get_task_info(self, task_name: str) -> Dict[str, str]:
        """è·å–ä»»åŠ¡ä¿¡æ¯"""
        if task_name not in self.tasks:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡: {task_name}")
        
        return self.tasks[task_name].get_task_info()
    
    def evaluate_task(self, 
                     task_name: str, 
                     predictions: Any, 
                     references: Any, 
                     **kwargs) -> BenchmarkResult:
        """
        è¯„ä¼°å•ä¸ªä»»åŠ¡
        
        Args:
            task_name: ä»»åŠ¡åç§°
            predictions: é¢„æµ‹ç»“æœ
            references: å‚è€ƒç­”æ¡ˆ
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ä»»åŠ¡è¯„ä¼°ç»“æœ
        """
        if task_name not in self.tasks:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡: {task_name}. å¯ç”¨ä»»åŠ¡: {list(self.tasks.keys())}")
        
        task = self.tasks[task_name]
        return task.evaluate(predictions, references, **kwargs)
    
    def evaluate_multiple_tasks(self, 
                               task_results: Dict[str, Tuple[Any, Any]], 
                               **kwargs) -> Dict[str, BenchmarkResult]:
        """
        è¯„ä¼°å¤šä¸ªä»»åŠ¡
        
        Args:
            task_results: ä»»åŠ¡ç»“æœå­—å…¸ {task_name: (predictions, references)}
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ‰€æœ‰ä»»åŠ¡çš„è¯„ä¼°ç»“æœ
        """
        results = {}
        
        for task_name, (predictions, references) in task_results.items():
            try:
                results[task_name] = self.evaluate_task(task_name, predictions, references, **kwargs)
            except Exception as e:
                logger.error(f"ä»»åŠ¡ {task_name} è¯„ä¼°å¤±è´¥: {e}")
                results[task_name] = BenchmarkResult(
                    task_name=task_name,
                    metric_name="error",
                    score=0.0,
                    details={"error": str(e)},
                    description=f"ä»»åŠ¡ {task_name} è¯„ä¼°å¤±è´¥"
                )
        
        return results
    
    def compute_overall_score(self, task_results: Dict[str, BenchmarkResult]) -> float:
        """
        è®¡ç®—GLUEæ€»åˆ†
        
        Args:
            task_results: å„ä»»åŠ¡è¯„ä¼°ç»“æœ
            
        Returns:
            GLUEæ€»åˆ†ï¼ˆæ‰€æœ‰ä»»åŠ¡åˆ†æ•°çš„å¹³å‡å€¼ï¼‰
        """
        valid_scores = []
        
        for task_name, result in task_results.items():
            if result.score > 0:  # åªè®¡ç®—æœ‰æ•ˆåˆ†æ•°
                valid_scores.append(result.score)
        
        return sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
    
    def generate_report(self, task_results: Dict[str, BenchmarkResult]) -> str:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            task_results: å„ä»»åŠ¡è¯„ä¼°ç»“æœ
            
        Returns:
            æ ¼å¼åŒ–çš„è¯„ä¼°æŠ¥å‘Š
        """
        report_lines = []
        report_lines.append(f"ğŸ† {self.name} åŸºå‡†è¯„ä¼°æŠ¥å‘Š")
        report_lines.append("=" * 50)
        
        overall_score = self.compute_overall_score(task_results)
        report_lines.append(f"ğŸ“Š æ€»åˆ†: {overall_score:.4f}")
        report_lines.append("")
        
        report_lines.append("ğŸ“‹ å„ä»»åŠ¡è¯¦æƒ…:")
        report_lines.append("-" * 30)
        
        for task_name, result in task_results.items():
            task_info = self.get_task_info(task_name)
            report_lines.append(f"â€¢ {task_name.upper()}: {result.score:.4f} ({result.metric_name})")
            report_lines.append(f"  æè¿°: {task_info['description']}")
            
            if 'error' not in result.details:
                if task_name == 'mrpc':
                    # MRPCæ˜¾ç¤ºF1å’Œå‡†ç¡®ç‡
                    details = result.details
                    report_lines.append(f"  F1: {details.get('f1', 0.0):.4f}, å‡†ç¡®ç‡: {details.get('accuracy', 0.0):.4f}")
                elif task_name == 'stsb':
                    # STS-Bæ˜¾ç¤ºå›å½’æŒ‡æ ‡
                    details = result.details
                    report_lines.append(f"  MSE: {details.get('mse', 0.0):.4f}, RÂ²: {details.get('r2', 0.0):.4f}")
            else:
                report_lines.append(f"  âŒ é”™è¯¯: {result.details['error']}")
            
            report_lines.append("")
        
        report_lines.append("ğŸ¯ åŸºå‡†ä¿¡æ¯:")
        report_lines.append(f"  åç§°: {self.description}")
        report_lines.append(f"  ç‰ˆæœ¬: {self.version}")
        report_lines.append(f"  ä»»åŠ¡æ•°é‡: {len(self.tasks)}")
        
        return "\n".join(report_lines)


def demo():
    """GLUEåŸºå‡†æ¼”ç¤º"""
    print("ğŸš€ GLUEåŸºå‡†è¯„ä¼°æ¼”ç¤º")
    print("=" * 50)
    
    if not METRICS_AVAILABLE:
        print("âŒ è¯„ä¼°æŒ‡æ ‡æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡Œæ¼”ç¤º")
        return
    
    # åˆ›å»ºGLUEåŸºå‡†
    glue = GLUEBenchmark()
    
    print("ğŸ“‹ å¯ç”¨ä»»åŠ¡:")
    for task_name in glue.get_available_tasks():
        info = glue.get_task_info(task_name)
        print(f"  â€¢ {task_name.upper()}: {info['description']} (æŒ‡æ ‡: {info['metric']})")
    
    print(f"\nğŸ§ª æ¨¡æ‹Ÿè¯„ä¼°æ¼”ç¤º:")
    print("-" * 30)
    
    # æ¨¡æ‹Ÿä¸€äº›ä»»åŠ¡çš„è¯„ä¼°ç»“æœ
    mock_results = {}
    
    # CoLA - Matthewsç›¸å…³ç³»æ•°
    cola_preds = [0, 1, 1, 0, 1, 0, 1, 1, 0, 1]
    cola_refs = [0, 1, 0, 0, 1, 1, 1, 1, 0, 0]
    mock_results['cola'] = (cola_preds, cola_refs)
    
    # SST-2 - å‡†ç¡®ç‡
    sst2_preds = [1, 0, 1, 1, 0, 1, 0, 1, 1, 0]
    sst2_refs = [1, 0, 1, 0, 0, 1, 0, 1, 1, 1]
    mock_results['sst2'] = (sst2_preds, sst2_refs)
    
    # MRPC - F1å’Œå‡†ç¡®ç‡
    mrpc_preds = [1, 0, 1, 1, 0, 0, 1, 0, 1, 1]
    mrpc_refs = [1, 0, 0, 1, 0, 1, 1, 0, 1, 0]
    mock_results['mrpc'] = (mrpc_preds, mrpc_refs)
    
    # STS-B - çš®å°”é€Šç›¸å…³ç³»æ•°
    stsb_preds = [3.2, 1.8, 4.1, 2.5, 3.7, 1.2, 4.5, 2.8, 3.9, 1.9]
    stsb_refs = [3.0, 2.0, 4.0, 2.8, 3.5, 1.5, 4.2, 2.5, 4.1, 2.2]
    mock_results['stsb'] = (stsb_preds, stsb_refs)
    
    # RTE - å‡†ç¡®ç‡
    rte_preds = [0, 1, 0, 1, 1, 0, 1, 0, 1, 0]
    rte_refs = [0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
    mock_results['rte'] = (rte_preds, rte_refs)
    
    # è¯„ä¼°æ‰€æœ‰ä»»åŠ¡
    results = glue.evaluate_multiple_tasks(mock_results)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = glue.generate_report(results)
    print(report)
    
    print(f"\nğŸ‰ GLUEåŸºå‡†æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    demo()